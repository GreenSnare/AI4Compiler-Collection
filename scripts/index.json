[
  {
    "id": "leather2020machine",
    "title": "Machine learning in compilers: Past, present and future",
    "authors": "Leather, Hugh and Cummins, Chris",
    "year": "2020",
    "source": "2020 Forum for Specification and Design Languages (FDL)",
    "category": "survey",
    "keywords": [],
    "abstract": "Writing optimising compilers is difficult. The range of programs that may be presented to the compiler is huge and the systems on which they run are complex, heterogeneous, non-deterministic, and constantly changing. The space of possible optimisations is also vast, making it very hard for compiler writers to design heuristics that take all of these considerations into account. As a result, many compiler optimisations are out of date or poorly tuned. Near the turn of the century it was first shown how compilers could be made to automatically search the optimisation space, producing programs far better optimised than previously possible, and without the need for compiler writers to worry about architecture or program specifics. The searches, though, were slow, so in the years that followed, machine learning was developed to learn heuristics from the results of previous searches so that thereafter the search could be avoided and much of the benefit could be gained in a single shot. In this paper we will give a retrospective of machine learning in compiler optimisation from its earliest inception, through some of the works that set themselves apart, to today's deep learning, finishing with our vision of the field's future.",
    "bibtex": "@inproceedings{leather2020machine,\n abstract = {Writing optimising compilers is difficult. The range of programs that may be presented to the compiler is huge and the systems on which they run are complex, heterogeneous, non-deterministic, and constantly changing. The space of possible optimisations is also vast, making it very hard for compiler writers to design heuristics that take all of these considerations into account. As a result, many compiler optimisations are out of date or poorly tuned. Near the turn of the century it was first shown how compilers could be made to automatically search the optimisation space, producing programs far better optimised than previously possible, and without the need for compiler writers to worry about architecture or program specifics. The searches, though, were slow, so in the years that followed, machine learning was developed to learn heuristics from the results of previous searches so that thereafter the search could be avoided and much of the benefit could be gained in a single shot. In this paper we will give a retrospective of machine learning in compiler optimisation from its earliest inception, through some of the works that set themselves apart, to today's deep learning, finishing with our vision of the field's future.},\n author = {Leather, Hugh and Cummins, Chris},\n booktitle = {2020 Forum for Specification and Design Languages (FDL)},\n link = {https://doi.org/10.1109/FDL50818.2020.9232934},\n organization = {IEEE},\n pages = {1--8},\n title = {Machine learning in compilers: Past, present and future},\n year = {2020}\n}\n",
    "link": "https://doi.org/10.1109/FDL50818.2020.9232934"
  },
  {
    "id": "wang2018machine",
    "title": "Machine learning in compiler optimisation",
    "authors": "Wang, Zheng and O'Boyle, Michael",
    "year": "2018",
    "source": "arXiv preprint arXiv:1805.03441",
    "category": "survey",
    "keywords": [],
    "abstract": "In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements.\n",
    "bibtex": "@article{wang2018machine,\n abstract = {In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements.\n},\n author = {Wang, Zheng and O'Boyle, Michael},\n journal = {arXiv preprint arXiv:1805.03441},\n link = {https://doi.org/10.1109/JPROC.2018.2817118},\n title = {Machine learning in compiler optimisation},\n year = {2018}\n}\n",
    "link": "https://doi.org/10.1109/JPROC.2018.2817118"
  },
  {
    "id": "ashouri2018survey",
    "title": "A survey on compiler autotuning using machine learning",
    "authors": "Ashouri, Amir H and Killian, William and Cavazos, John and Palermo, Gianluca and Silvano, Cristina",
    "year": "2018",
    "source": "ACM Computing Surveys (CSUR)",
    "category": "survey",
    "keywords": [],
    "abstract": "Since the mid-1990s, researchers have been trying to use machine-learning-based approaches to solve a number of different compiler optimization problems. These techniques primarily enhance the quality of the obtained results and, more importantly, make it feasible to tackle two main compiler optimization problems: optimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of applying optimizations). The compiler optimization space continues to grow due to the advancement of applications, increasing number of compiler optimizations, and new target architectures. Generic optimization passes in compilers cannot fully leverage newly introduced optimizations and, therefore, cannot keep up with the pace of increasing options. This survey summarizes and classifies the recent advances in using machine learning for the compiler optimization field, particularly on the two major problems of (1) selecting the best optimizations, and (2) the phase-ordering of optimizations. The survey highlights the approaches taken so far, the obtained results, the fine-grain classification among different approaches, and finally, the influential papers of the field.",
    "bibtex": "@article{ashouri2018survey,\n abstract = {Since the mid-1990s, researchers have been trying to use machine-learning-based approaches to solve a number of different compiler optimization problems. These techniques primarily enhance the quality of the obtained results and, more importantly, make it feasible to tackle two main compiler optimization problems: optimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of applying optimizations). The compiler optimization space continues to grow due to the advancement of applications, increasing number of compiler optimizations, and new target architectures. Generic optimization passes in compilers cannot fully leverage newly introduced optimizations and, therefore, cannot keep up with the pace of increasing options. This survey summarizes and classifies the recent advances in using machine learning for the compiler optimization field, particularly on the two major problems of (1) selecting the best optimizations, and (2) the phase-ordering of optimizations. The survey highlights the approaches taken so far, the obtained results, the fine-grain classification among different approaches, and finally, the influential papers of the field.},\n author = {Ashouri, Amir H and Killian, William and Cavazos, John and Palermo, Gianluca and Silvano, Cristina},\n journal = {ACM Computing Surveys (CSUR)},\n link = {https://doi.org/10.1145/3197978},\n number = {5},\n pages = {1--42},\n publisher = {ACM New York, NY, USA},\n title = {A survey on compiler autotuning using machine learning},\n volume = {51},\n year = {2018}\n}\n",
    "link": "https://doi.org/10.1145/3197978"
  },
  {
    "id": "ashouri2016compiler",
    "title": "Compiler autotuning using machine learning techniques",
    "authors": "Ashouri, Amir Hossein",
    "year": "2016",
    "source": "",
    "category": "survey",
    "keywords": [],
    "abstract": "Recent developments in silicon production and fabrication led to the creation of much faster computational units such as CPUs, GPUs, FPGAs, and similar chips with varying instruction set architectures (ISAs). Software (SW) programming paradigms including OpenMP, MPI, OpenCL, and OpenACC allow software developers to exploit Hardware (HW) parallelism to port legacy serial codes on these emerging platforms to attain application speedups. Compilers struggle to keep up with the increasing development pace of ever-expanding hardware and software programming paradigms. Additionally, growing complexity of the modern compilers and the concern over security are among the more serious problems that compilers should answer. Moore’s law states that transistor density should double every two years; however, the rate of compilers, which are faced with many open-research problems, have not been able to improve more than a few percentage points each year. Diversity of today’s architectures have forced programmers to spend additional ef- fort to port and tune their application code across different platforms. Compilers within this process need additional tuning which is a hard task itself. Recent compilers of- fer a vast number of multilayered optimizations, capable of targeting different code segments of an application. Choosing among these optimizations can significantly im- pact the performance of the code being optimized. The selection of the right set of compiler optimizations for a particular code segment is a very hard problem, but find- ing the best ordering of these optimizations adds further complexity. In fact, finding the best ordering is a long standing problem in compilation research called the phase- ordering problem. The traditional approach of constructing compiler heuristics to solve this problem simply can not cope with the enormous complexity of choosing the right ordering of optimizations for every code segment in an application. In this PhD thesis, we provide break-through approaches to tackle and mitigate the well-known problems of compiler optimization using design space exploration and ma- chine learning techniques. We show that not all the optimization passes are beneficial to be used within an optimization sequence and in fact many of the available passes are obliterating the effect of one another when ordering of the phases are taken into account. Experimental results show major improvement in performance metrics when our customized prediction models are in place versus standard fixed optimization passes predefined within state-of-the-art compiler frameworks e.g. GCC, LLVM, etc. We per- form application specific optimization based on the characteristics of applications under analysis and we show that this methodology is beneficial to mitigate the hard problem of selecting the best compiler optimizations and the phase-ordering problem. Late but not least, we hope that the proposed approaches in this PhD thesis will be useful for a wide range of readers, including computer architects, compiler developers, researchers and technical professionals.",
    "bibtex": "@article{ashouri2016compiler,\n abstract = {Recent developments in silicon production and fabrication led to the creation of much faster computational units such as CPUs, GPUs, FPGAs, and similar chips with varying instruction set architectures (ISAs). Software (SW) programming paradigms including OpenMP, MPI, OpenCL, and OpenACC allow software developers to exploit Hardware (HW) parallelism to port legacy serial codes on these emerging platforms to attain application speedups. Compilers struggle to keep up with the increasing development pace of ever-expanding hardware and software programming paradigms. Additionally, growing complexity of the modern compilers and the concern over security are among the more serious problems that compilers should answer. Moore’s law states that transistor density should double every two years; however, the rate of compilers, which are faced with many open-research problems, have not been able to improve more than a few percentage points each year. Diversity of today’s architectures have forced programmers to spend additional ef- fort to port and tune their application code across different platforms. Compilers within this process need additional tuning which is a hard task itself. Recent compilers of- fer a vast number of multilayered optimizations, capable of targeting different code segments of an application. Choosing among these optimizations can significantly im- pact the performance of the code being optimized. The selection of the right set of compiler optimizations for a particular code segment is a very hard problem, but find- ing the best ordering of these optimizations adds further complexity. In fact, finding the best ordering is a long standing problem in compilation research called the phase- ordering problem. The traditional approach of constructing compiler heuristics to solve this problem simply can not cope with the enormous complexity of choosing the right ordering of optimizations for every code segment in an application. In this PhD thesis, we provide break-through approaches to tackle and mitigate the well-known problems of compiler optimization using design space exploration and ma- chine learning techniques. We show that not all the optimization passes are beneficial to be used within an optimization sequence and in fact many of the available passes are obliterating the effect of one another when ordering of the phases are taken into account. Experimental results show major improvement in performance metrics when our customized prediction models are in place versus standard fixed optimization passes predefined within state-of-the-art compiler frameworks e.g. GCC, LLVM, etc. We per- form application specific optimization based on the characteristics of applications under analysis and we show that this methodology is beneficial to mitigate the hard problem of selecting the best compiler optimizations and the phase-ordering problem. Late but not least, we hope that the proposed approaches in this PhD thesis will be useful for a wide range of readers, including computer architects, compiler developers, researchers and technical professionals.},\n author = {Ashouri, Amir Hossein},\n link = {https://www.politesi.polimi.it/handle/10589/129561},\n publisher = {Politecnico di Milano},\n title = {Compiler autotuning using machine learning techniques},\n year = {2016}\n}\n",
    "link": "https://www.politesi.polimi.it/handle/10589/129561"
  },
  {
    "id": "mithul2024exploring",
    "title": "Exploring Compiler Optimization: A Survey of ML, DL and RL Techniques",
    "authors": "Mithul, C and Abdulla, D Mohammad and Virinchi, M Hari and Sathvik, M and Belwal, Meena",
    "year": "2024",
    "source": "2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS)",
    "category": "survey",
    "keywords": [],
    "abstract": "The past few years, traditional compiler optimization methods have been found to be further enhanced by machine learning (ML), deep learning (DL) and reinforcement learning (RL). These differ from classical techniques that often use rule of thumb based decision making. Rather, ML/DL/RL based approaches provide a means for learning from data thus improving performance in different dimensions such as code generation, resource allocation and runtime. In this paper we give an overview of current research and methodologies utilizing ML, DL and RL for compiler optimization purposes. We analyze the major models in terms of their employed learning strategies and desired optimizations within a compiler framework. Moreover, we highlight some of the difficulties faced when these compilers are embedded with these learning models such as adaptability, generalization and overhead trade-offs. Additionally, our survey presents case studies demonstrating Quantitative improvements on well-known benchmarks mainly focusing on models' adaptability to different architectures and their role in supporting the decision-making process of compilers. We conclude outlining open research questions as well as possible future directions for further investigations into this emerging interdisciplinary field.",
    "bibtex": "@inproceedings{mithul2024exploring,\n abstract = {The past few years, traditional compiler optimization methods have been found to be further enhanced by machine learning (ML), deep learning (DL) and reinforcement learning (RL). These differ from classical techniques that often use rule of thumb based decision making. Rather, ML/DL/RL based approaches provide a means for learning from data thus improving performance in different dimensions such as code generation, resource allocation and runtime. In this paper we give an overview of current research and methodologies utilizing ML, DL and RL for compiler optimization purposes. We analyze the major models in terms of their employed learning strategies and desired optimizations within a compiler framework. Moreover, we highlight some of the difficulties faced when these compilers are embedded with these learning models such as adaptability, generalization and overhead trade-offs. Additionally, our survey presents case studies demonstrating Quantitative improvements on well-known benchmarks mainly focusing on models' adaptability to different architectures and their role in supporting the decision-making process of compilers. We conclude outlining open research questions as well as possible future directions for further investigations into this emerging interdisciplinary field.},\n author = {Mithul, C and Abdulla, D Mohammad and Virinchi, M Hari and Sathvik, M and Belwal, Meena},\n booktitle = {2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS)},\n link = {https://doi.org/10.1109/CSITSS64042.2024.10816929},\n organization = {IEEE},\n pages = {1--6},\n title = {Exploring Compiler Optimization: A Survey of ML, DL and RL Techniques},\n year = {2024}\n}\n",
    "link": "https://doi.org/10.1109/CSITSS64042.2024.10816929"
  },
  {
    "id": "pandey2024survey",
    "title": "A Survey of Optimized Compiler Using Advanced Machine learning and Deep Learning Techniques",
    "authors": "Pandey, Lal Bahadur and Sharma, Manisha and Tiwari, Rajesh and Panda, Radhe Shyam and Roy, Partha",
    "year": "2024",
    "source": "2024 IEEE 6th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)",
    "category": "survey",
    "keywords": [],
    "abstract": "Optimizing compilers is a difficult and time-consuming task, especially when done by hand. As far as we know, the compiler handles both translation and optimization. An efficient compiler system can become more automated and simple, as evidenced by recent studies using deep learning and machine learning approaches. Model training, prediction, optimization, and feature selection are handled by most machine learning and deep learning methods. In this case, choosing the optimal characteristics is necessary in order to use deep learning and machine learning techniques to enhance the optimization quality. This study examines various approaches that might be utilized to enhance and refine the quality of the chosen heuristics as well as the general quality of machine learning and deep learning models in order to boost the compiler's efficiency. The phase-ordering problem, the amount of iterative program evaluations, and the time needed to obtain the best forecast are only a few of the many subjects covered by these approaches.",
    "bibtex": "@inproceedings{pandey2024survey,\n abstract = {Optimizing compilers is a difficult and time-consuming task, especially when done by hand. As far as we know, the compiler handles both translation and optimization. An efficient compiler system can become more automated and simple, as evidenced by recent studies using deep learning and machine learning approaches. Model training, prediction, optimization, and feature selection are handled by most machine learning and deep learning methods. In this case, choosing the optimal characteristics is necessary in order to use deep learning and machine learning techniques to enhance the optimization quality. This study examines various approaches that might be utilized to enhance and refine the quality of the chosen heuristics as well as the general quality of machine learning and deep learning models in order to boost the compiler's efficiency. The phase-ordering problem, the amount of iterative program evaluations, and the time needed to obtain the best forecast are only a few of the many subjects covered by these approaches.},\n author = {Pandey, Lal Bahadur and Sharma, Manisha and Tiwari, Rajesh and Panda, Radhe Shyam and Roy, Partha},\n booktitle = {2024 IEEE 6th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)},\n link = {https://doi.org/10.1109/ICCCMLA63077.2024.10871813},\n organization = {IEEE},\n pages = {256--259},\n title = {A Survey of Optimized Compiler Using Advanced Machine learning and Deep Learning Techniques},\n year = {2024}\n}\n",
    "link": "https://doi.org/10.1109/ICCCMLA63077.2024.10871813"
  },
  {
    "id": "wu2022survey",
    "title": "A survey of machine learning for computer architecture and systems",
    "authors": "Wu, Nan and Xie, Yuan",
    "year": "2022",
    "source": "ACM Computing Surveys (CSUR)",
    "category": "survey",
    "keywords": [],
    "abstract": "It has been a long time that computer architecture and systems are optimized for efficient execution of machine learning (ML) models. Now, it is time to reconsider the relationship between ML and systems and let ML transform the way that computer architecture and systems are designed. This embraces a twofold meaning: improvement of designers’ productivity and completion of the virtuous cycle. In this article, we present a comprehensive review of the work that applies ML for computer architecture and system design. First, we perform a high-level taxonomy by considering the typical role that ML techniques take in architecture/system design, i.e., either for fast predictive modeling or as the design methodology. Then, we summarize the common problems in computer architecture/system design that can be solved by ML techniques and the typical ML techniques employed to resolve each of them. In addition to emphasis on computer architecture in a narrow sense, we adopt the concept that data centers can be recognized as warehouse-scale computers; sketchy discussions are provided in adjacent computer systems, such as code generation and compiler; we also give attention to how ML techniques can aid and transform design automation. We further provide a future vision of opportunities and potential directions and envision that applying ML for computer architecture and systems would thrive in the community.",
    "bibtex": "@article{wu2022survey,\n abstract = {It has been a long time that computer architecture and systems are optimized for efficient execution of machine learning (ML) models. Now, it is time to reconsider the relationship between ML and systems and let ML transform the way that computer architecture and systems are designed. This embraces a twofold meaning: improvement of designers’ productivity and completion of the virtuous cycle. In this article, we present a comprehensive review of the work that applies ML for computer architecture and system design. First, we perform a high-level taxonomy by considering the typical role that ML techniques take in architecture/system design, i.e., either for fast predictive modeling or as the design methodology. Then, we summarize the common problems in computer architecture/system design that can be solved by ML techniques and the typical ML techniques employed to resolve each of them. In addition to emphasis on computer architecture in a narrow sense, we adopt the concept that data centers can be recognized as warehouse-scale computers; sketchy discussions are provided in adjacent computer systems, such as code generation and compiler; we also give attention to how ML techniques can aid and transform design automation. We further provide a future vision of opportunities and potential directions and envision that applying ML for computer architecture and systems would thrive in the community.},\n author = {Wu, Nan and Xie, Yuan},\n journal = {ACM Computing Surveys (CSUR)},\n link = {https://doi.org/10.1145/3494523},\n number = {3},\n pages = {1--39},\n publisher = {ACM New York, NY},\n title = {A survey of machine learning for computer architecture and systems},\n volume = {55},\n year = {2022}\n}\n",
    "link": "https://doi.org/10.1145/3494523"
  },
  {
    "id": "cummins2017end",
    "title": "End-to-end deep learning of optimization heuristics",
    "authors": "Cummins, Chris and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh",
    "year": "2017",
    "source": "2017 26th International Conference on Parallel Architectures and Compilation Techniques (PACT)",
    "category": "intrapass-tuning",
    "keywords": [
      "herustic learning"
    ],
    "abstract": "Accurate automatic optimization heuristics are necessary for dealing with thecomplexity and diversity of modern hardware and software. Machine learning is aproven technique for learning such heuristics, but its success is bound by thequality of the features used. These features must be hand crafted by developersthrough a combination of expert domain knowledge and trial and error. This makesthe quality of the final model directly dependent on the skill and availabletime of the system architect. Our work introduces a better way for building heuristics. We develop a deepneural network that learns heuristics over raw code, entirely without using codefeatures. The neural network simultaneously constructs appropriaterepresentations of the code and learns how best to optimize, removing the needfor manual feature creation. Further, we show that our neural nets can transferlearning from one optimization problem to another, improving the accuracy of newmodels, without the help of human experts. We compare the effectiveness of our automatically generated heuristics againstones with features hand-picked by experts. We examine two challenging tasks:predicting optimal mapping for heterogeneous parallelism and GPU threadcoarsening factors. In 89% of the cases, the quality of our fully automaticheuristics matches or surpasses that of state-of-the-art predictive models usinghand-crafted features, providing on average 14% and 12% more performance withno human effort expended on designing features.",
    "bibtex": "@inproceedings{cummins2017end,\n abstract = {Accurate automatic optimization heuristics are necessary for dealing with thecomplexity and diversity of modern hardware and software. Machine learning is aproven technique for learning such heuristics, but its success is bound by thequality of the features used. These features must be hand crafted by developersthrough a combination of expert domain knowledge and trial and error. This makesthe quality of the final model directly dependent on the skill and availabletime of the system architect. Our work introduces a better way for building heuristics. We develop a deepneural network that learns heuristics over raw code, entirely without using codefeatures. The neural network simultaneously constructs appropriaterepresentations of the code and learns how best to optimize, removing the needfor manual feature creation. Further, we show that our neural nets can transferlearning from one optimization problem to another, improving the accuracy of newmodels, without the help of human experts. We compare the effectiveness of our automatically generated heuristics againstones with features hand-picked by experts. We examine two challenging tasks:predicting optimal mapping for heterogeneous parallelism and GPU threadcoarsening factors. In 89% of the cases, the quality of our fully automaticheuristics matches or surpasses that of state-of-the-art predictive models usinghand-crafted features, providing on average 14% and 12% more performance withno human effort expended on designing features.},\n author = {Cummins, Chris and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},\n booktitle = {2017 26th International Conference on Parallel Architectures and Compilation Techniques (PACT)},\n keywords = {herustic learning},\n link = {https://doi.org/10.1109/PACT.2017.24},\n organization = {IEEE},\n pages = {219--232},\n title = {End-to-end deep learning of optimization heuristics},\n year = {2017}\n}\n",
    "link": "https://doi.org/10.1109/PACT.2017.24"
  },
  {
    "id": "cummins2021programl",
    "title": "Programl: A graph-based program representation for data flow analysis and compiler optimizations",
    "authors": "Cummins, Chris and Fisches, Zacharias V and Ben-Nun, Tal and Hoefler, Torsten and O’Boyle, Michael FP and Leather, Hugh",
    "year": "2021",
    "source": "International Conference on Machine Learning",
    "category": "intrapass-tuning",
    "keywords": [
      "loop",
      "dfg",
      "SVM"
    ],
    "abstract": "Machine learning (ML) is increasingly seen as a viable approach for building compiler optimization heuristics, but many ML methods cannot replicate even the simplest of the data flow analyses that are critical to making good optimization decisions. We posit that if ML cannot do that, then it is insufficiently able to reason about programs. We formulate data flow analyses as supervised learning tasks and introduce a large open dataset of programs and their corresponding labels from several analyses. We use this dataset to benchmark ML methods and show that they struggle on these fundamental program reasoning tasks. We propose ProGraML - Program Graphs for Machine Learning - a language-independent, portable representation of program semantics. ProGraML overcomes the limitations of prior works and yields improved performance on downstream optimization tasks.",
    "bibtex": "@inproceedings{cummins2021programl,\n abstract = {Machine learning (ML) is increasingly seen as a viable approach for building compiler optimization heuristics, but many ML methods cannot replicate even the simplest of the data flow analyses that are critical to making good optimization decisions. We posit that if ML cannot do that, then it is insufficiently able to reason about programs. We formulate data flow analyses as supervised learning tasks and introduce a large open dataset of programs and their corresponding labels from several analyses. We use this dataset to benchmark ML methods and show that they struggle on these fundamental program reasoning tasks. We propose ProGraML - Program Graphs for Machine Learning - a language-independent, portable representation of program semantics. ProGraML overcomes the limitations of prior works and yields improved performance on downstream optimization tasks.},\n author = {Cummins, Chris and Fisches, Zacharias V and Ben-Nun, Tal and Hoefler, Torsten and O’Boyle, Michael FP and Leather, Hugh},\n booktitle = {International Conference on Machine Learning},\n keywords = {loop,dfg,SVM},\n link = {https://proceedings.mlr.press/v139/cummins21a.html},\n organization = {PMLR},\n pages = {2244--2253},\n title = {Programl: A graph-based program representation for data flow analysis and compiler optimizations},\n year = {2021}\n}\n",
    "link": "https://proceedings.mlr.press/v139/cummins21a.html"
  },
  {
    "id": "haj2020neurovectorizer",
    "title": "Neurovectorizer: End-to-end vectorization with deep reinforcement learning",
    "authors": "Haj-Ali, Ameer and Ahmed, Nesreen K and Willke, Ted and Shao, Yakun Sophia and Asanovic, Krste and Stoica, Ion",
    "year": "2020",
    "source": "Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization",
    "category": "intrapass-tuning",
    "keywords": [
      "loop",
      "rl"
    ],
    "abstract": "One of the key challenges arising when compilers vectorize loops for today’s SIMD-compatible architectures is to decide if vectorization or interleaving is beneficial. Then, the compiler has to determine the number of instructions to pack together and the interleaving level (stride). Compilers are designed today to use fixed-cost models that are based on heuristics to make vectorization decisions on loops. However, these models are unable to capture the data dependency, the computation graph, or the organization of instructions. Alternatively, software engineers often hand-write the vectorization factors of every loop. This, however, places a huge burden on them, since it requires prior experience and significantly increases the development time.\nIn this work, we explore a novel approach for handling loop vectorization and propose an end-to-end solution using deep reinforcement learning (RL). We conjecture that deep RL can capture different instructions, dependencies, and data structures to enable learning a sophisticated model that can better predict the actual performance cost and determine the optimal vectorization factors. We develop an end-to-end framework, from code to vectorization, that integrates deep RL in the LLVM compiler. Our proposed framework takes benchmark codes as input and extracts the loop codes. These loop codes are then fed to a loop embedding generator that learns an embedding for these loops. Finally, the learned embeddings are used as input to a Deep RL agent, which dynamically determines the vectorization factors for all the loops. We further extend our framework to support random search, decision trees, supervised neural networks, and nearest-neighbor search. We evaluate our approaches against the currently used LLVM vectorizer and loop polyhedral optimization techniques. Our experiments show 1.29×−4.73× performance speedup compared to baseline and only 3% worse than the brute-force search on a wide range of benchmarks.",
    "bibtex": "@inproceedings{haj2020neurovectorizer,\n abstract = {One of the key challenges arising when compilers vectorize loops for today’s SIMD-compatible architectures is to decide if vectorization or interleaving is beneficial. Then, the compiler has to determine the number of instructions to pack together and the interleaving level (stride). Compilers are designed today to use fixed-cost models that are based on heuristics to make vectorization decisions on loops. However, these models are unable to capture the data dependency, the computation graph, or the organization of instructions. Alternatively, software engineers often hand-write the vectorization factors of every loop. This, however, places a huge burden on them, since it requires prior experience and significantly increases the development time.\nIn this work, we explore a novel approach for handling loop vectorization and propose an end-to-end solution using deep reinforcement learning (RL). We conjecture that deep RL can capture different instructions, dependencies, and data structures to enable learning a sophisticated model that can better predict the actual performance cost and determine the optimal vectorization factors. We develop an end-to-end framework, from code to vectorization, that integrates deep RL in the LLVM compiler. Our proposed framework takes benchmark codes as input and extracts the loop codes. These loop codes are then fed to a loop embedding generator that learns an embedding for these loops. Finally, the learned embeddings are used as input to a Deep RL agent, which dynamically determines the vectorization factors for all the loops. We further extend our framework to support random search, decision trees, supervised neural networks, and nearest-neighbor search. We evaluate our approaches against the currently used LLVM vectorizer and loop polyhedral optimization techniques. Our experiments show 1.29×−4.73× performance speedup compared to baseline and only 3% worse than the brute-force search on a wide range of benchmarks.},\n author = {Haj-Ali, Ameer and Ahmed, Nesreen K and Willke, Ted and Shao, Yakun Sophia and Asanovic, Krste and Stoica, Ion},\n booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},\n keywords = {loop,rl},\n link = {https://dl.acm.org/doi/abs/10.1145/3368826.3377928},\n pages = {242--255},\n title = {Neurovectorizer: End-to-end vectorization with deep reinforcement learning},\n year = {2020}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3368826.3377928"
  },
  {
    "id": "brauckmann2021polygym",
    "title": "Polygym: Polyhedral optimizations as an environment for reinforcement learning",
    "authors": "Brauckmann, Alexander and Goens, Andr{\\'e}s and Castrillon, Jeronimo",
    "year": "2021",
    "source": "2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)",
    "category": "intrapass-tuning",
    "keywords": [
      "rl",
      "poly"
    ],
    "abstract": "The polyhedral model allows a structured way of defining semantics-preserving transformations to improve the performance of a large class of loops. Finding profitable points in this space is a hard problem which is usually approached by heuristics that generalize from domain-expert knowledge. Existing search space formulations in state-of-the-art heuristics depend on the shape of particular loops, making it hard to leverage generic and more powerful optimization techniques from the machine learning domain. In this paper, we propose a shape-agnostic formulation for the space of legal transformations in the polyhedral model as a Markov Decision Process (MDP). Instead of using transformations, the formulation is based on an abstract space of possible schedules. In this formulation, states model partial schedules, which are constructed by actions that are reusable across different loops. With a simple heuristic to traverse the space, we demonstrate that our formulation is powerful enough to match and outperform state-of-the-art heuristics. On the Polybench benchmark suite, we found the search space to contain transformations that lead to a speedup of 3.39x over LLVM O3, which is 1.34x better than the best transformations found in the search space of isl, and 1.83x better than the speedup achieved by the default heuristics of isl. Our generic MDP formulation enables future work to use reinforcement learning to learn optimization heuristics over a wide range of loops. This also contributes to the emerging field of machine learning in compilers, as it exposes a novel problem formulation that can push the limits of existing methods.",
    "bibtex": "@inproceedings{brauckmann2021polygym,\n abstract = {The polyhedral model allows a structured way of defining semantics-preserving transformations to improve the performance of a large class of loops. Finding profitable points in this space is a hard problem which is usually approached by heuristics that generalize from domain-expert knowledge. Existing search space formulations in state-of-the-art heuristics depend on the shape of particular loops, making it hard to leverage generic and more powerful optimization techniques from the machine learning domain. In this paper, we propose a shape-agnostic formulation for the space of legal transformations in the polyhedral model as a Markov Decision Process (MDP). Instead of using transformations, the formulation is based on an abstract space of possible schedules. In this formulation, states model partial schedules, which are constructed by actions that are reusable across different loops. With a simple heuristic to traverse the space, we demonstrate that our formulation is powerful enough to match and outperform state-of-the-art heuristics. On the Polybench benchmark suite, we found the search space to contain transformations that lead to a speedup of 3.39x over LLVM O3, which is 1.34x better than the best transformations found in the search space of isl, and 1.83x better than the speedup achieved by the default heuristics of isl. Our generic MDP formulation enables future work to use reinforcement learning to learn optimization heuristics over a wide range of loops. This also contributes to the emerging field of machine learning in compilers, as it exposes a novel problem formulation that can push the limits of existing methods.},\n author = {Brauckmann, Alexander and Goens, Andr{\\'e}s and Castrillon, Jeronimo},\n booktitle = {2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)},\n keywords = {rl,poly},\n link = {https://doi.org/10.1109/PACT52795.2021.00009},\n organization = {IEEE},\n pages = {17--29},\n title = {Polygym: Polyhedral optimizations as an environment for reinforcement learning},\n year = {2021}\n}\n",
    "link": "https://doi.org/10.1109/PACT52795.2021.00009"
  },
  {
    "id": "trofin2021mlgo",
    "title": "Mlgo: a machine learning guided compiler optimizations framework",
    "authors": "Trofin, Mircea and Qian, Yundi and Brevdo, Eugene and Lin, Zinan and Choromanski, Krzysztof and Li, David",
    "year": "2021",
    "source": "arXiv preprint arXiv:2101.04808",
    "category": "intrapass-tuning",
    "keywords": [
      "codesize",
      "inline"
    ],
    "abstract": "Leveraging machine-learning (ML) techniques for compiler optimizations has been widely studied and explored in academia. However, the adoption of ML in general-purpose, industry strength compilers has yet to happen. We propose MLGO, a framework for integrating ML techniques systematically in an industrial compiler -- LLVM. As a case study, we present the details and results of replacing the heuristics-based inlining-for-size optimization in LLVM with machine learned models. To the best of our knowledge, this work is the first full integration of ML in a complex compiler pass in a real-world setting. It is available in the main LLVM repository. We use two different ML algorithms: Policy Gradient and Evolution Strategies, to train the inlining-for-size model, and achieve up to 7\\% size reduction, when compared to state of the art LLVM -Oz. The same model, trained on one corpus, generalizes well to a diversity of real-world targets, as well as to the same set of targets after months of active development. This property of the trained models is beneficial to deploy ML techniques in real-world settings.",
    "bibtex": "@article{trofin2021mlgo,\n abstract = {Leveraging machine-learning (ML) techniques for compiler optimizations has been widely studied and explored in academia. However, the adoption of ML in general-purpose, industry strength compilers has yet to happen. We propose MLGO, a framework for integrating ML techniques systematically in an industrial compiler -- LLVM. As a case study, we present the details and results of replacing the heuristics-based inlining-for-size optimization in LLVM with machine learned models. To the best of our knowledge, this work is the first full integration of ML in a complex compiler pass in a real-world setting. It is available in the main LLVM repository. We use two different ML algorithms: Policy Gradient and Evolution Strategies, to train the inlining-for-size model, and achieve up to 7\\% size reduction, when compared to state of the art LLVM -Oz. The same model, trained on one corpus, generalizes well to a diversity of real-world targets, as well as to the same set of targets after months of active development. This property of the trained models is beneficial to deploy ML techniques in real-world settings.},\n author = {Trofin, Mircea and Qian, Yundi and Brevdo, Eugene and Lin, Zinan and Choromanski, Krzysztof and Li, David},\n journal = {arXiv preprint arXiv:2101.04808},\n keywords = {codesize,inline},\n link = {https://arxiv.org/abs/2101.04808},\n title = {Mlgo: a machine learning guided compiler optimizations framework},\n year = {2021}\n}\n",
    "link": "https://arxiv.org/abs/2101.04808"
  },
  {
    "id": "venkatakeerthy2023rl4real",
    "title": "Rl4real: Reinforcement learning for register allocation",
    "authors": "VenkataKeerthy, S and Jain, Siddharth and Kundu, Anilava and Aggarwal, Rohit and Cohen, Albert and Upadrasta, Ramakrishna",
    "year": "2023",
    "source": "Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction",
    "category": "intrapass-tuning",
    "keywords": [
      "rl",
      "register alloc",
      "llvm-mca"
    ],
    "abstract": "We aim to automate decades of research and experience in register allocation, leveraging machine learning. We tackle this problem by embedding a multi-agent reinforcement learning algorithm within LLVM, training it with the state of the art techniques. We formalize the constraints that precisely define the problem for a given instruction-set architecture, while ensuring that the generated code preserves semantic correctness. We also develop a gRPC based framework providing a modular and efficient compiler interface for training and inference. Our approach is architecture independent: we show experimental results targeting Intel x86 and ARM AArch64. Our results match or out-perform the heavily tuned, production-grade register allocators of LLVM.",
    "bibtex": "@inproceedings{venkatakeerthy2023rl4real,\n abstract = {We aim to automate decades of research and experience in register allocation, leveraging machine learning. We tackle this problem by embedding a multi-agent reinforcement learning algorithm within LLVM, training it with the state of the art techniques. We formalize the constraints that precisely define the problem for a given instruction-set architecture, while ensuring that the generated code preserves semantic correctness. We also develop a gRPC based framework providing a modular and efficient compiler interface for training and inference. Our approach is architecture independent: we show experimental results targeting Intel x86 and ARM AArch64. Our results match or out-perform the heavily tuned, production-grade register allocators of LLVM.},\n author = {VenkataKeerthy, S and Jain, Siddharth and Kundu, Anilava and Aggarwal, Rohit and Cohen, Albert and Upadrasta, Ramakrishna},\n booktitle = {Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction},\n keywords = {rl,register alloc,llvm-mca},\n link = {https://dl.acm.org/doi/abs/10.1145/3578360.3580273},\n pages = {133--144},\n title = {Rl4real: Reinforcement learning for register allocation},\n year = {2023}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3578360.3580273"
  },
  {
    "id": "zheng2024mloop",
    "title": "mLOOP: Optimize Loop Unrolling in Compilation with a ML-based Approach",
    "authors": "Zheng, Zhongchun and Wu, Yuan and Zhang, Xianwei",
    "year": "2024",
    "source": "2024 International Conference on Networking, Architecture and Storage (NAS)",
    "category": "intrapass-tuning",
    "keywords": [
      "loop"
    ],
    "abstract": "Loops are a fundamental component of programs, providing an structured and efficient way to execute repetitive tasks. Given their prevalence and significance, the performance of loops has a direct impact on the overall execution of a program. Predicting loop unroll factor holds remarkable importance in the domain of loop optimization and vectorization parallelism. With the rapid advancements in this field, leveraging machine learning (ML) methods for compilation optimization has emerged as a new research focus. Whereas traditional heuristic algorithms lack precision and Profile-Guided Optimization (PGO) techniques incur considerable compilation overhead, ML method serve as a more balanced approach with respect to accuracy and compilation time. Nonetheless, existing ML approaches are commonly confined to individual optimizations and fail to consider the interplay between multiple optimizations. Additionally, there is inadequate utilization of compilation optimization parameters, resulting in redundant calculations across different optimization processes. This paper proposes mLOOP, a method that employs the XGBoost model to predict loop unroll factors which are integrated into the metadata for use throughout the compilation pipeline. To facilitate deployment and testing in practices, mLOOP is encapsulated into a LLVM optimization pass. By testing on multiple loop-intensive benchmarks, mLOOP achieves 7% speedup on X86 platform and 12% on ARM.",
    "bibtex": "@inproceedings{zheng2024mloop,\n abstract = {Loops are a fundamental component of programs, providing an structured and efficient way to execute repetitive tasks. Given their prevalence and significance, the performance of loops has a direct impact on the overall execution of a program. Predicting loop unroll factor holds remarkable importance in the domain of loop optimization and vectorization parallelism. With the rapid advancements in this field, leveraging machine learning (ML) methods for compilation optimization has emerged as a new research focus. Whereas traditional heuristic algorithms lack precision and Profile-Guided Optimization (PGO) techniques incur considerable compilation overhead, ML method serve as a more balanced approach with respect to accuracy and compilation time. Nonetheless, existing ML approaches are commonly confined to individual optimizations and fail to consider the interplay between multiple optimizations. Additionally, there is inadequate utilization of compilation optimization parameters, resulting in redundant calculations across different optimization processes. This paper proposes mLOOP, a method that employs the XGBoost model to predict loop unroll factors which are integrated into the metadata for use throughout the compilation pipeline. To facilitate deployment and testing in practices, mLOOP is encapsulated into a LLVM optimization pass. By testing on multiple loop-intensive benchmarks, mLOOP achieves 7% speedup on X86 platform and 12% on ARM.},\n author = {Zheng, Zhongchun and Wu, Yuan and Zhang, Xianwei},\n booktitle = {2024 International Conference on Networking, Architecture and Storage (NAS)},\n keywords = {loop},\n link = {https://doi.org/10.1109/NAS63802.2024.10781373},\n organization = {IEEE},\n pages = {1--8},\n title = {mLOOP: Optimize Loop Unrolling in Compilation with a ML-based Approach},\n year = {2024}\n}\n",
    "link": "https://doi.org/10.1109/NAS63802.2024.10781373"
  },
  {
    "id": "ashouri2022mlgoperf",
    "title": "Mlgoperf: An ml guided inliner to optimize performance",
    "authors": "Ashouri, Amir H and Elhoushi, Mostafa and Hua, Yuzhe and Wang, Xiang and Manzoor, Muhammad Asif and Chan, Bryan and Gao, Yaoqing",
    "year": "2022",
    "source": "arXiv preprint arXiv:2207.08389",
    "category": "intrapass-tuning",
    "keywords": [
      "inline"
    ],
    "abstract": "For the past 25 years, we have witnessed an extensive application of Machine Learning to the Compiler space; the selection and the phase-ordering problem. However, limited works have been upstreamed into the state-of-the-art compilers, i.e., LLVM, to seamlessly integrate the former into the optimization pipeline of a compiler to be readily deployed by the user. MLGO was among the first of such projects and it only strives to reduce the code size of a binary with an ML-based Inliner using Reinforcement Learning.This paper presents MLGOPerf; the first end-to-end framework capable of optimizing performance using LLVM's ML-Inliner. It employs a secondary ML model to generate rewards used for training a retargeted Reinforcement learning agent, previously used as the primary model by MLGO. It does so by predicting the post-inlining speedup of a function under analysis and it enables a fast training framework for the primary model which otherwise wouldn't be practical. The experimental results show MLGOPerf is able to gain up to 1.8% and 2.2% with respect to LLVM's optimization at O3 when trained for performance on SPEC CPU2006 and Cbench benchmarks, respectively. Furthermore, the proposed approach provides up to 26% increased opportunities to autotune code regions for our benchmarks which can be translated into an additional 3.7% speedup value.",
    "bibtex": "@article{ashouri2022mlgoperf,\n abstract = {For the past 25 years, we have witnessed an extensive application of Machine Learning to the Compiler space; the selection and the phase-ordering problem. However, limited works have been upstreamed into the state-of-the-art compilers, i.e., LLVM, to seamlessly integrate the former into the optimization pipeline of a compiler to be readily deployed by the user. MLGO was among the first of such projects and it only strives to reduce the code size of a binary with an ML-based Inliner using Reinforcement Learning.This paper presents MLGOPerf; the first end-to-end framework capable of optimizing performance using LLVM's ML-Inliner. It employs a secondary ML model to generate rewards used for training a retargeted Reinforcement learning agent, previously used as the primary model by MLGO. It does so by predicting the post-inlining speedup of a function under analysis and it enables a fast training framework for the primary model which otherwise wouldn't be practical. The experimental results show MLGOPerf is able to gain up to 1.8% and 2.2% with respect to LLVM's optimization at O3 when trained for performance on SPEC CPU2006 and Cbench benchmarks, respectively. Furthermore, the proposed approach provides up to 26% increased opportunities to autotune code regions for our benchmarks which can be translated into an additional 3.7% speedup value.},\n author = {Ashouri, Amir H and Elhoushi, Mostafa and Hua, Yuzhe and Wang, Xiang and Manzoor, Muhammad Asif and Chan, Bryan and Gao, Yaoqing},\n journal = {arXiv preprint arXiv:2207.08389},\n keywords = {inline},\n link = {https://arxiv.org/abs/2207.08389},\n title = {Mlgoperf: An ml guided inliner to optimize performance},\n year = {2022}\n}\n",
    "link": "https://arxiv.org/abs/2207.08389"
  },
  {
    "id": "mendis2019compiler",
    "title": "Compiler auto-vectorization with imitation learning",
    "authors": "Mendis, Charith and Yang, Cambridge and Pu, Yewen and Amarasinghe, Dr Saman and Carbin, Michael",
    "year": "2019",
    "source": "Advances in Neural Information Processing Systems",
    "category": "intrapass-tuning",
    "keywords": [
      "loop",
      "slp"
    ],
    "abstract": "Modern microprocessors are equipped with single instruction multiple data (SIMD) or vector instruction sets which allow compilers to exploit fine-grained data level parallelism. To exploit this parallelism, compilers employ auto-vectorization techniques to automatically convert scalar code into vector code. Larsen & Amarasinghe (2000) first introduced superword level parallelism (SLP) based vectorization, which is one form of vectorization popularly used by compilers. Current compilers employ hand-crafted heuristics and typically only follow one SLP vectorization strategy which can be suboptimal. Recently, Mendis & Amarasinghe (2018) formulated the instruction packing problem of SLP vectorization by leveraging an integer linear programming (ILP) solver, achieving superior runtime performance. In this work, we explore whether it is feasible to imitate optimal decisions made by their ILP solution by fitting a graph neural network policy. We show that the learnt policy produces a vectorization scheme which is better than industry standard compiler heuristics both in terms of static measures and runtime performance. More specifically, the learnt agent produces a vectorization scheme which has a 22.6% higher average reduction in cost compared to LLVM compiler when measured using its own cost model and achieves a geometric mean runtime speedup of 1.015× on the NAS benchmark suite when compared to LLVM’s SLP vectorizer.",
    "bibtex": "@article{mendis2019compiler,\n abstract = {Modern microprocessors are equipped with single instruction multiple data (SIMD) or vector instruction sets which allow compilers to exploit fine-grained data level parallelism. To exploit this parallelism, compilers employ auto-vectorization techniques to automatically convert scalar code into vector code. Larsen & Amarasinghe (2000) first introduced superword level parallelism (SLP) based vectorization, which is one form of vectorization popularly used by compilers. Current compilers employ hand-crafted heuristics and typically only follow one SLP vectorization strategy which can be suboptimal. Recently, Mendis & Amarasinghe (2018) formulated the instruction packing problem of SLP vectorization by leveraging an integer linear programming (ILP) solver, achieving superior runtime performance. In this work, we explore whether it is feasible to imitate optimal decisions made by their ILP solution by fitting a graph neural network policy. We show that the learnt policy produces a vectorization scheme which is better than industry standard compiler heuristics both in terms of static measures and runtime performance. More specifically, the learnt agent produces a vectorization scheme which has a 22.6% higher average reduction in cost compared to LLVM compiler when measured using its own cost model and achieves a geometric mean runtime speedup of 1.015× on the NAS benchmark suite when compared to LLVM’s SLP vectorizer.},\n author = {Mendis, Charith and Yang, Cambridge and Pu, Yewen and Amarasinghe, Dr Saman and Carbin, Michael},\n journal = {Advances in Neural Information Processing Systems},\n keywords = {loop,slp},\n link = {https://proceedings.neurips.cc/paper/2019/hash/d1d5923fc822531bbfd9d87d4760914b-Abstract.html},\n title = {Compiler auto-vectorization with imitation learning},\n volume = {32},\n year = {2019}\n}\n",
    "link": "https://proceedings.neurips.cc/paper/2019/hash/d1d5923fc822531bbfd9d87d4760914b-Abstract.html"
  },
  {
    "id": "pan2025towards",
    "title": "Towards efficient compiler auto-tuning: Leveraging synergistic search spaces",
    "authors": "Pan, Haolin and Wei, Yuanyu and Xing, Mingjie and Wu, Yanjun and Zhao, Chen",
    "year": "2025",
    "source": "Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization",
    "category": "interpass-tuning",
    "keywords": [
      "kmeans",
      "codesize",
      "GA"
    ],
    "abstract": "Determining the optimal sequence of compiler optimization passes is challenging due to the extensive and intricate search space. Traditional auto-tuning techniques, such as iterative compilation and machine learning methods, are often limited by high computational costs and difficulties in generalizing to new programs. These approaches can be inefficient and may not fully address the varying optimization needs across different programs. This paper introduces a novel approach that leverages the synergistic relationships between optimization passes to effectively reduce the search space. By focusing on chained synergy pass pairs that jointly optimize a specific target, our method uses K-means clustering to capture common optimization patterns across programs and forms these pairs into coresets. Leveraging a supervised learning model trained on these coresets, we effectively predict the most beneficial coreset for new programs, streamlining the search for optimal sequences. By integrating various search strategies, our method quickly converges to near-optimal solutions. Our approach achieves state-of-the-art performance on ten benchmark datasets, including MiBench, CBench, NPB, and CHStone, demonstrating an average reduction of 7.5% in Intermediate Representation (IR) instruction count compared to Oz. Furthermore, this set of chained synergy pass pairs is also well-suited for iterative search studies by other researchers, as it enables achieving an average codesize reduction of 13.9% compared to Oz with a simple search strategy that takes only about 5 seconds, outperforming existing search-based techniques in the initial pass search space across five datasets.",
    "bibtex": "@inproceedings{pan2025towards,\n abstract = {Determining the optimal sequence of compiler optimization passes is challenging due to the extensive and intricate search space. Traditional auto-tuning techniques, such as iterative compilation and machine learning methods, are often limited by high computational costs and difficulties in generalizing to new programs. These approaches can be inefficient and may not fully address the varying optimization needs across different programs. This paper introduces a novel approach that leverages the synergistic relationships between optimization passes to effectively reduce the search space. By focusing on chained synergy pass pairs that jointly optimize a specific target, our method uses K-means clustering to capture common optimization patterns across programs and forms these pairs into coresets. Leveraging a supervised learning model trained on these coresets, we effectively predict the most beneficial coreset for new programs, streamlining the search for optimal sequences. By integrating various search strategies, our method quickly converges to near-optimal solutions. Our approach achieves state-of-the-art performance on ten benchmark datasets, including MiBench, CBench, NPB, and CHStone, demonstrating an average reduction of 7.5% in Intermediate Representation (IR) instruction count compared to Oz. Furthermore, this set of chained synergy pass pairs is also well-suited for iterative search studies by other researchers, as it enables achieving an average codesize reduction of 13.9% compared to Oz with a simple search strategy that takes only about 5 seconds, outperforming existing search-based techniques in the initial pass search space across five datasets.},\n author = {Pan, Haolin and Wei, Yuanyu and Xing, Mingjie and Wu, Yanjun and Zhao, Chen},\n booktitle = {Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},\n keywords = {kmeans,codesize,GA,},\n link = {https://doi.org/10.1145/3696443.3708961},\n pages = {614--627},\n title = {Towards efficient compiler auto-tuning: Leveraging synergistic search spaces},\n year = {2025}\n}\n",
    "link": "https://doi.org/10.1145/3696443.3708961"
  },
  {
    "id": "ogilvie2017minimizing",
    "title": "Minimizing the cost of iterative compilation with active learning",
    "authors": "Ogilvie, William F and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh",
    "year": "2017",
    "source": "2017 IEEE/ACM international symposium on code generation and optimization (CGO)",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Since performance is not portable between platforms, engineers must fine-tune heuristics for each processor in turn. This is such a laborious task that high-profile compilers, supporting many architectures, cannot keep up with hardware innovation and are actually out-of-date. Iterative compilation driven by machine learning has been shown to be efficient at generating portable optimization models automatically. However, good quality models require costly, repetitive, and extensive training which greatly hinders the wide adoption of this powerful technique. In this work, we show that much of this cost is spent collecting training data, runtime measurements for different optimization decisions, which contribute little to the final heuristic. Current implementations evaluate randomly chosen, often redundant, training examples a pre-configured, almost always excessive, number of times - a large source of wasted effort. Our approach optimizes not only the selection of training examples but also the number of samples per example, independently. To evaluate, we construct 11 high-quality models which use a combination of optimization settings to predict the runtime of benchmarks from the SPAPT suite. Our novel, broadly applicable, methodology is able to reduce the training overhead by up to 26x compared to an approach with a fixed number of sample runs, transforming what is potentially months of work into days.",
    "bibtex": "@inproceedings{ogilvie2017minimizing,\n abstract = {Since performance is not portable between platforms, engineers must fine-tune heuristics for each processor in turn. This is such a laborious task that high-profile compilers, supporting many architectures, cannot keep up with hardware innovation and are actually out-of-date. Iterative compilation driven by machine learning has been shown to be efficient at generating portable optimization models automatically. However, good quality models require costly, repetitive, and extensive training which greatly hinders the wide adoption of this powerful technique. In this work, we show that much of this cost is spent collecting training data, runtime measurements for different optimization decisions, which contribute little to the final heuristic. Current implementations evaluate randomly chosen, often redundant, training examples a pre-configured, almost always excessive, number of times - a large source of wasted effort. Our approach optimizes not only the selection of training examples but also the number of samples per example, independently. To evaluate, we construct 11 high-quality models which use a combination of optimization settings to predict the runtime of benchmarks from the SPAPT suite. Our novel, broadly applicable, methodology is able to reduce the training overhead by up to 26x compared to an approach with a fixed number of sample runs, transforming what is potentially months of work into days.},\n author = {Ogilvie, William F and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},\n booktitle = {2017 IEEE/ACM international symposium on code generation and optimization (CGO)},\n link = {https://doi.org/10.1109/CGO.2017.7863744},\n organization = {IEEE},\n pages = {245--256},\n title = {Minimizing the cost of iterative compilation with active learning},\n year = {2017}\n}\n",
    "link": "https://doi.org/10.1109/CGO.2017.7863744"
  },
  {
    "id": "liang2023learning",
    "title": "Learning compiler pass orders using coreset and normalized value prediction",
    "authors": "Liang, Youwei and Stone, Kevin and Shameli, Ali and Cummins, Chris and Elhoushi, Mostafa and Guo, Jiadong and Steiner, Benoit and Yang, Xiaomeng and Xie, Pengtao and Leather, Hugh James and others",
    "year": "2023",
    "source": "International Conference on Machine Learning",
    "category": "interpass-tuning",
    "keywords": [
      "codesize",
      "opensource"
    ],
    "abstract": "Finding the optimal pass sequence of compilation can lead to a significant reduction in program size. Prior works on compilation pass ordering have two major drawbacks. They either require an excessive budget (in terms of the number of compilation passes) at compile time or fail to generalize to unseen programs. In this work, instead of predicting passes sequentially, we directly learn a policy on the pass sequence space, which outperforms the default -Oz flag by an average of 4.5% over a large collection (4683) of unseen code repositories from diverse domains across 14 datasets. To achieve this, we first identify a small set (termed coreset) of pass sequences that generally optimize the size of most programs. Then, a policy is learned to pick the optimal sequences by predicting the normalized values of the pass sequences in the coreset. Our results demonstrate that existing human-designed compiler passes can be improved with a simple yet effective technique that leverages pass sequence space which contains dense rewards, while approaches operating on the individual pass space may suffer from issues of sparse reward, and do not generalize well to held-out programs from different domains. Website: https://rlcompopt.github.io.",
    "bibtex": "@inproceedings{liang2023learning,\n abstract = {Finding the optimal pass sequence of compilation can lead to a significant reduction in program size. Prior works on compilation pass ordering have two major drawbacks. They either require an excessive budget (in terms of the number of compilation passes) at compile time or fail to generalize to unseen programs. In this work, instead of predicting passes sequentially, we directly learn a policy on the pass sequence space, which outperforms the default -Oz flag by an average of 4.5% over a large collection (4683) of unseen code repositories from diverse domains across 14 datasets. To achieve this, we first identify a small set (termed coreset) of pass sequences that generally optimize the size of most programs. Then, a policy is learned to pick the optimal sequences by predicting the normalized values of the pass sequences in the coreset. Our results demonstrate that existing human-designed compiler passes can be improved with a simple yet effective technique that leverages pass sequence space which contains dense rewards, while approaches operating on the individual pass space may suffer from issues of sparse reward, and do not generalize well to held-out programs from different domains. Website: https://rlcompopt.github.io.},\n author = {Liang, Youwei and Stone, Kevin and Shameli, Ali and Cummins, Chris and Elhoushi, Mostafa and Guo, Jiadong and Steiner, Benoit and Yang, Xiaomeng and Xie, Pengtao and Leather, Hugh James and others},\n booktitle = {International Conference on Machine Learning},\n keywords = {codesize,opensource},\n link = {https://proceedings.mlr.press/v202/liang23f.html},\n organization = {PMLR},\n pages = {20746--20762},\n title = {Learning compiler pass orders using coreset and normalized value prediction},\n year = {2023}\n}\n",
    "link": "https://proceedings.mlr.press/v202/liang23f.html"
  },
  {
    "id": "liu2022compiler",
    "title": "Compiler optimization parameter selection method based on ensemble learning",
    "authors": "Liu, Hui and Xu, Jinlong and Chen, Sen and Guo, Te",
    "year": "2022",
    "source": "Electronics",
    "category": "interpass-tuning",
    "keywords": [
      "PSO"
    ],
    "abstract": "Iterative compilation based on machine learning can effectively predict a program’s compiler optimization parameters. Although having some limits, such as the low efficiency of optimization parameter search and prediction accuracy, machine learning-based solutions have been a frontier research field in the field of iterative compilation and have gained increasing attention. The research challenges are focused on learning algorithm selection, optimal parameter search, and program feature representation. For the existing problems, we propose an ensemble learning-based optimization parameter selection (ELOPS) method for the compiler. First, in order to further improve the optimization parameter search efficiency and accuracy, we proposed a multi-objective particle swarm optimization (PSO) algorithm to determine the optimal compiler parameters of the program. Second, we extracted the mixed features of the program through the feature-class relevance method, rather than using static or dynamic features alone. Finally, as the existing research usually uses a separate machine learning algorithm to build prediction models, an ensemble learning model using program features and optimization parameters was constructed to effectively predict compiler optimization parameters of the new program. Using standard performance evaluation corporation 2006 (SPEC2006) and NAS parallel benchmark (NPB) benchmarks as well as some typical scientific computing programs, we compared ELOPS with the existing methods. The experimental results showed that we can respectively achieve 1.29× and 1.26× speedup when using our method on two platforms, which are better results than those of existing methods.",
    "bibtex": "@article{liu2022compiler,\n abstract = {Iterative compilation based on machine learning can effectively predict a program’s compiler optimization parameters. Although having some limits, such as the low efficiency of optimization parameter search and prediction accuracy, machine learning-based solutions have been a frontier research field in the field of iterative compilation and have gained increasing attention. The research challenges are focused on learning algorithm selection, optimal parameter search, and program feature representation. For the existing problems, we propose an ensemble learning-based optimization parameter selection (ELOPS) method for the compiler. First, in order to further improve the optimization parameter search efficiency and accuracy, we proposed a multi-objective particle swarm optimization (PSO) algorithm to determine the optimal compiler parameters of the program. Second, we extracted the mixed features of the program through the feature-class relevance method, rather than using static or dynamic features alone. Finally, as the existing research usually uses a separate machine learning algorithm to build prediction models, an ensemble learning model using program features and optimization parameters was constructed to effectively predict compiler optimization parameters of the new program. Using standard performance evaluation corporation 2006 (SPEC2006) and NAS parallel benchmark (NPB) benchmarks as well as some typical scientific computing programs, we compared ELOPS with the existing methods. The experimental results showed that we can respectively achieve 1.29× and 1.26× speedup when using our method on two platforms, which are better results than those of existing methods.},\n author = {Liu, Hui and Xu, Jinlong and Chen, Sen and Guo, Te},\n journal = {Electronics},\n keywords = {PSO},\n number = {15},\n pages = {2452},\n publisher = {MDPI},\n title = {Compiler optimization parameter selection method based on ensemble learning},\n volume = {11},\n year = {2022}\n}\n",
    "link": ""
  },
  {
    "id": "zhu2024compiler",
    "title": "Compiler autotuning through multiple-phase learning",
    "authors": "Zhu, Mingxuan and Hao, Dan and Chen, Junjie",
    "year": "2024",
    "source": "ACM Transactions on Software Engineering and Methodology",
    "category": "interpass-tuning",
    "keywords": [
      "PSO"
    ],
    "abstract": "Widely used compilers like GCC and LLVM usually have hundreds of optimizations controlled by optimization flags, which are enabled or disabled during compilation to improve the runtime performance (e.g., small execution time) of the compiler program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to manually tune compiler optimization flags. In the literature, a number of autotuning techniques have been proposed, which tune optimization flags for a compiled program by comparing its actual runtime performance with different optimization flag combinations. Due to the huge search space and heavy actual runtime cost, these techniques suffer from the widely recognized efficiency problem. To reduce the heavy runtime cost, in this article we propose a lightweight learning approach that uses a small number of actual runtime performance data to predict the runtime performance of a compiled program with various optimization flag combinations. Furthermore, to reduce the search space, we design a novel particle swarm algorithm that tunes compiler optimization flags with the prediction model. To evaluate the performance of the proposed approach, CompTuner, we conduct an extensive experimental study on two popular C compilers, GCC and LLVM, with two widely used benchmarks, cBench and PolyBench. The experimental results show that CompTuner significantly outperforms the six compared techniques, including the state-of-the-art technique BOCA.",
    "bibtex": "@article{zhu2024compiler,\n abstract = {Widely used compilers like GCC and LLVM usually have hundreds of optimizations controlled by optimization flags, which are enabled or disabled during compilation to improve the runtime performance (e.g., small execution time) of the compiler program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to manually tune compiler optimization flags. In the literature, a number of autotuning techniques have been proposed, which tune optimization flags for a compiled program by comparing its actual runtime performance with different optimization flag combinations. Due to the huge search space and heavy actual runtime cost, these techniques suffer from the widely recognized efficiency problem. To reduce the heavy runtime cost, in this article we propose a lightweight learning approach that uses a small number of actual runtime performance data to predict the runtime performance of a compiled program with various optimization flag combinations. Furthermore, to reduce the search space, we design a novel particle swarm algorithm that tunes compiler optimization flags with the prediction model. To evaluate the performance of the proposed approach, CompTuner, we conduct an extensive experimental study on two popular C compilers, GCC and LLVM, with two widely used benchmarks, cBench and PolyBench. The experimental results show that CompTuner significantly outperforms the six compared techniques, including the state-of-the-art technique BOCA.},\n author = {Zhu, Mingxuan and Hao, Dan and Chen, Junjie},\n journal = {ACM Transactions on Software Engineering and Methodology},\n keywords = {PSO},\n link = {https://dl.acm.org/doi/abs/10.1145/3640330},\n number = {4},\n pages = {1--38},\n publisher = {ACM New York, NY},\n title = {Compiler autotuning through multiple-phase learning},\n volume = {33},\n year = {2024}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3640330"
  },
  {
    "id": "ni2024tsoa",
    "title": "Tsoa: a two-stage optimization approach for GCC compilation options to minimize execution time",
    "authors": "Ni, Youcong and Du, Xin and Yuan, Yuan and Xiao, Ruliang and Chen, Gaolin",
    "year": "2024",
    "source": "Automated Software Engineering",
    "category": "interpass-tuning",
    "keywords": [
      "randomforest"
    ],
    "abstract": "The open-source compiler GCC offers numerous options to improve execution time. Two categories of approaches, machine learning-based and design space exploration, have emerged for selecting the optimal set of options. However, they continue to face challenge in quickly obtaining high-quality solutions due to the large and discrete optimization space, time-consuming utility evaluation for selected options, and complex interactions among options. To address these challenges, we propose TSOA, a Two-Stage Optimization Approach for GCC compilation options to minimize execution time. In the first stage, we present OPPM, an Option Preselection algorithm based on Pattern Mining. OPPM generates diverse samples to cover a wide range of option interactions. It subsequently mines frequent options from both objective-improved and non-improved samples. The mining results are further validated using CRC codes to precisely preselect options and reduce the optimization space. Transitioning to the second stage, we present OSEA, an Option Selection Evolutionary optimization Algorithm. OSEA is grounded in solution preselection and an option interaction graph. The solution preselection employs a random forest to build a classifier, efficiently identifying promising solutions for the next-generation population and thereby reducing the time spent on utility evaluation. Simultaneously, the option interaction graph is built to capture option interplays and their influence on objectives from evaluated solutions. Then, high-quality solutions are generated based on the option interaction graph. We evaluate the performance of TSOA by comparing it with representative machine learning-based and design space exploration approaches across a diverse set of 20 problem instances from two benchmark platforms. Additionally, we validate the effectiveness of OPPM and conduct related ablation experiments. The experimental results show that TSOA outperforms state-of-the-art approaches significantly in both optimization time and solution quality. Moreover, OPPM outperforms other option preselection algorithms, while the effectiveness of random forest-assisted solution preselection, along with new solution generation based on the option interaction graph, has been verified.",
    "bibtex": "@article{ni2024tsoa,\n abstract = {The open-source compiler GCC offers numerous options to improve execution time. Two categories of approaches, machine learning-based and design space exploration, have emerged for selecting the optimal set of options. However, they continue to face challenge in quickly obtaining high-quality solutions due to the large and discrete optimization space, time-consuming utility evaluation for selected options, and complex interactions among options. To address these challenges, we propose TSOA, a Two-Stage Optimization Approach for GCC compilation options to minimize execution time. In the first stage, we present OPPM, an Option Preselection algorithm based on Pattern Mining. OPPM generates diverse samples to cover a wide range of option interactions. It subsequently mines frequent options from both objective-improved and non-improved samples. The mining results are further validated using CRC codes to precisely preselect options and reduce the optimization space. Transitioning to the second stage, we present OSEA, an Option Selection Evolutionary optimization Algorithm. OSEA is grounded in solution preselection and an option interaction graph. The solution preselection employs a random forest to build a classifier, efficiently identifying promising solutions for the next-generation population and thereby reducing the time spent on utility evaluation. Simultaneously, the option interaction graph is built to capture option interplays and their influence on objectives from evaluated solutions. Then, high-quality solutions are generated based on the option interaction graph. We evaluate the performance of TSOA by comparing it with representative machine learning-based and design space exploration approaches across a diverse set of 20 problem instances from two benchmark platforms. Additionally, we validate the effectiveness of OPPM and conduct related ablation experiments. The experimental results show that TSOA outperforms state-of-the-art approaches significantly in both optimization time and solution quality. Moreover, OPPM outperforms other option preselection algorithms, while the effectiveness of random forest-assisted solution preselection, along with new solution generation based on the option interaction graph, has been verified.},\n author = {Ni, Youcong and Du, Xin and Yuan, Yuan and Xiao, Ruliang and Chen, Gaolin},\n journal = {Automated Software Engineering},\n keywords = {randomforest},\n link = {https://link.springer.com/article/10.1007/s10515-024-00437-w},\n number = {2},\n pages = {39},\n publisher = {Springer},\n title = {Tsoa: a two-stage optimization approach for GCC compilation options to minimize execution time},\n volume = {31},\n year = {2024}\n}\n",
    "link": "https://link.springer.com/article/10.1007/s10515-024-00437-w"
  },
  {
    "id": "park2022srtuner",
    "title": "Srtuner: Effective compiler optimization customization by exposing synergistic relations",
    "authors": "Park, Sunghyun and Latifi, Salar and Park, Yongjun and Behroozi, Armand and Jeon, Byungsoo and Mahlke, Scott",
    "year": "2022",
    "source": "2022 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Despite ceaseless efforts, extremely large and complex optimization space makes even the state-of-the-art compilers fail in delivering the most performant setting that can fully utilize the underlying hardware. Although this inefficiency suggests opportunity for tuning, it has been challenging for prior tuning methods to consider the complex interactions between optimizations and maximize the tuning quality while handling local optima efficiently. To tackle this problem, we suggest an intelligent auto-tuning strategy, called SRTuner, which searches for the best optimization setting by exposing important optimization interactions and directly using them to focus on promising subspaces. To reveal high-impact inter-optimization relations, SRTuner proposes a multistage structure and a distribution-based estimation method that approximates the impact of an optimization effectively. Besides, to efficiently handle local optima, our technique defines optimization decisions as a series of multi-armed bandit problems to formulate the exploration-exploitation dilemma. SRTuner is evaluated with three representative compilers from various domains on different target hardware: GCC (traditional C/ C++ compiler) on CPU, TVM (domain-specific machine learning compiler) on GPU, and OpenCL compilers (kernel compiler for heterogeneous computing) on both CPU/GPU. Results show that SRTuner accelerates target executions by 1.24×, 2.03× and 34.4× compared to the highest level of optimization provided by each compiler and outperforms state-of-the-art works by 1.04×−1.14×. As a byproduct of our unique tuning strategy, SRTuner can offer synergistic optimizations for each workload, which allows it to in part identify why it outperformed current compilers. With this information, we are able to find important optimizations that each compiler misused and demonstrate how this information can benefit future tuning strategies.",
    "bibtex": "@inproceedings{park2022srtuner,\n abstract = {Despite ceaseless efforts, extremely large and complex optimization space makes even the state-of-the-art compilers fail in delivering the most performant setting that can fully utilize the underlying hardware. Although this inefficiency suggests opportunity for tuning, it has been challenging for prior tuning methods to consider the complex interactions between optimizations and maximize the tuning quality while handling local optima efficiently. To tackle this problem, we suggest an intelligent auto-tuning strategy, called SRTuner, which searches for the best optimization setting by exposing important optimization interactions and directly using them to focus on promising subspaces. To reveal high-impact inter-optimization relations, SRTuner proposes a multistage structure and a distribution-based estimation method that approximates the impact of an optimization effectively. Besides, to efficiently handle local optima, our technique defines optimization decisions as a series of multi-armed bandit problems to formulate the exploration-exploitation dilemma. SRTuner is evaluated with three representative compilers from various domains on different target hardware: GCC (traditional C/ C++ compiler) on CPU, TVM (domain-specific machine learning compiler) on GPU, and OpenCL compilers (kernel compiler for heterogeneous computing) on both CPU/GPU. Results show that SRTuner accelerates target executions by 1.24×, 2.03× and 34.4× compared to the highest level of optimization provided by each compiler and outperforms state-of-the-art works by 1.04×−1.14×. As a byproduct of our unique tuning strategy, SRTuner can offer synergistic optimizations for each workload, which allows it to in part identify why it outperformed current compilers. With this information, we are able to find important optimizations that each compiler misused and demonstrate how this information can benefit future tuning strategies.},\n author = {Park, Sunghyun and Latifi, Salar and Park, Yongjun and Behroozi, Armand and Jeon, Byungsoo and Mahlke, Scott},\n booktitle = {2022 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},\n link = {https://ieeexplore.ieee.org/abstract/document/9741263},\n organization = {IEEE},\n pages = {118--130},\n title = {Srtuner: Effective compiler optimization customization by exposing synergistic relations},\n year = {2022}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/9741263"
  },
  {
    "id": "taugtekin2021foga",
    "title": "Foga: Flag optimization with genetic algorithm",
    "authors": "Ta{\\u{g}}tekin, Burak and H{\\\"o}ke, Berkan and Sezer, Mert Kutay and {\\\"O}zt{\\\"u}rk, Mahiye Uluya{\\u{g}}mur",
    "year": "2021",
    "source": "2021 International Conference on INnovations in Intelligent SysTems and Applications (INISTA)",
    "category": "interpass-tuning",
    "keywords": [
      "GA"
    ],
    "abstract": "Recently, program autotuning has become very popular especially in embedded systems, when we have limited resources such as computing power and memory where these systems run generally time-critical applications. Compiler optimization space gradually expands with the renewed compiler options and inclusion of new architectures. These advancements bring autotuning even more important position. In this paper, we introduced Flag Optimization with Genetic Algorithm (FOGA) as an autotuning solution for GCC flag optimization. FOGA has two main advantages over the other autotuning approaches: the first one is the hyperparameter tuning of the genetic algorithm (GA), the second one is the maximum iteration parameter to stop when no further improvement occurs. We demonstrated remarkable speedup in the execution time of C++ source codes with the help of optimization flags provided by FOGA when compared to the state of the art framework OpenTuner.",
    "bibtex": "@inproceedings{taugtekin2021foga,\n abstract = {Recently, program autotuning has become very popular especially in embedded systems, when we have limited resources such as computing power and memory where these systems run generally time-critical applications. Compiler optimization space gradually expands with the renewed compiler options and inclusion of new architectures. These advancements bring autotuning even more important position. In this paper, we introduced Flag Optimization with Genetic Algorithm (FOGA) as an autotuning solution for GCC flag optimization. FOGA has two main advantages over the other autotuning approaches: the first one is the hyperparameter tuning of the genetic algorithm (GA), the second one is the maximum iteration parameter to stop when no further improvement occurs. We demonstrated remarkable speedup in the execution time of C++ source codes with the help of optimization flags provided by FOGA when compared to the state of the art framework OpenTuner.},\n author = {Ta{\\u{g}}tekin, Burak and H{\\\"o}ke, Berkan and Sezer, Mert Kutay and {\\\"O}zt{\\\"u}rk, Mahiye Uluya{\\u{g}}mur},\n booktitle = {2021 International Conference on INnovations in Intelligent SysTems and Applications (INISTA)},\n keywords = {GA},\n link = {https://doi.org/10.1109/INISTA52262.2021.9548573},\n organization = {IEEE},\n pages = {1--6},\n title = {Foga: Flag optimization with genetic algorithm},\n year = {2021}\n}\n",
    "link": "https://doi.org/10.1109/INISTA52262.2021.9548573"
  },
  {
    "id": "liu2021iterative",
    "title": "Iterative compilation optimization based on metric learning and collaborative filtering",
    "authors": "Liu, Hongzhi and Luo, Jie and Li, Ying and Wu, Zhonghai",
    "year": "2021",
    "source": "ACM Transactions on Architecture and Code Optimization (TACO)",
    "category": "interpass-tuning",
    "keywords": [
      "PCA",
      "EFA"
    ],
    "abstract": "Pass selection and phase ordering are two critical compiler auto-tuning problems. Traditional heuristic methods cannot effectively address these NP-hard problems especially given the increasing number of compiler passes and diverse hardware architectures. Recent research efforts have attempted to address these problems through machine learning. However, the large search space of candidate pass sequences, the large numbers of redundant and irrelevant features, and the lack of training program instances make it difficult to learn models well. Several methods have tried to use expert knowledge to simplify the problems, such as using only the compiler passes or subsequences in the standard levels (e.g., -O1, -O2, and -O3) provided by compiler designers. However, these methods ignore other useful compiler passes that are not contained in the standard levels. Principal component analysis (PCA) and exploratory factor analysis (EFA) have been utilized to reduce the redundancy of feature data. However, these unsupervised methods retain all the information irrelevant to the performance of compilation optimization, which may mislead the subsequent model learning.\nTo solve these problems, we propose a compiler pass selection and phase ordering approach, called Iterative Compilation based on Metric learning and Collaborative filtering (ICMC). First, we propose a data-driven method to construct pass subsequences according to the observed collaborative interactions and dependency among passes on a given program set. Therefore, we can make use of all available compiler passes and prune the search space. Then, a supervised metric learning method is utilized to retain useful feature information for compilation optimization while removing both the irrelevant and the redundant information. Based on the learned similarity metric, a neighborhood-based collaborative filtering method is employed to iteratively recommend a few superior compiler passes for each target program. Last, an iterative data enhancement method is designed to alleviate the problem of lacking training program instances and to enhance the performance of iterative pass recommendations. The experimental results using the LLVM compiler on all 32 cBench programs show the following: (1) ICMC significantly outperforms several state-of-the-art compiler phase ordering methods, (2) it performs the same or better than the standard level -O3 on all the test programs, and (3) it can reach an average performance speedup of 1.20 (up to 1.46) compared with the standard level -O3.",
    "bibtex": "@article{liu2021iterative,\n abstract = {Pass selection and phase ordering are two critical compiler auto-tuning problems. Traditional heuristic methods cannot effectively address these NP-hard problems especially given the increasing number of compiler passes and diverse hardware architectures. Recent research efforts have attempted to address these problems through machine learning. However, the large search space of candidate pass sequences, the large numbers of redundant and irrelevant features, and the lack of training program instances make it difficult to learn models well. Several methods have tried to use expert knowledge to simplify the problems, such as using only the compiler passes or subsequences in the standard levels (e.g., -O1, -O2, and -O3) provided by compiler designers. However, these methods ignore other useful compiler passes that are not contained in the standard levels. Principal component analysis (PCA) and exploratory factor analysis (EFA) have been utilized to reduce the redundancy of feature data. However, these unsupervised methods retain all the information irrelevant to the performance of compilation optimization, which may mislead the subsequent model learning.\nTo solve these problems, we propose a compiler pass selection and phase ordering approach, called Iterative Compilation based on Metric learning and Collaborative filtering (ICMC). First, we propose a data-driven method to construct pass subsequences according to the observed collaborative interactions and dependency among passes on a given program set. Therefore, we can make use of all available compiler passes and prune the search space. Then, a supervised metric learning method is utilized to retain useful feature information for compilation optimization while removing both the irrelevant and the redundant information. Based on the learned similarity metric, a neighborhood-based collaborative filtering method is employed to iteratively recommend a few superior compiler passes for each target program. Last, an iterative data enhancement method is designed to alleviate the problem of lacking training program instances and to enhance the performance of iterative pass recommendations. The experimental results using the LLVM compiler on all 32 cBench programs show the following: (1) ICMC significantly outperforms several state-of-the-art compiler phase ordering methods, (2) it performs the same or better than the standard level -O3 on all the test programs, and (3) it can reach an average performance speedup of 1.20 (up to 1.46) compared with the standard level -O3.},\n author = {Liu, Hongzhi and Luo, Jie and Li, Ying and Wu, Zhonghai},\n journal = {ACM Transactions on Architecture and Code Optimization (TACO)},\n keywords = {PCA,EFA},\n link = {https://dl.acm.org/doi/full/10.1145/3480250},\n number = {1},\n pages = {1--25},\n publisher = {ACM New York, NY},\n title = {Iterative compilation optimization based on metric learning and collaborative filtering},\n volume = {19},\n year = {2021}\n}\n",
    "link": "https://dl.acm.org/doi/full/10.1145/3480250"
  },
  {
    "id": "jayatilaka2021towards",
    "title": "Towards compile-time-reducing compiler optimization selection via machine learning",
    "authors": "Jayatilaka, Tarindu and Ueno, Hideto and Georgakoudis, Giorgis and Park, EunJung and Doerfert, Johannes",
    "year": "2021",
    "source": "50th International Conference on Parallel Processing Workshop",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Compilers come with a multitude of optimizations to choose from, and the chosen optimizations significantly affect the performance of the code being optimized. Selecting the optimal set of optimizations to apply and determining the order to run them is non-trivial. All of these optimizations closely interact with each other, and an optimization’s ability to improve the code heavily depends on how the previous optimizations modified it. The current approach to solve this is to use a one-size-fits-all optimization sequence, that is designed to perform reasonably well for any given source code. In other words, they are not designed to optimize depending on the nature of the code, which usually results in sub-optimal performance. In this paper, we present preliminary work tackling the problem from the perspective of compile-time by adapting the optimization sequence to cater to different program types. We start by analyzing how the source code interacts with the passes, as well as how the passes interact with each other. We use this information and propose two potential methods driven by machine learning to run customized optimization sequences on the source code. First, we look at how we can use a neural network to predict and skip passes that do not optimize the code to improve compilation time. Second, we look at how we can use clustering and predictive models to select custom pass pipelines. We believe that our approach has the potential to replace the current one-size-fits-all approach, with better optimization sequences that are tailored to perform better depending on the code. At the same time, it will allow testing the potential pipelines thoroughly, a practical requirement to gain confidence in the correctness of the compiler.",
    "bibtex": "@inproceedings{jayatilaka2021towards,\n abstract = {Compilers come with a multitude of optimizations to choose from, and the chosen optimizations significantly affect the performance of the code being optimized. Selecting the optimal set of optimizations to apply and determining the order to run them is non-trivial. All of these optimizations closely interact with each other, and an optimization’s ability to improve the code heavily depends on how the previous optimizations modified it. The current approach to solve this is to use a one-size-fits-all optimization sequence, that is designed to perform reasonably well for any given source code. In other words, they are not designed to optimize depending on the nature of the code, which usually results in sub-optimal performance. In this paper, we present preliminary work tackling the problem from the perspective of compile-time by adapting the optimization sequence to cater to different program types. We start by analyzing how the source code interacts with the passes, as well as how the passes interact with each other. We use this information and propose two potential methods driven by machine learning to run customized optimization sequences on the source code. First, we look at how we can use a neural network to predict and skip passes that do not optimize the code to improve compilation time. Second, we look at how we can use clustering and predictive models to select custom pass pipelines. We believe that our approach has the potential to replace the current one-size-fits-all approach, with better optimization sequences that are tailored to perform better depending on the code. At the same time, it will allow testing the potential pipelines thoroughly, a practical requirement to gain confidence in the correctness of the compiler.},\n author = {Jayatilaka, Tarindu and Ueno, Hideto and Georgakoudis, Giorgis and Park, EunJung and Doerfert, Johannes},\n booktitle = {50th International Conference on Parallel Processing Workshop},\n link = {https://dl.acm.org/doi/abs/10.1145/3458744.3473355},\n pages = {1--6},\n title = {Towards compile-time-reducing compiler optimization selection via machine learning},\n year = {2021}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3458744.3473355"
  },
  {
    "id": "colucci2021mlcomp",
    "title": "MLComp: A methodology for machine learning-based performance estimation and adaptive selection of Pareto-optimal compiler optimization sequences",
    "authors": "Colucci, Alessio and Juh{\\'a}sz, D{\\'a}vid and Mosbeck, Martin and Marchisio, Alberto and Rehman, Semeen and Kreutzer, Manfred and Nadbath, G{\\\"u}nther and Jantsch, Axel and Shafique, Muhammad",
    "year": "2021",
    "source": "2021 Design, Automation \\& Test in Europe Conference \\& Exhibition (DATE)",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Embedded systems have proliferated in various consumer and industrial applications with the evolution of Cyber-Physical Systems and the Internet of Things. These systems are subjected to stringent constraints so that embedded software must be optimized for multiple objectives simultaneously, namely reduced energy consumption, execution time, and code size. Compilers offer optimization phases to improve these metrics. However, proper selection and ordering of them depends on multiple factors and typically requires expert knowledge. State-of-the-art optimizers facilitate different platforms and applications case by case, and they are limited by optimizing one metric at a time, as well as requiring a time-consuming adaptation for different targets through dynamic profiling. To address these problems, we propose the novel MLComp methodology, in which optimization phases are sequenced by a Reinforcement Learning-based policy. Training of the policy is supported by Machine Learning-based analytical models for quick performance estimation, thereby drastically reducing the time spent for dynamic profiling. In our framework, different Machine Learning models are automatically tested to choose the best-fitting one. The trained Performance Estimator model is leveraged to efficiently devise Reinforcement Learning-based multi-objective policies for creating quasi-optimal phase sequences. Compared to state-of-the-art estimation models, our Performance Estimator model achieves lower relative error (< 2%) with up to 50 × faster training time over multiple platforms and application domains. Our Phase Selection Policy improves execution time and energy consumption of a given code by up to 12% and 6%, respectively. The Performance Estimator and the Phase Selection Policy can be trained efficiently for any target platform and application domain.",
    "bibtex": "@inproceedings{colucci2021mlcomp,\n abstract = {Embedded systems have proliferated in various consumer and industrial applications with the evolution of Cyber-Physical Systems and the Internet of Things. These systems are subjected to stringent constraints so that embedded software must be optimized for multiple objectives simultaneously, namely reduced energy consumption, execution time, and code size. Compilers offer optimization phases to improve these metrics. However, proper selection and ordering of them depends on multiple factors and typically requires expert knowledge. State-of-the-art optimizers facilitate different platforms and applications case by case, and they are limited by optimizing one metric at a time, as well as requiring a time-consuming adaptation for different targets through dynamic profiling. To address these problems, we propose the novel MLComp methodology, in which optimization phases are sequenced by a Reinforcement Learning-based policy. Training of the policy is supported by Machine Learning-based analytical models for quick performance estimation, thereby drastically reducing the time spent for dynamic profiling. In our framework, different Machine Learning models are automatically tested to choose the best-fitting one. The trained Performance Estimator model is leveraged to efficiently devise Reinforcement Learning-based multi-objective policies for creating quasi-optimal phase sequences. Compared to state-of-the-art estimation models, our Performance Estimator model achieves lower relative error (< 2%) with up to 50 × faster training time over multiple platforms and application domains. Our Phase Selection Policy improves execution time and energy consumption of a given code by up to 12% and 6%, respectively. The Performance Estimator and the Phase Selection Policy can be trained efficiently for any target platform and application domain.},\n author = {Colucci, Alessio and Juh{\\'a}sz, D{\\'a}vid and Mosbeck, Martin and Marchisio, Alberto and Rehman, Semeen and Kreutzer, Manfred and Nadbath, G{\\\"u}nther and Jantsch, Axel and Shafique, Muhammad},\n booktitle = {2021 Design, Automation \\& Test in Europe Conference \\& Exhibition (DATE)},\n link = {https://ieeexplore.ieee.org/abstract/document/9474158},\n organization = {IEEE},\n pages = {108--113},\n title = {MLComp: A methodology for machine learning-based performance estimation and adaptive selection of Pareto-optimal compiler optimization sequences},\n year = {2021}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/9474158"
  },
  {
    "id": "chen2021efficient",
    "title": "Efficient compiler autotuning via bayesian optimization",
    "authors": "Chen, Junjie and Xu, Ningxin and Chen, Peiqi and Zhang, Hongyu",
    "year": "2021",
    "source": "2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)",
    "category": "interpass-tuning",
    "keywords": [
      "Bayesian"
    ],
    "abstract": "A typical compiler such as GCC supports hundreds of optimizations controlled by compilation flags for improving the runtime performance of the compiled program. Due to the large number of compilation flags and the exponential number of flag combinations, it is impossible for compiler users to manually tune these optimization flags in order to achieve the required runtime performance of the compiled programs. Over the years, many compiler autotuning approaches have been proposed to automatically tune optimization flags, but they still suffer from the efficiency problem due to the huge search space. In this paper, we propose the first Bayesian optimization based approach, called BOCA, for efficient compiler autotuning. In BOCA, we leverage a tree-based model for approximating the objective function in order to make Bayesian optimization scalable to a large number of optimization flags. Moreover, we design a novel searching strategy to improve the efficiency of Bayesian optimization by incorporating the impact of each optimization flag measured by the tree-based model and a decay function to strike a balance between exploitation and exploration. We conduct extensive experiments to investigate the effectiveness of BOCA on two most popular C compilers (i.e., GCC and LLVM) and two widely-used C benchmarks (i.e., cBench and PolyBench). The results show that BOCA significantly outperforms the state-of-the-art compiler autotuning approaches and Bayesion optimization methods in terms of the time spent on achieving specified speedups, demonstrating the effectiveness of BOCA.",
    "bibtex": "@inproceedings{chen2021efficient,\n abstract = {A typical compiler such as GCC supports hundreds of optimizations controlled by compilation flags for improving the runtime performance of the compiled program. Due to the large number of compilation flags and the exponential number of flag combinations, it is impossible for compiler users to manually tune these optimization flags in order to achieve the required runtime performance of the compiled programs. Over the years, many compiler autotuning approaches have been proposed to automatically tune optimization flags, but they still suffer from the efficiency problem due to the huge search space. In this paper, we propose the first Bayesian optimization based approach, called BOCA, for efficient compiler autotuning. In BOCA, we leverage a tree-based model for approximating the objective function in order to make Bayesian optimization scalable to a large number of optimization flags. Moreover, we design a novel searching strategy to improve the efficiency of Bayesian optimization by incorporating the impact of each optimization flag measured by the tree-based model and a decay function to strike a balance between exploitation and exploration. We conduct extensive experiments to investigate the effectiveness of BOCA on two most popular C compilers (i.e., GCC and LLVM) and two widely-used C benchmarks (i.e., cBench and PolyBench). The results show that BOCA significantly outperforms the state-of-the-art compiler autotuning approaches and Bayesion optimization methods in terms of the time spent on achieving specified speedups, demonstrating the effectiveness of BOCA.},\n author = {Chen, Junjie and Xu, Ningxin and Chen, Peiqi and Zhang, Hongyu},\n booktitle = {2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},\n keywords = {Bayesian},\n link = {https://ieeexplore.ieee.org/abstract/document/9401979},\n organization = {IEEE},\n pages = {1198--1209},\n title = {Efficient compiler autotuning via bayesian optimization},\n year = {2021}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/9401979"
  },
  {
    "id": "huang2019autophase",
    "title": "Autophase: Compiler phase-ordering for hls with deep reinforcement learning",
    "authors": "Huang, Qijing and Haj-Ali, Ameer and Moses, William and Xiang, John and Stoica, Ion and Asanovic, Krste and Wawrzynek, John",
    "year": "2019",
    "source": "2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "The performance of the code generated by a compiler depends on the order in which the optimization passes are applied. In high-level synthesis, the quality of the generated circuit relates directly to the code generated by the front-end compiler. Choosing a good order-often referred to as the phase-ordering problem-is an NP-hard problem. In this paper, we evaluate a new technique to address the phase-ordering problem: deep reinforcement learning. We implement a framework in the context of the LLVM compiler to optimize the ordering for HLS programs and compare the performance of deep reinforcement learning to state-of-the-art algorithms that address the phase-ordering problem. Overall, our framework runs one to two orders of magnitude faster than these algorithms, and achieves a 16% improvement in circuit performance over the -O3 compiler flag.",
    "bibtex": "@inproceedings{huang2019autophase,\n abstract = {The performance of the code generated by a compiler depends on the order in which the optimization passes are applied. In high-level synthesis, the quality of the generated circuit relates directly to the code generated by the front-end compiler. Choosing a good order-often referred to as the phase-ordering problem-is an NP-hard problem. In this paper, we evaluate a new technique to address the phase-ordering problem: deep reinforcement learning. We implement a framework in the context of the LLVM compiler to optimize the ordering for HLS programs and compare the performance of deep reinforcement learning to state-of-the-art algorithms that address the phase-ordering problem. Overall, our framework runs one to two orders of magnitude faster than these algorithms, and achieves a 16% improvement in circuit performance over the -O3 compiler flag.},\n author = {Huang, Qijing and Haj-Ali, Ameer and Moses, William and Xiang, John and Stoica, Ion and Asanovic, Krste and Wawrzynek, John},\n booktitle = {2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},\n link = {https://doi.org/10.1109/FCCM.2019.00049},\n organization = {IEEE},\n pages = {308--308},\n title = {Autophase: Compiler phase-ordering for hls with deep reinforcement learning},\n year = {2019}\n}\n",
    "link": "https://doi.org/10.1109/FCCM.2019.00049"
  },
  {
    "id": "ashouri2017micomp",
    "title": "Micomp: Mitigating the compiler phase-ordering problem using optimization sub-sequences and machine learning",
    "authors": "Ashouri, Amir H and Bignoli, Andrea and Palermo, Gianluca and Silvano, Cristina and Kulkarni, Sameer and Cavazos, John",
    "year": "2017",
    "source": "ACM Transactions on Architecture and Code Optimization (TACO)",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Recent compilers offer a vast number of multilayered optimizations targeting different code segments of an application. Choosing among these optimizations can significantly impact the performance of the code being optimized. The selection of the right set of compiler optimizations for a particular code segment is a very hard problem, but finding the best ordering of these optimizations adds further complexity. Finding the best ordering represents a long standing problem in compilation research, named the phase-ordering problem. The traditional approach of constructing compiler heuristics to solve this problem simply cannot cope with the enormous complexity of choosing the right ordering of optimizations for every code segment in an application.This article proposes an automatic optimization framework we call MiCOMP, which <u>Mi</u>tigates the <u>Com</u>piler <u>P</u>hase-ordering problem. We perform phase ordering of the optimizations in LLVM’s highest optimization level using optimization sub-sequences and machine learning. The idea is to cluster the optimization passes of LLVM’s O3 setting into different clusters to predict the speedup of a complete sequence of all the optimization clusters instead of having to deal with the ordering of more than 60 different individual optimizations. The predictive model uses (1) dynamic features, (2) an encoded version of the compiler sequence, and (3) an exploration heuristic to tackle the problem.Experimental results using the LLVM compiler framework and the Cbench suite show the effectiveness of the proposed clustering and encoding techniques to application-based reordering of passes, while using a number of predictive models. We perform statistical analysis on the results and compare against (1) random iterative compilation, (2) standard optimization levels, and (3) two recent prediction approaches. We show that MiCOMP’s iterative compilation using its sub-sequences can reach an average performance speedup of 1.31 (up to 1.51). Additionally, we demonstrate that MiCOMP’s prediction model outperforms the -O1, -O2, and -O3 optimization levels within using just a few predictions and reduces the prediction error rate down to only 5%. Overall, it achieves 90% of the available speedup by exploring less than 0.001% of the optimization space.",
    "bibtex": "@article{ashouri2017micomp,\n abstract = {Recent compilers offer a vast number of multilayered optimizations targeting different code segments of an application. Choosing among these optimizations can significantly impact the performance of the code being optimized. The selection of the right set of compiler optimizations for a particular code segment is a very hard problem, but finding the best ordering of these optimizations adds further complexity. Finding the best ordering represents a long standing problem in compilation research, named the phase-ordering problem. The traditional approach of constructing compiler heuristics to solve this problem simply cannot cope with the enormous complexity of choosing the right ordering of optimizations for every code segment in an application.This article proposes an automatic optimization framework we call MiCOMP, which <u>Mi</u>tigates the <u>Com</u>piler <u>P</u>hase-ordering problem. We perform phase ordering of the optimizations in LLVM’s highest optimization level using optimization sub-sequences and machine learning. The idea is to cluster the optimization passes of LLVM’s O3 setting into different clusters to predict the speedup of a complete sequence of all the optimization clusters instead of having to deal with the ordering of more than 60 different individual optimizations. The predictive model uses (1) dynamic features, (2) an encoded version of the compiler sequence, and (3) an exploration heuristic to tackle the problem.Experimental results using the LLVM compiler framework and the Cbench suite show the effectiveness of the proposed clustering and encoding techniques to application-based reordering of passes, while using a number of predictive models. We perform statistical analysis on the results and compare against (1) random iterative compilation, (2) standard optimization levels, and (3) two recent prediction approaches. We show that MiCOMP’s iterative compilation using its sub-sequences can reach an average performance speedup of 1.31 (up to 1.51). Additionally, we demonstrate that MiCOMP’s prediction model outperforms the -O1, -O2, and -O3 optimization levels within using just a few predictions and reduces the prediction error rate down to only 5%. Overall, it achieves 90% of the available speedup by exploring less than 0.001% of the optimization space.},\n author = {Ashouri, Amir H and Bignoli, Andrea and Palermo, Gianluca and Silvano, Cristina and Kulkarni, Sameer and Cavazos, John},\n journal = {ACM Transactions on Architecture and Code Optimization (TACO)},\n link = {https://dl.acm.org/doi/abs/10.1145/3124452},\n number = {3},\n pages = {1--28},\n publisher = {ACM New York, NY, USA},\n title = {Micomp: Mitigating the compiler phase-ordering problem using optimization sub-sequences and machine learning},\n volume = {14},\n year = {2017}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3124452"
  },
  {
    "id": "seeker2024revealing",
    "title": "Revealing compiler heuristics through automated discovery and optimization",
    "authors": "Seeker, Volker and Cummins, Chris and Cole, Murray and Franke, Bj{\\\"o}rn and Hazelwood, Kim and Leather, Hugh",
    "year": "2024",
    "source": "2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)",
    "category": "interpass-tuning",
    "keywords": [
      "search space"
    ],
    "abstract": "Tuning compiler heuristics and parameters is well known to improve optimization outcomes dramatically. Prior works have tuned command line flags and a few expert identified heuristics. However, there are an unknown number of heuristics buried, unmarked and unexposed inside the compiler as a consequence of decades of development without auto-tuning being foremost in the minds of developers. Many may not even have been considered heuristics by the developers who wrote them. The result is that auto-tuning search and machine learning can optimize only a tiny fraction of what could be possible if all heuristics were available to tune. Manually discovering all of these heuristics hidden among millions of lines of code and exposing them to auto-tuning tools is a Herculean task that is simply not practical. What is needed is a method of automatically finding these heuristics to extract every last drop of potential optimization. In this work, we propose Heureka, a framework that automatically identifies potential heuristics in the compiler that are highly profitable optimization targets and then automatically finds available tuning parameters for those heuristics with minimal human involvement. Our work is based on the following key insight: When modifying the output of a heuristic within an acceptable value range, the calling code using that output will still function correctly and produce semantically correct results. Building on that, we automatically manipulate the output of potential heuristic code in the compiler and decide using a Differential Testing approach if we found a heuristic or not. During output manipulation, we also explore acceptable value ranges of the targeted code. Heuristics identified in this way can then be tuned to optimize an objective function. We used Heureka to search for heuristics among eight thousand functions from the LLVM optimization passes, which is about 2% of all available functions. We then use identified heuristics to tune the compilation of 38 applications from the NAS and Polybench benchmark suites. Compared to an -ozbaseline we reduce binary sizes by up to 11.6% considering single heuristics only and up to 19.5% when stacking the effects of multiple identified tuning targets and applying a random search with minimal search effort. Generalizing from existing analysis results, Heureka needs, on average, a little under an hour on a single machine to identify relevant heuristic targets for a previously unseen application.",
    "bibtex": "@inproceedings{seeker2024revealing,\n abstract = {Tuning compiler heuristics and parameters is well known to improve optimization outcomes dramatically. Prior works have tuned command line flags and a few expert identified heuristics. However, there are an unknown number of heuristics buried, unmarked and unexposed inside the compiler as a consequence of decades of development without auto-tuning being foremost in the minds of developers. Many may not even have been considered heuristics by the developers who wrote them. The result is that auto-tuning search and machine learning can optimize only a tiny fraction of what could be possible if all heuristics were available to tune. Manually discovering all of these heuristics hidden among millions of lines of code and exposing them to auto-tuning tools is a Herculean task that is simply not practical. What is needed is a method of automatically finding these heuristics to extract every last drop of potential optimization. In this work, we propose Heureka, a framework that automatically identifies potential heuristics in the compiler that are highly profitable optimization targets and then automatically finds available tuning parameters for those heuristics with minimal human involvement. Our work is based on the following key insight: When modifying the output of a heuristic within an acceptable value range, the calling code using that output will still function correctly and produce semantically correct results. Building on that, we automatically manipulate the output of potential heuristic code in the compiler and decide using a Differential Testing approach if we found a heuristic or not. During output manipulation, we also explore acceptable value ranges of the targeted code. Heuristics identified in this way can then be tuned to optimize an objective function. We used Heureka to search for heuristics among eight thousand functions from the LLVM optimization passes, which is about 2% of all available functions. We then use identified heuristics to tune the compilation of 38 applications from the NAS and Polybench benchmark suites. Compared to an -ozbaseline we reduce binary sizes by up to 11.6% considering single heuristics only and up to 19.5% when stacking the effects of multiple identified tuning targets and applying a random search with minimal search effort. Generalizing from existing analysis results, Heureka needs, on average, a little under an hour on a single machine to identify relevant heuristic targets for a previously unseen application.},\n author = {Seeker, Volker and Cummins, Chris and Cole, Murray and Franke, Bj{\\\"o}rn and Hazelwood, Kim and Leather, Hugh},\n booktitle = {2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},\n keywords = {search space},\n link = {https://doi.org/10.1109/CGO57630.2024.10444847},\n organization = {IEEE},\n pages = {55--66},\n title = {Revealing compiler heuristics through automated discovery and optimization},\n year = {2024}\n}\n",
    "link": "https://doi.org/10.1109/CGO57630.2024.10444847"
  },
  {
    "id": "burgstaller2024optimization",
    "title": "Optimization Space Learning: A Lightweight, Noniterative Technique for Compiler Autotuning",
    "authors": "Burgstaller, Tamim and Garber, Damian and Le, Viet-Man and Felfernig, Alexander",
    "year": "2024",
    "source": "Proceedings of the 28th ACM International Systems and Software Product Line Conference",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Compilers are highly configurable systems. One can influence the performance of a compiled program by activating and deactivating selected compiler optimizations. However, automatically finding well-performing configurations is a challenging task. We consider expensive iteration, paired with recompilation of the program to optimize, as one of the main shortcomings of state-of-the-art approaches. Therefore, we propose Optimization Space Learning, a lightweight and noniterative technique. It exploits concepts known from configuration space learning and recommender systems to discover well-performing compiler configurations. This reduces the overhead induced by the approach significantly, compared to existing approaches. The process of finding a well-performing configuration is 800k times faster than with the state-of-the-art techniques.",
    "bibtex": "@inproceedings{burgstaller2024optimization,\n abstract = {Compilers are highly configurable systems. One can influence the performance of a compiled program by activating and deactivating selected compiler optimizations. However, automatically finding well-performing configurations is a challenging task. We consider expensive iteration, paired with recompilation of the program to optimize, as one of the main shortcomings of state-of-the-art approaches. Therefore, we propose Optimization Space Learning, a lightweight and noniterative technique. It exploits concepts known from configuration space learning and recommender systems to discover well-performing compiler configurations. This reduces the overhead induced by the approach significantly, compared to existing approaches. The process of finding a well-performing configuration is 800k times faster than with the state-of-the-art techniques.},\n author = {Burgstaller, Tamim and Garber, Damian and Le, Viet-Man and Felfernig, Alexander},\n booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},\n link = {https://dl.acm.org/doi/abs/10.1145/3646548.3672588},\n pages = {36--46},\n title = {Optimization Space Learning: A Lightweight, Noniterative Technique for Compiler Autotuning},\n year = {2024}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3646548.3672588"
  },
  {
    "id": "garciarena2016evolutionary",
    "title": "Evolutionary optimization of compiler flag selection by learning and exploiting flags interactions",
    "authors": "Garciarena, Unai and Santana, Roberto",
    "year": "2016",
    "source": "Proceedings of the 2016 on Genetic and Evolutionary Computation Conference Companion",
    "category": "interpass-tuning",
    "keywords": [
      "EA"
    ],
    "abstract": "Compiler flag selection can be an effective way to increase the quality of executable code according to different code quality criteria. Evolutionary algorithms have been successfully applied to this optimization problem. However, previous approaches have only partially addressed the question of capturing and exploiting the interactions between compilation options to improve the search. In this paper we deal with this question comparing estimation of distribution algorithms (EDAs) and a traditional genetic algorithm approach. We show that EDAs that learn bivariate interactions can improve the results of GAs for some of the programs considered. We also show that the probabilistic models generated as a result of the search for optimal flag combinations can be used to unveil the (problem-dependent) interactions between the flags, allowing the user a more informed choice of compilation options.",
    "bibtex": "@inproceedings{garciarena2016evolutionary,\n abstract = {Compiler flag selection can be an effective way to increase the quality of executable code according to different code quality criteria. Evolutionary algorithms have been successfully applied to this optimization problem. However, previous approaches have only partially addressed the question of capturing and exploiting the interactions between compilation options to improve the search. In this paper we deal with this question comparing estimation of distribution algorithms (EDAs) and a traditional genetic algorithm approach. We show that EDAs that learn bivariate interactions can improve the results of GAs for some of the programs considered. We also show that the probabilistic models generated as a result of the search for optimal flag combinations can be used to unveil the (problem-dependent) interactions between the flags, allowing the user a more informed choice of compilation options.},\n author = {Garciarena, Unai and Santana, Roberto},\n booktitle = {Proceedings of the 2016 on Genetic and Evolutionary Computation Conference Companion},\n keywords = {EA},\n link = {https://dl.acm.org/doi/abs/10.1145/2908961.2931696},\n pages = {1159--1166},\n title = {Evolutionary optimization of compiler flag selection by learning and exploiting flags interactions},\n year = {2016}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/2908961.2931696"
  },
  {
    "id": "Gong2025OptimizingWB",
    "title": "Optimizing WebAssembly Bytecode for IoT Devices Using Deep Reinforcement Learning",
    "authors": "Kaijie Gong and Ruiqi Yang and Haoyu Li and Yi Gao and Wei Dong",
    "year": "2025",
    "source": "ACM Transactions on Internet Technology",
    "category": "interpass-tuning",
    "keywords": [
      "drl"
    ],
    "abstract": "WebAssembly has shown promising potential on various IoT devices to achieve the desired features such as multi-language support and seamless device-cloud integration. The execution performance of WebAssembly bytecode is directly influenced by compilation sequences. While existing research has explored the optimization of compilation sequences for native code, these approaches are not suitable to WebAssembly bytecode due to its unique instruction format and control flow graph structure. In this work, we propose WasmRL, a novel efficient deep reinforcement learning (DRL)-based compiler optimization framework tailored for WebAssembly bytecode. We conduct a fine-grained analysis of the characteristics of WebAssembly instructions and associated compilation flags. We observe that the same compilation sequence may yield contrasting performance outcomes in WebAssembly and native code. Motivated by our observation, we introduce a WebAssembly-specific DRL state representation that simultaneously captures the impact of various compilation sequences on the WebAssembly bytecode and its runtime performance. To enhance the training efficiency of the DRL model, we propose a tree-based action space refinement method. Furthermore, we develop a pluggable cross-platform training strategy to optimize WebAssembly bytecode across different IoT devices. We evaluate the performance of WasmRL extensively on PolybenchC, MiBench, Shootout public datasets and real-world IoT applications. Experimental results show: (1) The DRL model trained on a specific device achieves 1.4x/1.1x speedups over -O3 for seen/unseen programs; (2) The DRL model trained on different devices simultaneously achieves 1.21x/1.06x improvements respectively. The code has been available at https://github.com/CarrollAdmin/WasmRL.",
    "bibtex": "@article{Gong2025OptimizingWB,\n abstract = {WebAssembly has shown promising potential on various IoT devices to achieve the desired features such as multi-language support and seamless device-cloud integration. The execution performance of WebAssembly bytecode is directly influenced by compilation sequences. While existing research has explored the optimization of compilation sequences for native code, these approaches are not suitable to WebAssembly bytecode due to its unique instruction format and control flow graph structure. In this work, we propose WasmRL, a novel efficient deep reinforcement learning (DRL)-based compiler optimization framework tailored for WebAssembly bytecode. We conduct a fine-grained analysis of the characteristics of WebAssembly instructions and associated compilation flags. We observe that the same compilation sequence may yield contrasting performance outcomes in WebAssembly and native code. Motivated by our observation, we introduce a WebAssembly-specific DRL state representation that simultaneously captures the impact of various compilation sequences on the WebAssembly bytecode and its runtime performance. To enhance the training efficiency of the DRL model, we propose a tree-based action space refinement method. Furthermore, we develop a pluggable cross-platform training strategy to optimize WebAssembly bytecode across different IoT devices. We evaluate the performance of WasmRL extensively on PolybenchC, MiBench, Shootout public datasets and real-world IoT applications. Experimental results show: (1) The DRL model trained on a specific device achieves 1.4x/1.1x speedups over -O3 for seen/unseen programs; (2) The DRL model trained on different devices simultaneously achieves 1.21x/1.06x improvements respectively. The code has been available at https://github.com/CarrollAdmin/WasmRL.},\n author = {Kaijie Gong and Ruiqi Yang and Haoyu Li and Yi Gao and Wei Dong},\n journal = {ACM Transactions on Internet Technology},\n keywords = {drl},\n link = {https://dl.acm.org/doi/abs/10.1145/3731451},\n pages = {1 - 26},\n title = {Optimizing WebAssembly Bytecode for IoT Devices Using Deep Reinforcement Learning},\n volume = {25},\n year = {2025}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3731451"
  },
  {
    "id": "zhu2023compiler",
    "title": "Compiler auto-tuning via critical flag selection",
    "authors": "Zhu, Mingxuan and Hao, Dan",
    "year": "2023",
    "source": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
    "category": "interpass-tuning",
    "keywords": [
      "gcc"
    ],
    "abstract": "Widely used compilers like GCC usually have hundreds of optimizations controlled by optimization flags, which can be enabled or disabled during compilation to improve the runtime performance of a compiled program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to tune compiler optimization flags manually. In the literature, many auto-tuning techniques have been proposed, which find a desired setting on all optimization flags (i.e., an optimization sequence) by designing different search strategies in the entire optimization space. Due to the huge search space, these techniques suffer from the widely-recognized efficiency problem. To reduce the search space, in this paper, we propose a critical-flag selection based approach CFSCA which first finds flags potentially relevant to the target program by analyzing program structure and compiler documentation, and then identifies critical flags through statistical analysis on the program's predicted runtime performance with various optimization sequences. With the reduced search space, CFSCA selects a desired optimization sequence. To evaluate the performance of the proposed approach CFSCA, we conduct an extensive experimental study on the latest version of the compiler GCC with a widely used benchmark cBench. The experimental results show that CFSCA significantly outperforms the four compared techniques, including the state-of-art technique BOCA.",
    "bibtex": "@inproceedings{zhu2023compiler,\n abstract = {Widely used compilers like GCC usually have hundreds of optimizations controlled by optimization flags, which can be enabled or disabled during compilation to improve the runtime performance of a compiled program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to tune compiler optimization flags manually. In the literature, many auto-tuning techniques have been proposed, which find a desired setting on all optimization flags (i.e., an optimization sequence) by designing different search strategies in the entire optimization space. Due to the huge search space, these techniques suffer from the widely-recognized efficiency problem. To reduce the search space, in this paper, we propose a critical-flag selection based approach CFSCA which first finds flags potentially relevant to the target program by analyzing program structure and compiler documentation, and then identifies critical flags through statistical analysis on the program's predicted runtime performance with various optimization sequences. With the reduced search space, CFSCA selects a desired optimization sequence. To evaluate the performance of the proposed approach CFSCA, we conduct an extensive experimental study on the latest version of the compiler GCC with a widely used benchmark cBench. The experimental results show that CFSCA significantly outperforms the four compared techniques, including the state-of-art technique BOCA.},\n author = {Zhu, Mingxuan and Hao, Dan},\n booktitle = {2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)},\n keywords = {gcc},\n link = {https://ieeexplore.ieee.org/abstract/document/10298446},\n organization = {IEEE},\n pages = {1000--1011},\n title = {Compiler auto-tuning via critical flag selection},\n year = {2023}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/10298446"
  },
  {
    "id": "cereda2020collaborative",
    "title": "A collaborative filtering approach for the automatic tuning of compiler optimisations",
    "authors": "Cereda, Stefano and Palermo, Gianluca and Cremonesi, Paolo and Doni, Stefano",
    "year": "2020",
    "source": "The 21st ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Selecting the right compiler optimisations has a severe impact on programs' performance. Still, the available optimisations keep increasing, and their effect depends on the specific program, making the task human intractable. Researchers proposed several techniques to search in the space of compiler optimisations. Some approaches focus on finding better search algorithms, while others try to speed up the search by leveraging previously collected knowledge. The possibility to effectively reuse previous compilation results inspired us toward the investigation of techniques derived from the Recommender Systems field. The proposed approach exploits previously collected knowledge and improves its characterisation over time. Differently from current state-of-the-art solutions, our approach is not based on performance counters but relies on Reaction Matching, an algorithm able to characterise programs looking at how they react to different optimisation sets. The proposed approach has been validated using two widely used benchmark suites, cBench and PolyBench, including 54 different programs. Our solution, on average, extracted 90% of the available performance improvement 10 iterations before current state-of-the-art solutions,which corresponds to 40% fewer compilations and performance tests to perform.",
    "bibtex": "@inproceedings{cereda2020collaborative,\n abstract = {Selecting the right compiler optimisations has a severe impact on programs' performance. Still, the available optimisations keep increasing, and their effect depends on the specific program, making the task human intractable. Researchers proposed several techniques to search in the space of compiler optimisations. Some approaches focus on finding better search algorithms, while others try to speed up the search by leveraging previously collected knowledge. The possibility to effectively reuse previous compilation results inspired us toward the investigation of techniques derived from the Recommender Systems field. The proposed approach exploits previously collected knowledge and improves its characterisation over time. Differently from current state-of-the-art solutions, our approach is not based on performance counters but relies on Reaction Matching, an algorithm able to characterise programs looking at how they react to different optimisation sets. The proposed approach has been validated using two widely used benchmark suites, cBench and PolyBench, including 54 different programs. Our solution, on average, extracted 90% of the available performance improvement 10 iterations before current state-of-the-art solutions,which corresponds to 40% fewer compilations and performance tests to perform.},\n author = {Cereda, Stefano and Palermo, Gianluca and Cremonesi, Paolo and Doni, Stefano},\n booktitle = {The 21st ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems},\n keywords = {},\n link = {https://dl.acm.org/doi/abs/10.1145/3372799.3394361},\n pages = {15--25},\n title = {A collaborative filtering approach for the automatic tuning of compiler optimisations},\n year = {2020}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3372799.3394361"
  },
  {
    "id": "shahzad2022reinforcement",
    "title": "Reinforcement learning strategies for compiler optimization in high level synthesis",
    "authors": "Shahzad, Hafsah and Sanaullah, Ahmed and Arora, Sanjay and Munafo, Robert and Yao, Xiteng and Drepper, Ulrich and Herbordt, Martin",
    "year": "2022",
    "source": "2022 IEEE/ACM Eighth Workshop on the LLVM Compiler Infrastructure in HPC (LLVM-HPC)",
    "category": "interpass-tuning",
    "keywords": [
      "ML",
      "llvm",
      "HLS"
    ],
    "abstract": "High Level Synthesis (HLS) offers a possible programmability solution for FPGAs by automatically compiling CPU codes to custom hardware configurations, but currently delivers far lower hardware quality than circuits written using Hardware Description Languages (HDLs). One reason is because the standard set of code optimizations used by CPU compilers, such as LLVM, are not well suited for a FPGA back end. Code performance is impacted largely by the order in which passes are applied. Similarly, it is also imperative to find a reasonable number of passes to apply and the optimum pass parameter values. In order to bridge the gap between hand tuned and automatically generated hardware, it is thus important to determine the optimal sequence of passes for HLS compilations, which could vary substantially across different workloads. Machine learning (ML) offers one popular approach to automate finding optimal compiler passes but requires selecting the right method. Supervised ML is not ideal since it requires labeled data mapping workload to optimal (or close to optimal) sequence of passes, which is computationally prohibitive. Unsupervised ML techniques don’t take into account the requirement that a quantity representing performance needs to be maximized. Reinforcement learning, which represents the problem of maximizing longterm rewards without requiring labeled data has been used for such planning problems before. While much work has been done along these lines for compilers in general, that directed towards HLS has been limited and conservative. In this paper, we address these limitations by expanding both the number of learning strategies for HLS compiler tuning and the metrics used to evaluate their impact. Our results show improvements over state-of-art for each standard benchmark evaluated and learning quality metric investigated. Choosing just the right strategy can give an improvement of 23× in learning speed, 4× in performance potential, 3×...",
    "bibtex": "@inproceedings{shahzad2022reinforcement,\n abstract = {High Level Synthesis (HLS) offers a possible programmability solution for FPGAs by automatically compiling CPU codes to custom hardware configurations, but currently delivers far lower hardware quality than circuits written using Hardware Description Languages (HDLs). One reason is because the standard set of code optimizations used by CPU compilers, such as LLVM, are not well suited for a FPGA back end. Code performance is impacted largely by the order in which passes are applied. Similarly, it is also imperative to find a reasonable number of passes to apply and the optimum pass parameter values. In order to bridge the gap between hand tuned and automatically generated hardware, it is thus important to determine the optimal sequence of passes for HLS compilations, which could vary substantially across different workloads. Machine learning (ML) offers one popular approach to automate finding optimal compiler passes but requires selecting the right method. Supervised ML is not ideal since it requires labeled data mapping workload to optimal (or close to optimal) sequence of passes, which is computationally prohibitive. Unsupervised ML techniques don’t take into account the requirement that a quantity representing performance needs to be maximized. Reinforcement learning, which represents the problem of maximizing longterm rewards without requiring labeled data has been used for such planning problems before. While much work has been done along these lines for compilers in general, that directed towards HLS has been limited and conservative. In this paper, we address these limitations by expanding both the number of learning strategies for HLS compiler tuning and the metrics used to evaluate their impact. Our results show improvements over state-of-art for each standard benchmark evaluated and learning quality metric investigated. Choosing just the right strategy can give an improvement of 23× in learning speed, 4× in performance potential, 3×...},\n author = {Shahzad, Hafsah and Sanaullah, Ahmed and Arora, Sanjay and Munafo, Robert and Yao, Xiteng and Drepper, Ulrich and Herbordt, Martin},\n booktitle = {2022 IEEE/ACM Eighth Workshop on the LLVM Compiler Infrastructure in HPC (LLVM-HPC)},\n keywords = {ML,llvm,HLS},\n link = {https://ieeexplore.ieee.org/abstract/document/10027131},\n organization = {IEEE},\n pages = {13--22},\n title = {Reinforcement learning strategies for compiler optimization in high level synthesis},\n year = {2022}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/10027131"
  },
  {
    "id": "xiao2024eatuner",
    "title": "EAtuner: Comparative Study of Evolutionary Algorithms for Compiler Auto-tuning",
    "authors": "Xiao, Guojian and Qin, Siyuan and Li, Kuan and Chen, Juan and Yin, Jianping",
    "year": "2024",
    "source": "2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)",
    "category": "interpass-tuning",
    "keywords": [
      "DE",
      "llvm"
    ],
    "abstract": "The manual adjustment of compilation flags by compiler users is impractical due to the exponential size of the search space. To address this, machine learning-based compiler auto-tuning methods, particularly evolutionary algorithms, have been proposed. However, existing works use different benchmarks and experimental setups, making it difficult to compare the strengths and weaknesses of various algorithms. To address this, we present EAtuner, an evolutionary algorithm-based framework for compiler auto-tuning, with the goal of benchmarking and identifying suitable algorithms for compiler auto-tuning. We implement ten discrete binary evolutionary algorithms and evaluate their effectiveness on the LLVM compiler through experiments. Notably, eight of these algorithms have not been previously applied to compiler flag optimization problems before our work. The results show that all ten algorithms can effectively achieve compiler auto-tuning, resulting in an average speedup of 1.204. However, there are notable differences in the effectiveness and efficiency of each algorithm, particularly in optimization efficiency, which is positively correlated with the number of program compilations. Based on this, we classify the algorithms into three levels, with Differential Evolution (DE) showing significant advantages in optimization effectiveness and efficiency. Additionally, we provide a comprehensive summary of the applicability of compiler flags, the correlation between them, and their relationship with programs.",
    "bibtex": "@inproceedings{xiao2024eatuner,\n abstract = {The manual adjustment of compilation flags by compiler users is impractical due to the exponential size of the search space. To address this, machine learning-based compiler auto-tuning methods, particularly evolutionary algorithms, have been proposed. However, existing works use different benchmarks and experimental setups, making it difficult to compare the strengths and weaknesses of various algorithms. To address this, we present EAtuner, an evolutionary algorithm-based framework for compiler auto-tuning, with the goal of benchmarking and identifying suitable algorithms for compiler auto-tuning. We implement ten discrete binary evolutionary algorithms and evaluate their effectiveness on the LLVM compiler through experiments. Notably, eight of these algorithms have not been previously applied to compiler flag optimization problems before our work. The results show that all ten algorithms can effectively achieve compiler auto-tuning, resulting in an average speedup of 1.204. However, there are notable differences in the effectiveness and efficiency of each algorithm, particularly in optimization efficiency, which is positively correlated with the number of program compilations. Based on this, we classify the algorithms into three levels, with Differential Evolution (DE) showing significant advantages in optimization effectiveness and efficiency. Additionally, we provide a comprehensive summary of the applicability of compiler flags, the correlation between them, and their relationship with programs.},\n author = {Xiao, Guojian and Qin, Siyuan and Li, Kuan and Chen, Juan and Yin, Jianping},\n booktitle = {2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)},\n keywords = {DE,llvm},\n link = {https://ieeexplore.ieee.org/abstract/document/10580120},\n organization = {IEEE},\n pages = {419--426},\n title = {EAtuner: Comparative Study of Evolutionary Algorithms for Compiler Auto-tuning},\n year = {2024}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/10580120"
  },
  {
    "id": "gao2025grouptuner",
    "title": "Grouptuner: Efficient Group-Aware Compiler Auto-tuning",
    "authors": "Gao, Bingyu and Yao, Mengyu and Wang, Ziming and Liu, Dong and Li, Ding and Chen, Xiangqun and Guo, Yao",
    "year": "2025",
    "source": "Proceedings of the 26th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Modern compilers typically provide hundreds of options to optimize program performance, but users often cannot fully leverage them due to the huge number of options. While standard optimization combinations (e.g., -O3) provide reasonable defaults, they often fail to deliver near-peak performance across diverse programs and architectures. To address this challenge, compiler auto-tuning techniques have emerged to automate the discovery of improved option combinations. Existing techniques typically focus on identifying critical options and prioritizing them during the search to improve efficiency. However, due to limited tuning iterations, the resulting data is often sparse and noisy, making it highly challenging to accurately identify critical options. As a result, these algorithms are prone to being trapped in local optima. To address this limitation, we propose GroupTuner, a group-aware auto-tuning technique that directly applies localized mutation to coherent option groups based on historically best-performing combinations, thus avoiding explicitly identifying critical options. By forgoing the need to know precisely which options are most important, GroupTuner maximizes the use of existing performance data, ensuring more targeted exploration. Extensive experiments demonstrate that GroupTuner can efficiently discover competitive option combinations, achieving an average performance improvement of 12.39% over -O3 while requiring only 77.21% of the time compared to the random search algorithm, significantly outperforming state-of-the-art methods.",
    "bibtex": "@inproceedings{gao2025grouptuner,\n abstract = {Modern compilers typically provide hundreds of options to optimize program performance, but users often cannot fully leverage them due to the huge number of options. While standard optimization combinations (e.g., -O3) provide reasonable defaults, they often fail to deliver near-peak performance across diverse programs and architectures. To address this challenge, compiler auto-tuning techniques have emerged to automate the discovery of improved option combinations. Existing techniques typically focus on identifying critical options and prioritizing them during the search to improve efficiency. However, due to limited tuning iterations, the resulting data is often sparse and noisy, making it highly challenging to accurately identify critical options. As a result, these algorithms are prone to being trapped in local optima. To address this limitation, we propose GroupTuner, a group-aware auto-tuning technique that directly applies localized mutation to coherent option groups based on historically best-performing combinations, thus avoiding explicitly identifying critical options. By forgoing the need to know precisely which options are most important, GroupTuner maximizes the use of existing performance data, ensuring more targeted exploration. Extensive experiments demonstrate that GroupTuner can efficiently discover competitive option combinations, achieving an average performance improvement of 12.39% over -O3 while requiring only 77.21% of the time compared to the random search algorithm, significantly outperforming state-of-the-art methods.},\n author = {Gao, Bingyu and Yao, Mengyu and Wang, Ziming and Liu, Dong and Li, Ding and Chen, Xiangqun and Guo, Yao},\n booktitle = {Proceedings of the 26th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},\n keywords = {},\n link = {https://doi.org/10.1145/3735452.3735530},\n pages = {122--133},\n title = {Grouptuner: Efficient Group-Aware Compiler Auto-tuning},\n year = {2025}\n}\n",
    "link": "https://doi.org/10.1145/3735452.3735530"
  },
  {
    "id": "li2022unleashing",
    "title": "Unleashing the power of compiler intermediate representation to enhance neural program embeddings",
    "authors": "Li, Zongjie and Ma, Pingchuan and Wang, Huaijin and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi",
    "year": "2022",
    "source": "Proceedings of the 44th International Conference on Software Engineering",
    "category": "representation-learning",
    "keywords": [
      "GA",
      "embedding"
    ],
    "abstract": "Neural program embeddings have demonstrated considerable promise in a range of program analysis tasks, including clone identification, program repair, code completion, and program synthesis. However, most existing methods generate neural program embeddings directly from the program source codes, by learning from features such as tokens, abstract syntax trees, and control flow graphs. This paper takes a fresh look at how to improve program embeddings by leveraging compiler intermediate representation (IR). We first demonstrate simple yet highly effective methods for enhancing embedding quality by training embedding models alongside source code and LLVM IR generated by default optimization levels (e.g., -O2). We then introduce IRGen, a framework based on genetic algorithms (GA), to identify (near-)optimal sequences of optimization flags that can significantly improve embedding quality. We use IRGen to find optimal sequences of LLVM optimization flags by performing GA on source code datasets. We then extend a popular code embedding model, CodeCMR, by adding a new objective based on triplet loss to enable a joint learning over source code and LLVM IR. We benchmark the quality of embedding using a representative downstream application, code clone detection. When CodeCMR was trained with source code and LLVM IRs optimized by findings of IRGen, the embedding quality was significantly improved, outperforming the state-of-the-art model, CodeBERT, which was trained only with source code. Our augmented CodeCMR also outperformed CodeCMR trained over source code and IR optimized with default optimization levels. We investigate the properties of optimization flags that increase embedding quality, demonstrate IRGen's generalization in boosting other embedding models, and establish IRGen's use in settings with extremely limited training data. Our research and findings demonstrate that a straightforward addition to modern neural code embedding models can provide a highly effective enhancement.",
    "bibtex": "@inproceedings{li2022unleashing,\n abstract = {Neural program embeddings have demonstrated considerable promise in a range of program analysis tasks, including clone identification, program repair, code completion, and program synthesis. However, most existing methods generate neural program embeddings directly from the program source codes, by learning from features such as tokens, abstract syntax trees, and control flow graphs. This paper takes a fresh look at how to improve program embeddings by leveraging compiler intermediate representation (IR). We first demonstrate simple yet highly effective methods for enhancing embedding quality by training embedding models alongside source code and LLVM IR generated by default optimization levels (e.g., -O2). We then introduce IRGen, a framework based on genetic algorithms (GA), to identify (near-)optimal sequences of optimization flags that can significantly improve embedding quality. We use IRGen to find optimal sequences of LLVM optimization flags by performing GA on source code datasets. We then extend a popular code embedding model, CodeCMR, by adding a new objective based on triplet loss to enable a joint learning over source code and LLVM IR. We benchmark the quality of embedding using a representative downstream application, code clone detection. When CodeCMR was trained with source code and LLVM IRs optimized by findings of IRGen, the embedding quality was significantly improved, outperforming the state-of-the-art model, CodeBERT, which was trained only with source code. Our augmented CodeCMR also outperformed CodeCMR trained over source code and IR optimized with default optimization levels. We investigate the properties of optimization flags that increase embedding quality, demonstrate IRGen's generalization in boosting other embedding models, and establish IRGen's use in settings with extremely limited training data. Our research and findings demonstrate that a straightforward addition to modern neural code embedding models can provide a highly effective enhancement.},\n author = {Li, Zongjie and Ma, Pingchuan and Wang, Huaijin and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi},\n booktitle = {Proceedings of the 44th International Conference on Software Engineering},\n keywords = {GA,embedding},\n link = {https://dl.acm.org/doi/10.1145/3510003.3510217},\n pages = {2253--2265},\n title = {Unleashing the power of compiler intermediate representation to enhance neural program embeddings},\n year = {2022}\n}\n",
    "link": "https://dl.acm.org/doi/10.1145/3510003.3510217"
  },
  {
    "id": "mosaner2022machine",
    "title": "Machine-Learning-Based Self-Optimizing Compiler Heuristics✱",
    "authors": "Mosaner, Raphael and Leopoldseder, David and Kisling, Wolfgang and Stadler, Lukas and M{\\\"o}ssenb{\\\"o}ck, Hanspeter",
    "year": "2022",
    "source": "Proceedings of the 19th International Conference on Managed Programming Languages and Runtimes",
    "category": "jit",
    "keywords": [
      "GraalVM"
    ],
    "abstract": "Compiler optimizations are often based on hand-crafted heuristics to guide the optimization process. These heuristics are designed to benefit the average program and are otherwise static or only customized by profiling information. We propose machine-learning-based self-optimizing compiler heuristics, a novel approach for fitting optimization decisions in a dynamic compiler to specific environments. This is done by updating a machine learning model with extracted performance data at run time. Related work—which primarily targets static compilers—has already shown that machine learning can outperform hand-crafted heuristics. Our approach is specifically designed for dynamic compilation and uses concepts such as deoptimization for transparently switching between generating data and performing machine learning decisions in single program runs. We implemented our approach in the GraalVM, a high-performance production VM for dynamic compilation. When evaluating our approach by replacing loop peeling heuristics with learned models we encountered speedups larger than 30% for several benchmarks and only few slowdowns of up to 7%.",
    "bibtex": "@inproceedings{mosaner2022machine,\n abstract = {Compiler optimizations are often based on hand-crafted heuristics to guide the optimization process. These heuristics are designed to benefit the average program and are otherwise static or only customized by profiling information. We propose machine-learning-based self-optimizing compiler heuristics, a novel approach for fitting optimization decisions in a dynamic compiler to specific environments. This is done by updating a machine learning model with extracted performance data at run time. Related work—which primarily targets static compilers—has already shown that machine learning can outperform hand-crafted heuristics. Our approach is specifically designed for dynamic compilation and uses concepts such as deoptimization for transparently switching between generating data and performing machine learning decisions in single program runs. We implemented our approach in the GraalVM, a high-performance production VM for dynamic compilation. When evaluating our approach by replacing loop peeling heuristics with learned models we encountered speedups larger than 30% for several benchmarks and only few slowdowns of up to 7%.},\n author = {Mosaner, Raphael and Leopoldseder, David and Kisling, Wolfgang and Stadler, Lukas and M{\\\"o}ssenb{\\\"o}ck, Hanspeter},\n booktitle = {Proceedings of the 19th International Conference on Managed Programming Languages and Runtimes},\n keywords = {GraalVM},\n link = {https://dl.acm.org/doi/abs/10.1145/3546918.3546921},\n pages = {98--111},\n title = {Machine-Learning-Based Self-Optimizing Compiler Heuristics✱},\n year = {2022}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3546918.3546921"
  },
  {
    "id": "fang2024stacksight",
    "title": "StackSight: Unveiling webassembly through large language models and neurosymbolic chain-of-thought decompilation",
    "authors": "Fang, Weike and Zhou, Zhejian and He, Junzhou and Wang, Weihang",
    "year": "2024",
    "source": "arXiv preprint arXiv:2406.04568",
    "category": "binary-optimization",
    "keywords": [
      "webassembly",
      "CoT",
      "decompile"
    ],
    "abstract": "WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics.",
    "bibtex": "@article{fang2024stacksight,\n abstract = {WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics.},\n author = {Fang, Weike and Zhou, Zhejian and He, Junzhou and Wang, Weihang},\n journal = {arXiv preprint arXiv:2406.04568},\n keywords = {webassembly,CoT,decompile},\n link = {https://arxiv.org/abs/2406.04568},\n title = {StackSight: Unveiling webassembly through large language models and neurosymbolic chain-of-thought decompilation},\n year = {2024}\n}\n",
    "link": "https://arxiv.org/abs/2406.04568"
  },
  {
    "id": "taneja2025llm",
    "title": "Llm-vectorizer: Llm-based verified loop vectorizer",
    "authors": "Taneja, Jubi and Laird, Avery and Yan, Cong and Musuvathi, Madan and Lahiri, Shuvendu K",
    "year": "2025",
    "source": "Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization",
    "category": "code-repair",
    "keywords": [
      "vec",
      "LLM",
      "TSVC",
      "intrinsic"
    ],
    "abstract": "Vectorization is a powerful optimization technique that significantly boosts the performance of high performance computing applications operating on large data arrays. Despite decades of research on auto-vectorization, compilers frequently miss opportunities to vectorize code. On the other hand, writing vectorized code manually using compiler intrinsics is still a complex, error-prone task that demands deep knowledge of specific architecture and compilers. In this paper, we evaluate the potential of large-language models (LLMs) to generate vectorized (Single Instruction Multiple Data) code from scalar programs that process individual array elements. We propose a novel finite-state-machine multi-agents based approach that harnesses LLMs and test-based feedback to generate vectorized code. Our findings indicate that LLMs are capable of producing high-performance vectorized code with run-time speedup ranging from 1.1x to 9.4x as compared to the state-of-the-art compilers such as Intel Compiler, GCC, and Clang. To verify the correctness of vectorized code, we use Alive2, a leading bounded translation validation tool for LLVM IR. We describe a few domain-specific techniques to improve the scalability of Alive2 on our benchmark dataset. Overall, our approach is able to verify 38.2% of vectorizations as correct on the TSVC benchmark dataset.",
    "bibtex": "@inproceedings{taneja2025llm,\n abstract = {Vectorization is a powerful optimization technique that significantly boosts the performance of high performance computing applications operating on large data arrays. Despite decades of research on auto-vectorization, compilers frequently miss opportunities to vectorize code. On the other hand, writing vectorized code manually using compiler intrinsics is still a complex, error-prone task that demands deep knowledge of specific architecture and compilers. In this paper, we evaluate the potential of large-language models (LLMs) to generate vectorized (Single Instruction Multiple Data) code from scalar programs that process individual array elements. We propose a novel finite-state-machine multi-agents based approach that harnesses LLMs and test-based feedback to generate vectorized code. Our findings indicate that LLMs are capable of producing high-performance vectorized code with run-time speedup ranging from 1.1x to 9.4x as compared to the state-of-the-art compilers such as Intel Compiler, GCC, and Clang. To verify the correctness of vectorized code, we use Alive2, a leading bounded translation validation tool for LLVM IR. We describe a few domain-specific techniques to improve the scalability of Alive2 on our benchmark dataset. Overall, our approach is able to verify 38.2% of vectorizations as correct on the TSVC benchmark dataset.},\n author = {Taneja, Jubi and Laird, Avery and Yan, Cong and Musuvathi, Madan and Lahiri, Shuvendu K},\n booktitle = {Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},\n keywords = {vec,LLM,TSVC,intrinsic},\n link = {https://dl.acm.org/doi/abs/10.1145/3696443.3708929},\n pages = {137--149},\n title = {Llm-vectorizer: Llm-based verified loop vectorizer},\n year = {2025}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3696443.3708929"
  },
  {
    "id": "ashouri2016cobayn",
    "title": "Cobayn: Compiler autotuning framework using bayesian networks",
    "authors": "Ashouri, Amir Hossein and Mariani, Giovanni and Palermo, Gianluca and Park, Eunjung and Cavazos, John and Silvano, Cristina",
    "year": "2016",
    "source": "ACM Transactions on Architecture and Code Optimization (TACO)",
    "category": "embedded&mobile",
    "keywords": [
      "Bayesian",
      "gcc",
      "embedded"
    ],
    "abstract": "The variety of today’s architectures forces programmers to spend a great deal of time porting and tuning application codes across different platforms. Compilers themselves need additional tuning, which has considerable complexity as the standard optimization levels, usually designed for the average case and the specific target architecture, often fail to bring the best results.This article proposes COBAYN: Compiler autotuning framework using BAYesian Networks, an approach for a compiler autotuning methodology using machine learning to speed up application performance and to reduce the cost of the compiler optimization phases. The proposed framework is based on the application characterization done dynamically by using independent microarchitecture features and Bayesian networks. The article also presents an evaluation based on using static analysis and hybrid feature collection approaches. In addition, the article compares Bayesian networks with respect to several state-of-the-art machine-learning models.Experiments were carried out on an ARM embedded platform and GCC compiler by considering two benchmark suites with 39 applications. The set of compiler configurations, selected by the model (less than 7% of the search space), demonstrated an application performance speedup of up to 4.6 × on Polybench (1.85 × on average) and 3.1 × on cBench (1.54 × on average) with respect to standard optimization levels. Moreover, the comparison of the proposed technique with (i) random iterative compilation, (ii) machine learning--based iterative compilation, and (iii) noniterative predictive modeling techniques shows, on average, 1.2 ×, 1.37 ×, and 1.48 × speedup, respectively. Finally, the proposed method demonstrates 4 × and 3 × speedup, respectively, on cBench and Polybench in terms of exploration efficiency given the same quality of the solutions generated by the random iterative compilation model.",
    "bibtex": "@article{ashouri2016cobayn,\n abstract = {The variety of today’s architectures forces programmers to spend a great deal of time porting and tuning application codes across different platforms. Compilers themselves need additional tuning, which has considerable complexity as the standard optimization levels, usually designed for the average case and the specific target architecture, often fail to bring the best results.This article proposes COBAYN: Compiler autotuning framework using BAYesian Networks, an approach for a compiler autotuning methodology using machine learning to speed up application performance and to reduce the cost of the compiler optimization phases. The proposed framework is based on the application characterization done dynamically by using independent microarchitecture features and Bayesian networks. The article also presents an evaluation based on using static analysis and hybrid feature collection approaches. In addition, the article compares Bayesian networks with respect to several state-of-the-art machine-learning models.Experiments were carried out on an ARM embedded platform and GCC compiler by considering two benchmark suites with 39 applications. The set of compiler configurations, selected by the model (less than 7% of the search space), demonstrated an application performance speedup of up to 4.6 × on Polybench (1.85 × on average) and 3.1 × on cBench (1.54 × on average) with respect to standard optimization levels. Moreover, the comparison of the proposed technique with (i) random iterative compilation, (ii) machine learning--based iterative compilation, and (iii) noniterative predictive modeling techniques shows, on average, 1.2 ×, 1.37 ×, and 1.48 × speedup, respectively. Finally, the proposed method demonstrates 4 × and 3 × speedup, respectively, on cBench and Polybench in terms of exploration efficiency given the same quality of the solutions generated by the random iterative compilation model.},\n author = {Ashouri, Amir Hossein and Mariani, Giovanni and Palermo, Gianluca and Park, Eunjung and Cavazos, John and Silvano, Cristina},\n journal = {ACM Transactions on Architecture and Code Optimization (TACO)},\n keywords = {Bayesian,gcc,embedded},\n link = {https://dl.acm.org/doi/abs/10.1145/2928270},\n number = {2},\n pages = {1--25},\n publisher = {ACM New York, NY, USA},\n title = {Cobayn: Compiler autotuning framework using bayesian networks},\n volume = {13},\n year = {2016}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/2928270"
  },
  {
    "id": "grubisic2024compiler",
    "title": "Compiler generated feedback for large language models",
    "authors": "Grubisic, Dejan and Cummins, Chris and Seeker, Volker and Leather, Hugh",
    "year": "2024",
    "source": "arXiv preprint arXiv:2403.14714",
    "category": "embedded&mobile",
    "keywords": [
      "codesize",
      "llm"
    ],
    "abstract": "We introduce a novel paradigm in compiler optimization powered by Large Language Models with compiler feedback to optimize the code size of LLVM assembly. The model takes unoptimized LLVM IR as input and produces optimized IR, the best optimization passes, and instruction counts of both unoptimized and optimized IRs. Then we compile the input with generated optimization passes and evaluate if the predicted instruction count is correct, generated IR is compilable, and corresponds to compiled code. We provide this feedback back to LLM and give it another chance to optimize code. This approach adds an extra 0.53% improvement over -Oz to the original model. Even though, adding more information with feedback seems intuitive, simple sampling techniques achieve much higher performance given 10 or more samples.",
    "bibtex": "@article{grubisic2024compiler,\n abstract = {We introduce a novel paradigm in compiler optimization powered by Large Language Models with compiler feedback to optimize the code size of LLVM assembly. The model takes unoptimized LLVM IR as input and produces optimized IR, the best optimization passes, and instruction counts of both unoptimized and optimized IRs. Then we compile the input with generated optimization passes and evaluate if the predicted instruction count is correct, generated IR is compilable, and corresponds to compiled code. We provide this feedback back to LLM and give it another chance to optimize code. This approach adds an extra 0.53% improvement over -Oz to the original model. Even though, adding more information with feedback seems intuitive, simple sampling techniques achieve much higher performance given 10 or more samples.},\n author = {Grubisic, Dejan and Cummins, Chris and Seeker, Volker and Leather, Hugh},\n journal = {arXiv preprint arXiv:2403.14714},\n keywords = {codesize,llm},\n link = {https://arxiv.org/abs/2403.14714},\n title = {Compiler generated feedback for large language models},\n year = {2024}\n}\n",
    "link": "https://arxiv.org/abs/2403.14714"
  },
  {
    "id": "kasampalis2021language",
    "title": "Language-parametric compiler validation with application to LLVM",
    "authors": "Kasampalis, Theodoros and Park, Daejun and Lin, Zhengyao and Adve, Vikram S and Ro{\\c{s}}u, Grigore",
    "year": "2021",
    "source": "Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
    "category": "validation",
    "keywords": [
      "translation",
      "MIR",
      "IR"
    ],
    "abstract": "We propose a new design for a Translation Validation (TV) system geared towards practical use with modern optimizing compilers, such as LLVM. Unlike existing TV systems, which are custom-tailored for a particular sequence of transformations and a specific, common language for input and output programs, our design clearly separates the transformation-specific components from the rest of the system, and generalizes the transformation-independent components. Specifically, we present Keq, the first program equivalence checker that is parametric to the input and output language semantics and has no dependence on the transformation between the input and output programs. The Keq algorithm is based on a rigorous formalization, namely cut-bisimulation, and is proven correct. We have prototyped a TV system for the Instruction Selection pass of LLVM, being able to automatically prove equivalence for translations from LLVM IR to the MachineIR used in compiling to x86-64. This transformation uses different input and output languages, and as such has not been previously addressed by the state of the art. An experimental evaluation shows that Keq successfully proves correct the translation of over 90% of 4732 supported functions in GCC from SPEC 2006.",
    "bibtex": "@inproceedings{kasampalis2021language,\n abstract = {We propose a new design for a Translation Validation (TV) system geared towards practical use with modern optimizing compilers, such as LLVM. Unlike existing TV systems, which are custom-tailored for a particular sequence of transformations and a specific, common language for input and output programs, our design clearly separates the transformation-specific components from the rest of the system, and generalizes the transformation-independent components. Specifically, we present Keq, the first program equivalence checker that is parametric to the input and output language semantics and has no dependence on the transformation between the input and output programs. The Keq algorithm is based on a rigorous formalization, namely cut-bisimulation, and is proven correct. We have prototyped a TV system for the Instruction Selection pass of LLVM, being able to automatically prove equivalence for translations from LLVM IR to the MachineIR used in compiling to x86-64. This transformation uses different input and output languages, and as such has not been previously addressed by the state of the art. An experimental evaluation shows that Keq successfully proves correct the translation of over 90% of 4732 supported functions in GCC from SPEC 2006.},\n author = {Kasampalis, Theodoros and Park, Daejun and Lin, Zhengyao and Adve, Vikram S and Ro{\\c{s}}u, Grigore},\n booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},\n keywords = {translation,MIR,IR},\n link = {https://dl.acm.org/doi/abs/10.1145/3445814.3446751},\n pages = {1004--1019},\n title = {Language-parametric compiler validation with application to LLVM},\n year = {2021}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3445814.3446751"
  },
  {
    "id": "stepp2011equality",
    "title": "Equality-based translation validator for LLVM",
    "authors": "Stepp, Michael and Tate, Ross and Lerner, Sorin",
    "year": "2011",
    "source": "Computer Aided Verification: 23rd International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings 23",
    "category": "validation",
    "keywords": [
      "SPEC"
    ],
    "abstract": "We updated our Peggy tool, previously presented in [6], to perform translation validation for the LLVM compiler using a technique called Equality Saturation. We present the tool, and illustrate its effectiveness at doing translation validation on SPEC 2006 benchmarks.",
    "bibtex": "@inproceedings{stepp2011equality,\n abstract = {We updated our Peggy tool, previously presented in [6], to perform translation validation for the LLVM compiler using a technique called Equality Saturation. We present the tool, and illustrate its effectiveness at doing translation validation on SPEC 2006 benchmarks.},\n author = {Stepp, Michael and Tate, Ross and Lerner, Sorin},\n booktitle = {Computer Aided Verification: 23rd International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings 23},\n keywords = {SPEC},\n link = {https://link.springer.com/chapter/10.1007/978-3-642-22110-1_59},\n organization = {Springer},\n pages = {737--742},\n title = {Equality-based translation validator for LLVM},\n year = {2011}\n}\n",
    "link": "https://link.springer.com/chapter/10.1007/978-3-642-22110-1_59"
  },
  {
    "id": "lopes2021alive2",
    "title": "Alive2: bounded translation validation for LLVM",
    "authors": "Lopes, Nuno P and Lee, Juneyoung and Hur, Chung-Kil and Liu, Zhengyang and Regehr, John",
    "year": "2021",
    "source": "Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
    "category": "validation",
    "keywords": [
      "SMT",
      "alive2"
    ],
    "abstract": "We designed, implemented, and deployed Alive2: a bounded translation validation tool for the LLVM compiler’s intermediate representation (IR). It limits resource consumption by, for example, unrolling loops up to some bound, which means there are circumstances in which it misses bugs. Alive2 is designed to avoid false alarms, is fully automatic through the use of an SMT solver, and requires no changes to LLVM. By running Alive2 over LLVM’s unit test suite, we discovered and reported 47 new bugs, 28 of which have been fixed already. Moreover, our work has led to eight patches to the LLVM Language Reference—the definitive description of the semantics of its IR—and we have participated in numerous discussions with the goal of clarifying ambiguities and fixing errors in these semantics. Alive2 is open source and we also made it available on the web, where it has active users from the LLVM community.",
    "bibtex": "@inproceedings{lopes2021alive2,\n abstract = {We designed, implemented, and deployed Alive2: a bounded translation validation tool for the LLVM compiler’s intermediate representation (IR). It limits resource consumption by, for example, unrolling loops up to some bound, which means there are circumstances in which it misses bugs. Alive2 is designed to avoid false alarms, is fully automatic through the use of an SMT solver, and requires no changes to LLVM. By running Alive2 over LLVM’s unit test suite, we discovered and reported 47 new bugs, 28 of which have been fixed already. Moreover, our work has led to eight patches to the LLVM Language Reference—the definitive description of the semantics of its IR—and we have participated in numerous discussions with the goal of clarifying ambiguities and fixing errors in these semantics. Alive2 is open source and we also made it available on the web, where it has active users from the LLVM community.},\n author = {Lopes, Nuno P and Lee, Juneyoung and Hur, Chung-Kil and Liu, Zhengyang and Regehr, John},\n booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},\n keywords = {SMT,alive2},\n link = {https://dl.acm.org/doi/abs/10.1145/3453483.3454030},\n pages = {65--79},\n title = {Alive2: bounded translation validation for LLVM},\n year = {2021}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3453483.3454030"
  },
  {
    "id": "zhang2022heterogen",
    "title": "HeteroGen: transpiling C to heterogeneous HLS code with automated test generation and program repair",
    "authors": "Zhang, Qian and Wang, Jiyuan and Xu, Guoqing Harry and Kim, Miryung",
    "year": "2022",
    "source": "Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
    "category": "transpiling",
    "keywords": [
      "HLS",
      "transpile",
      "C",
      "APR"
    ],
    "abstract": "Despite the trend of incorporating heterogeneity and specialization in hardware, the development of heterogeneous applications is limited to a handful of engineers with deep hardware expertise. We propose HeteroGen that takes C/C++ code as input and automatically generates an HLS version with test behavior preservation and better performance. Key to the success of HeteroGen is adapting the idea of search-based program repair to the heterogeneous computing domain, while addressing two technical challenges. First, the turn-around time of HLS compilation and simulation is much longer than the usual C/C++ compilation and execution time; therefore, HeteroGen applies pattern-oriented program edits guided by common fix patterns and their dependences. Second, behavior and performance checking requires testing, but test cases are often unavailable. Thus, HeteroGen auto-generates test inputs suitable for checking C to HLS-C conversion errors, while providing high branch coverage for the original C code. An evaluation of HeteroGen shows that it produces an HLS-compatible version for nine out of ten real-world heterogeneous applications fully automatically, applying up to 438 lines of edits to produce an HLS version 1.63x faster than the original version.",
    "bibtex": "@inproceedings{zhang2022heterogen,\n abstract = {Despite the trend of incorporating heterogeneity and specialization in hardware, the development of heterogeneous applications is limited to a handful of engineers with deep hardware expertise. We propose HeteroGen that takes C/C++ code as input and automatically generates an HLS version with test behavior preservation and better performance. Key to the success of HeteroGen is adapting the idea of search-based program repair to the heterogeneous computing domain, while addressing two technical challenges. First, the turn-around time of HLS compilation and simulation is much longer than the usual C/C++ compilation and execution time; therefore, HeteroGen applies pattern-oriented program edits guided by common fix patterns and their dependences. Second, behavior and performance checking requires testing, but test cases are often unavailable. Thus, HeteroGen auto-generates test inputs suitable for checking C to HLS-C conversion errors, while providing high branch coverage for the original C code. An evaluation of HeteroGen shows that it produces an HLS-compatible version for nine out of ten real-world heterogeneous applications fully automatically, applying up to 438 lines of edits to produce an HLS version 1.63x faster than the original version.},\n author = {Zhang, Qian and Wang, Jiyuan and Xu, Guoqing Harry and Kim, Miryung},\n booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},\n keywords = {HLS,transpile,C,APR},\n link = {https://dl.acm.org/doi/10.1145/3503222.3507748},\n pages = {1017--1029},\n title = {HeteroGen: transpiling C to heterogeneous HLS code with automated test generation and program repair},\n year = {2022}\n}\n",
    "link": "https://dl.acm.org/doi/10.1145/3503222.3507748"
  }
]