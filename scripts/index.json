[
  {
    "id": "leather2020machine",
    "title": "Machine learning in compilers: Past, present and future",
    "authors": "Leather, Hugh and Cummins, Chris",
    "year": "2020",
    "source": "2020 Forum for Specification and Design Languages (FDL)",
    "category": "survey",
    "keywords": [],
    "abstract": "Writing optimising compilers is difficult. The range of programs that may be presented to the compiler is huge and the systems on which they run are complex, heterogeneous, non-deterministic, and constantly changing. The space of possible optimisations is also vast, making it very hard for compiler writers to design heuristics that take all of these considerations into account. As a result, many compiler optimisations are out of date or poorly tuned. Near the turn of the century it was first shown how compilers could be made to automatically search the optimisation space, producing programs far better optimised than previously possible, and without the need for compiler writers to worry about architecture or program specifics. The searches, though, were slow, so in the years that followed, machine learning was developed to learn heuristics from the results of previous searches so that thereafter the search could be avoided and much of the benefit could be gained in a single shot. In this paper we will give a retrospective of machine learning in compiler optimisation from its earliest inception, through some of the works that set themselves apart, to today's deep learning, finishing with our vision of the field's future.",
    "bibtex": "@inproceedings{leather2020machine,\n abstract = {Writing optimising compilers is difficult. The range of programs that may be presented to the compiler is huge and the systems on which they run are complex, heterogeneous, non-deterministic, and constantly changing. The space of possible optimisations is also vast, making it very hard for compiler writers to design heuristics that take all of these considerations into account. As a result, many compiler optimisations are out of date or poorly tuned. Near the turn of the century it was first shown how compilers could be made to automatically search the optimisation space, producing programs far better optimised than previously possible, and without the need for compiler writers to worry about architecture or program specifics. The searches, though, were slow, so in the years that followed, machine learning was developed to learn heuristics from the results of previous searches so that thereafter the search could be avoided and much of the benefit could be gained in a single shot. In this paper we will give a retrospective of machine learning in compiler optimisation from its earliest inception, through some of the works that set themselves apart, to today's deep learning, finishing with our vision of the field's future.},\n author = {Leather, Hugh and Cummins, Chris},\n booktitle = {2020 Forum for Specification and Design Languages (FDL)},\n link = {https://doi.org/10.1109/FDL50818.2020.9232934},\n organization = {IEEE},\n pages = {1--8},\n title = {Machine learning in compilers: Past, present and future},\n year = {2020}\n}\n",
    "link": "https://doi.org/10.1109/FDL50818.2020.9232934"
  },
  {
    "id": "wang2018machine",
    "title": "Machine learning in compiler optimisation",
    "authors": "Wang, Zheng and O'Boyle, Michael",
    "year": "2018",
    "source": "arXiv preprint arXiv:1805.03441",
    "category": "survey",
    "keywords": [],
    "abstract": "In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements.\n",
    "bibtex": "@article{wang2018machine,\n abstract = {In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements.\n},\n author = {Wang, Zheng and O'Boyle, Michael},\n journal = {arXiv preprint arXiv:1805.03441},\n link = {https://doi.org/10.1109/JPROC.2018.2817118},\n title = {Machine learning in compiler optimisation},\n year = {2018}\n}\n",
    "link": "https://doi.org/10.1109/JPROC.2018.2817118"
  },
  {
    "id": "ashouri2018survey",
    "title": "A survey on compiler autotuning using machine learning",
    "authors": "Ashouri, Amir H and Killian, William and Cavazos, John and Palermo, Gianluca and Silvano, Cristina",
    "year": "2018",
    "source": "ACM Computing Surveys (CSUR)",
    "category": "survey",
    "keywords": [],
    "abstract": "Since the mid-1990s, researchers have been trying to use machine-learning-based approaches to solve a number of different compiler optimization problems. These techniques primarily enhance the quality of the obtained results and, more importantly, make it feasible to tackle two main compiler optimization problems: optimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of applying optimizations). The compiler optimization space continues to grow due to the advancement of applications, increasing number of compiler optimizations, and new target architectures. Generic optimization passes in compilers cannot fully leverage newly introduced optimizations and, therefore, cannot keep up with the pace of increasing options. This survey summarizes and classifies the recent advances in using machine learning for the compiler optimization field, particularly on the two major problems of (1) selecting the best optimizations, and (2) the phase-ordering of optimizations. The survey highlights the approaches taken so far, the obtained results, the fine-grain classification among different approaches, and finally, the influential papers of the field.",
    "bibtex": "@article{ashouri2018survey,\n abstract = {Since the mid-1990s, researchers have been trying to use machine-learning-based approaches to solve a number of different compiler optimization problems. These techniques primarily enhance the quality of the obtained results and, more importantly, make it feasible to tackle two main compiler optimization problems: optimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of applying optimizations). The compiler optimization space continues to grow due to the advancement of applications, increasing number of compiler optimizations, and new target architectures. Generic optimization passes in compilers cannot fully leverage newly introduced optimizations and, therefore, cannot keep up with the pace of increasing options. This survey summarizes and classifies the recent advances in using machine learning for the compiler optimization field, particularly on the two major problems of (1) selecting the best optimizations, and (2) the phase-ordering of optimizations. The survey highlights the approaches taken so far, the obtained results, the fine-grain classification among different approaches, and finally, the influential papers of the field.},\n author = {Ashouri, Amir H and Killian, William and Cavazos, John and Palermo, Gianluca and Silvano, Cristina},\n journal = {ACM Computing Surveys (CSUR)},\n link = {https://doi.org/10.1145/3197978},\n number = {5},\n pages = {1--42},\n publisher = {ACM New York, NY, USA},\n title = {A survey on compiler autotuning using machine learning},\n volume = {51},\n year = {2018}\n}\n",
    "link": "https://doi.org/10.1145/3197978"
  },
  {
    "id": "ashouri2016compiler",
    "title": "Compiler autotuning using machine learning techniques",
    "authors": "Ashouri, Amir Hossein",
    "year": "2016",
    "source": "",
    "category": "survey",
    "keywords": [],
    "abstract": "Recent developments in silicon production and fabrication led to the creation of much faster computational units such as CPUs, GPUs, FPGAs, and similar chips with varying instruction set architectures (ISAs). Software (SW) programming paradigms including OpenMP, MPI, OpenCL, and OpenACC allow software developers to exploit Hardware (HW) parallelism to port legacy serial codes on these emerging platforms to attain application speedups. Compilers struggle to keep up with the increasing development pace of ever-expanding hardware and software programming paradigms. Additionally, growing complexity of the modern compilers and the concern over security are among the more serious problems that compilers should answer. Moore’s law states that transistor density should double every two years; however, the rate of compilers, which are faced with many open-research problems, have not been able to improve more than a few percentage points each year. Diversity of today’s architectures have forced programmers to spend additional ef- fort to port and tune their application code across different platforms. Compilers within this process need additional tuning which is a hard task itself. Recent compilers of- fer a vast number of multilayered optimizations, capable of targeting different code segments of an application. Choosing among these optimizations can significantly im- pact the performance of the code being optimized. The selection of the right set of compiler optimizations for a particular code segment is a very hard problem, but find- ing the best ordering of these optimizations adds further complexity. In fact, finding the best ordering is a long standing problem in compilation research called the phase- ordering problem. The traditional approach of constructing compiler heuristics to solve this problem simply can not cope with the enormous complexity of choosing the right ordering of optimizations for every code segment in an application. In this PhD thesis, we provide break-through approaches to tackle and mitigate the well-known problems of compiler optimization using design space exploration and ma- chine learning techniques. We show that not all the optimization passes are beneficial to be used within an optimization sequence and in fact many of the available passes are obliterating the effect of one another when ordering of the phases are taken into account. Experimental results show major improvement in performance metrics when our customized prediction models are in place versus standard fixed optimization passes predefined within state-of-the-art compiler frameworks e.g. GCC, LLVM, etc. We per- form application specific optimization based on the characteristics of applications under analysis and we show that this methodology is beneficial to mitigate the hard problem of selecting the best compiler optimizations and the phase-ordering problem. Late but not least, we hope that the proposed approaches in this PhD thesis will be useful for a wide range of readers, including computer architects, compiler developers, researchers and technical professionals.",
    "bibtex": "@article{ashouri2016compiler,\n abstract = {Recent developments in silicon production and fabrication led to the creation of much faster computational units such as CPUs, GPUs, FPGAs, and similar chips with varying instruction set architectures (ISAs). Software (SW) programming paradigms including OpenMP, MPI, OpenCL, and OpenACC allow software developers to exploit Hardware (HW) parallelism to port legacy serial codes on these emerging platforms to attain application speedups. Compilers struggle to keep up with the increasing development pace of ever-expanding hardware and software programming paradigms. Additionally, growing complexity of the modern compilers and the concern over security are among the more serious problems that compilers should answer. Moore’s law states that transistor density should double every two years; however, the rate of compilers, which are faced with many open-research problems, have not been able to improve more than a few percentage points each year. Diversity of today’s architectures have forced programmers to spend additional ef- fort to port and tune their application code across different platforms. Compilers within this process need additional tuning which is a hard task itself. Recent compilers of- fer a vast number of multilayered optimizations, capable of targeting different code segments of an application. Choosing among these optimizations can significantly im- pact the performance of the code being optimized. The selection of the right set of compiler optimizations for a particular code segment is a very hard problem, but find- ing the best ordering of these optimizations adds further complexity. In fact, finding the best ordering is a long standing problem in compilation research called the phase- ordering problem. The traditional approach of constructing compiler heuristics to solve this problem simply can not cope with the enormous complexity of choosing the right ordering of optimizations for every code segment in an application. In this PhD thesis, we provide break-through approaches to tackle and mitigate the well-known problems of compiler optimization using design space exploration and ma- chine learning techniques. We show that not all the optimization passes are beneficial to be used within an optimization sequence and in fact many of the available passes are obliterating the effect of one another when ordering of the phases are taken into account. Experimental results show major improvement in performance metrics when our customized prediction models are in place versus standard fixed optimization passes predefined within state-of-the-art compiler frameworks e.g. GCC, LLVM, etc. We per- form application specific optimization based on the characteristics of applications under analysis and we show that this methodology is beneficial to mitigate the hard problem of selecting the best compiler optimizations and the phase-ordering problem. Late but not least, we hope that the proposed approaches in this PhD thesis will be useful for a wide range of readers, including computer architects, compiler developers, researchers and technical professionals.},\n author = {Ashouri, Amir Hossein},\n link = {https://www.politesi.polimi.it/handle/10589/129561},\n publisher = {Politecnico di Milano},\n title = {Compiler autotuning using machine learning techniques},\n year = {2016}\n}\n",
    "link": "https://www.politesi.polimi.it/handle/10589/129561"
  },
  {
    "id": "mithul2024exploring",
    "title": "Exploring Compiler Optimization: A Survey of ML, DL and RL Techniques",
    "authors": "Mithul, C and Abdulla, D Mohammad and Virinchi, M Hari and Sathvik, M and Belwal, Meena",
    "year": "2024",
    "source": "2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS)",
    "category": "survey",
    "keywords": [],
    "abstract": "The past few years, traditional compiler optimization methods have been found to be further enhanced by machine learning (ML), deep learning (DL) and reinforcement learning (RL). These differ from classical techniques that often use rule of thumb based decision making. Rather, ML/DL/RL based approaches provide a means for learning from data thus improving performance in different dimensions such as code generation, resource allocation and runtime. In this paper we give an overview of current research and methodologies utilizing ML, DL and RL for compiler optimization purposes. We analyze the major models in terms of their employed learning strategies and desired optimizations within a compiler framework. Moreover, we highlight some of the difficulties faced when these compilers are embedded with these learning models such as adaptability, generalization and overhead trade-offs. Additionally, our survey presents case studies demonstrating Quantitative improvements on well-known benchmarks mainly focusing on models' adaptability to different architectures and their role in supporting the decision-making process of compilers. We conclude outlining open research questions as well as possible future directions for further investigations into this emerging interdisciplinary field.",
    "bibtex": "@inproceedings{mithul2024exploring,\n abstract = {The past few years, traditional compiler optimization methods have been found to be further enhanced by machine learning (ML), deep learning (DL) and reinforcement learning (RL). These differ from classical techniques that often use rule of thumb based decision making. Rather, ML/DL/RL based approaches provide a means for learning from data thus improving performance in different dimensions such as code generation, resource allocation and runtime. In this paper we give an overview of current research and methodologies utilizing ML, DL and RL for compiler optimization purposes. We analyze the major models in terms of their employed learning strategies and desired optimizations within a compiler framework. Moreover, we highlight some of the difficulties faced when these compilers are embedded with these learning models such as adaptability, generalization and overhead trade-offs. Additionally, our survey presents case studies demonstrating Quantitative improvements on well-known benchmarks mainly focusing on models' adaptability to different architectures and their role in supporting the decision-making process of compilers. We conclude outlining open research questions as well as possible future directions for further investigations into this emerging interdisciplinary field.},\n author = {Mithul, C and Abdulla, D Mohammad and Virinchi, M Hari and Sathvik, M and Belwal, Meena},\n booktitle = {2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS)},\n link = {https://doi.org/10.1109/CSITSS64042.2024.10816929},\n organization = {IEEE},\n pages = {1--6},\n title = {Exploring Compiler Optimization: A Survey of ML, DL and RL Techniques},\n year = {2024}\n}\n",
    "link": "https://doi.org/10.1109/CSITSS64042.2024.10816929"
  },
  {
    "id": "pandey2024survey",
    "title": "A Survey of Optimized Compiler Using Advanced Machine learning and Deep Learning Techniques",
    "authors": "Pandey, Lal Bahadur and Sharma, Manisha and Tiwari, Rajesh and Panda, Radhe Shyam and Roy, Partha",
    "year": "2024",
    "source": "2024 IEEE 6th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)",
    "category": "survey",
    "keywords": [],
    "abstract": "Optimizing compilers is a difficult and time-consuming task, especially when done by hand. As far as we know, the compiler handles both translation and optimization. An efficient compiler system can become more automated and simple, as evidenced by recent studies using deep learning and machine learning approaches. Model training, prediction, optimization, and feature selection are handled by most machine learning and deep learning methods. In this case, choosing the optimal characteristics is necessary in order to use deep learning and machine learning techniques to enhance the optimization quality. This study examines various approaches that might be utilized to enhance and refine the quality of the chosen heuristics as well as the general quality of machine learning and deep learning models in order to boost the compiler's efficiency. The phase-ordering problem, the amount of iterative program evaluations, and the time needed to obtain the best forecast are only a few of the many subjects covered by these approaches.",
    "bibtex": "@inproceedings{pandey2024survey,\n abstract = {Optimizing compilers is a difficult and time-consuming task, especially when done by hand. As far as we know, the compiler handles both translation and optimization. An efficient compiler system can become more automated and simple, as evidenced by recent studies using deep learning and machine learning approaches. Model training, prediction, optimization, and feature selection are handled by most machine learning and deep learning methods. In this case, choosing the optimal characteristics is necessary in order to use deep learning and machine learning techniques to enhance the optimization quality. This study examines various approaches that might be utilized to enhance and refine the quality of the chosen heuristics as well as the general quality of machine learning and deep learning models in order to boost the compiler's efficiency. The phase-ordering problem, the amount of iterative program evaluations, and the time needed to obtain the best forecast are only a few of the many subjects covered by these approaches.},\n author = {Pandey, Lal Bahadur and Sharma, Manisha and Tiwari, Rajesh and Panda, Radhe Shyam and Roy, Partha},\n booktitle = {2024 IEEE 6th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)},\n link = {https://doi.org/10.1109/ICCCMLA63077.2024.10871813},\n organization = {IEEE},\n pages = {256--259},\n title = {A Survey of Optimized Compiler Using Advanced Machine learning and Deep Learning Techniques},\n year = {2024}\n}\n",
    "link": "https://doi.org/10.1109/ICCCMLA63077.2024.10871813"
  },
  {
    "id": "wu2022survey",
    "title": "A survey of machine learning for computer architecture and systems",
    "authors": "Wu, Nan and Xie, Yuan",
    "year": "2022",
    "source": "ACM Computing Surveys (CSUR)",
    "category": "survey",
    "keywords": [],
    "abstract": "It has been a long time that computer architecture and systems are optimized for efficient execution of machine learning (ML) models. Now, it is time to reconsider the relationship between ML and systems and let ML transform the way that computer architecture and systems are designed. This embraces a twofold meaning: improvement of designers’ productivity and completion of the virtuous cycle. In this article, we present a comprehensive review of the work that applies ML for computer architecture and system design. First, we perform a high-level taxonomy by considering the typical role that ML techniques take in architecture/system design, i.e., either for fast predictive modeling or as the design methodology. Then, we summarize the common problems in computer architecture/system design that can be solved by ML techniques and the typical ML techniques employed to resolve each of them. In addition to emphasis on computer architecture in a narrow sense, we adopt the concept that data centers can be recognized as warehouse-scale computers; sketchy discussions are provided in adjacent computer systems, such as code generation and compiler; we also give attention to how ML techniques can aid and transform design automation. We further provide a future vision of opportunities and potential directions and envision that applying ML for computer architecture and systems would thrive in the community.",
    "bibtex": "@article{wu2022survey,\n abstract = {It has been a long time that computer architecture and systems are optimized for efficient execution of machine learning (ML) models. Now, it is time to reconsider the relationship between ML and systems and let ML transform the way that computer architecture and systems are designed. This embraces a twofold meaning: improvement of designers’ productivity and completion of the virtuous cycle. In this article, we present a comprehensive review of the work that applies ML for computer architecture and system design. First, we perform a high-level taxonomy by considering the typical role that ML techniques take in architecture/system design, i.e., either for fast predictive modeling or as the design methodology. Then, we summarize the common problems in computer architecture/system design that can be solved by ML techniques and the typical ML techniques employed to resolve each of them. In addition to emphasis on computer architecture in a narrow sense, we adopt the concept that data centers can be recognized as warehouse-scale computers; sketchy discussions are provided in adjacent computer systems, such as code generation and compiler; we also give attention to how ML techniques can aid and transform design automation. We further provide a future vision of opportunities and potential directions and envision that applying ML for computer architecture and systems would thrive in the community.},\n author = {Wu, Nan and Xie, Yuan},\n journal = {ACM Computing Surveys (CSUR)},\n link = {https://doi.org/10.1145/3494523},\n number = {3},\n pages = {1--39},\n publisher = {ACM New York, NY},\n title = {A survey of machine learning for computer architecture and systems},\n volume = {55},\n year = {2022}\n}\n",
    "link": "https://doi.org/10.1145/3494523"
  },
  {
    "id": "zheng2021tenset",
    "title": "Tenset: A large-scale program performance dataset for learned tensor compilers",
    "authors": "Zheng, Lianmin and Liu, Ruochen and Shao, Junru and Chen, Tianqi and Gonzalez, Joseph E and Stoica, Ion and Ali, Ameer Haj",
    "year": "2021",
    "source": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)",
    "category": "benchmarks&datasets",
    "keywords": [
      "dataset",
      "tensor"
    ],
    "abstract": "Search-based tensor compilers can greatly accelerate the execution of machine learning models by generating high-performance tensor programs, such as matrix multiplications and convolutions. These compilers take a high-level mathematical expression as input and search for the fastest low-level implementations. At the core of the search procedure is a cost model which estimates the performance of different candidates to reduce the frequency of time-consuming on-device measurements. There has been a growing interest in using machine learning techniques to learn a cost model to ease the effort of building an analytical model. However, a standard dataset for pre-training and benchmarking learned cost models is lacking.\n",
    "bibtex": "@inproceedings{zheng2021tenset,\n abstract = {Search-based tensor compilers can greatly accelerate the execution of machine learning models by generating high-performance tensor programs, such as matrix multiplications and convolutions. These compilers take a high-level mathematical expression as input and search for the fastest low-level implementations. At the core of the search procedure is a cost model which estimates the performance of different candidates to reduce the frequency of time-consuming on-device measurements. There has been a growing interest in using machine learning techniques to learn a cost model to ease the effort of building an analytical model. However, a standard dataset for pre-training and benchmarking learned cost models is lacking.\n},\n author = {Zheng, Lianmin and Liu, Ruochen and Shao, Junru and Chen, Tianqi and Gonzalez, Joseph E and Stoica, Ion and Ali, Ameer Haj},\n booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},\n keywords = {dataset,tensor},\n link = {https://openreview.net/forum?id=aIfp8kLuvc9},\n title = {Tenset: A large-scale program performance dataset for learned tensor compilers},\n year = {2021}\n}\n",
    "link": "https://openreview.net/forum?id=aIfp8kLuvc9"
  },
  {
    "id": "puri2021codenet",
    "title": "CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks",
    "authors": "Puri, Ruchir and Kung, David and Janssen, Geert and Zhang, Wei and Domeniconi, Giacomo and Zolotov, Vladimir and Dolby, Julian and Chen, Jie and Choudhury, Mihir and Decker, Lindsey and others",
    "year": "2021",
    "source": "Annual Conference on Neural Information Processing Systems",
    "category": "benchmarks&datasets",
    "keywords": [
      "dataset"
    ],
    "abstract": "Over the last several decades, software has been woven into the fabric of every aspect of our society. As software development surges and code infrastructure of enterprise applications ages, it is now more critical than ever to increase software development productivity and modernize legacy applications. Advances in deep learning and machine learning algorithms have enabled breakthroughs in computer vision, speech recognition, natural language processing and beyond, motivating researchers to leverage AI techniques to improve software development efficiency. Thus, the fast-emerging research area of “AI for Code” has garnered new interest and gathered momentum. In this paper, we present a large-scale dataset CodeNet, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code. In addition to its large scale, CodeNet has a rich set of high-quality annotations to benchmark and help accelerate research in AI techniques for a variety of critical coding tasks, including code similarity and classification, code translation between a large variety of programming languages, and code performance (runtime and memory) improvement techniques. Additionally, CodeNet provides sample input and output test sets for 98.5% of the code samples, which can be used as an oracle for determining code correctness and potentially guide reinforcement learning for code quality improvements. As a usability feature, we provide several pre-processing tools in CodeNet to transform source code into representations that can be readily used as inputs into machine learning models. Results of code classification and code similarity experiments using the CodeNet dataset are provided as a reference. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer unprecedented research opportunities at the intersection of AI and Software Engineering.",
    "bibtex": "@inproceedings{puri2021codenet,\n abstract = {Over the last several decades, software has been woven into the fabric of every aspect of our society. As software development surges and code infrastructure of enterprise applications ages, it is now more critical than ever to increase software development productivity and modernize legacy applications. Advances in deep learning and machine learning algorithms have enabled breakthroughs in computer vision, speech recognition, natural language processing and beyond, motivating researchers to leverage AI techniques to improve software development efficiency. Thus, the fast-emerging research area of “AI for Code” has garnered new interest and gathered momentum. In this paper, we present a large-scale dataset CodeNet, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code. In addition to its large scale, CodeNet has a rich set of high-quality annotations to benchmark and help accelerate research in AI techniques for a variety of critical coding tasks, including code similarity and classification, code translation between a large variety of programming languages, and code performance (runtime and memory) improvement techniques. Additionally, CodeNet provides sample input and output test sets for 98.5% of the code samples, which can be used as an oracle for determining code correctness and potentially guide reinforcement learning for code quality improvements. As a usability feature, we provide several pre-processing tools in CodeNet to transform source code into representations that can be readily used as inputs into machine learning models. Results of code classification and code similarity experiments using the CodeNet dataset are provided as a reference. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer unprecedented research opportunities at the intersection of AI and Software Engineering.},\n author = {Puri, Ruchir and Kung, David and Janssen, Geert and Zhang, Wei and Domeniconi, Giacomo and Zolotov, Vladimir and Dolby, Julian and Chen, Jie and Choudhury, Mihir and Decker, Lindsey and others},\n booktitle = {Annual Conference on Neural Information Processing Systems},\n keywords = {dataset},\n link = {https://research.ibm.com/publications/project-codenet-a-large-scale-ai-for-code-dataset-for-learning-a-diversity-of-coding-tasks},\n title = {CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks},\n year = {2021}\n}\n",
    "link": "https://research.ibm.com/publications/project-codenet-a-large-scale-ai-for-code-dataset-for-learning-a-diversity-of-coding-tasks"
  },
  {
    "id": "da2021anghabench",
    "title": "Anghabench: A suite with one million compilable c benchmarks for code-size reduction",
    "authors": "Da Silva, Anderson Faustino and Kind, Bruno Conde and de Souza Magalh{\\~a}es, Jos{\\'e} Wesley and Rocha, Jer{\\^o}nimo Nunes and Guimaraes, Breno Campos Ferreira and Pereira, Fernando Magno Quin{\\~a}o",
    "year": "2021",
    "source": "2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)",
    "category": "benchmarks&datasets",
    "keywords": [
      "dataset",
      "codesize",
      "C"
    ],
    "abstract": "A predictive compiler uses properties of a program to decide how to optimize it. The compiler is trained on a collection of programs to derive a model which determines its actions in face of unknown codes. One of the challenges of predictive compilation is how to find good training sets. Regardless of the programming language, the availability of human-made benchmarks is limited. Moreover, current synthesizers produce code that is very different from actual programs, and mining compilable code from open repositories is difficult, due to program dependencies. In this paper, we use a combination of web crawling and type inference to overcome these problems for the C programming language. We use a type reconstructor based on Hindley-Milner's algorithm to produce ANGHABENCH, a virtually unlimited collection of real-world compilable C programs. Although ANGHABENCH programs are not executable, they can be transformed into object files by any C compliant compiler. Therefore, they can be used to train compilers for code size reduction. We have used thousands of ANGHABENCH programs to train YACOS, a predictive compiler based on LLVM. The version of YACOS autotuned with ANGHABENCH generates binaries for the LLVM test suite over 10% smaller than clang -Oz. It compresses code impervious even to the state-of-the-art Function Sequence Alignment technique published in 2019, as it does not require large binaries to work well.",
    "bibtex": "@inproceedings{da2021anghabench,\n abstract = {A predictive compiler uses properties of a program to decide how to optimize it. The compiler is trained on a collection of programs to derive a model which determines its actions in face of unknown codes. One of the challenges of predictive compilation is how to find good training sets. Regardless of the programming language, the availability of human-made benchmarks is limited. Moreover, current synthesizers produce code that is very different from actual programs, and mining compilable code from open repositories is difficult, due to program dependencies. In this paper, we use a combination of web crawling and type inference to overcome these problems for the C programming language. We use a type reconstructor based on Hindley-Milner's algorithm to produce ANGHABENCH, a virtually unlimited collection of real-world compilable C programs. Although ANGHABENCH programs are not executable, they can be transformed into object files by any C compliant compiler. Therefore, they can be used to train compilers for code size reduction. We have used thousands of ANGHABENCH programs to train YACOS, a predictive compiler based on LLVM. The version of YACOS autotuned with ANGHABENCH generates binaries for the LLVM test suite over 10% smaller than clang -Oz. It compresses code impervious even to the state-of-the-art Function Sequence Alignment technique published in 2019, as it does not require large binaries to work well.},\n author = {Da Silva, Anderson Faustino and Kind, Bruno Conde and de Souza Magalh{\\~a}es, Jos{\\'e} Wesley and Rocha, Jer{\\^o}nimo Nunes and Guimaraes, Breno Campos Ferreira and Pereira, Fernando Magno Quin{\\~a}o},\n booktitle = {2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},\n keywords = {dataset,codesize,C},\n link = {https://ieeexplore.ieee.org/abstract/document/9370322},\n organization = {IEEE},\n pages = {378--390},\n title = {Anghabench: A suite with one million compilable c benchmarks for code-size reduction},\n year = {2021}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/9370322"
  },
  {
    "id": "armengol2022exebench",
    "title": "ExeBench: an ML-scale dataset of executable C functions",
    "authors": "Armengol-Estap{\\'e}, Jordi and Woodruff, Jackson and Brauckmann, Alexander and Magalh{\\~a}es, Jos{\\'e} Wesley de Souza and O'Boyle, Michael FP",
    "year": "2022",
    "source": "Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming",
    "category": "benchmarks&datasets",
    "keywords": [
      "dataset"
    ],
    "abstract": "Machine-learning promises to transform compilation and software engineering, yet is frequently limited by the scope of available datasets. In particular, there is a lack of runnable, real-world datasets required for a range of tasks ranging from neural program synthesis to machine learning-guided program optimization. We introduce a new dataset, ExeBench, which attempts to address this. It tackles two key issues with real-world code: references to external types and functions and scalable generation of IO examples. ExeBench is the first publicly available dataset that pairs real-world C code taken from GitHub with IO examples that allow these programs to be run. We develop a toolchain that scrapes GitHub, analyzes the code, and generates runnable snippets of code. We analyze our benchmark suite using several metrics, and show it is representative of real-world code. ExeBench contains 4.5M compilable and 700k executable C functions. This scale of executable, real functions will enable the next generation of machine learning-based programming tasks.",
    "bibtex": "@inproceedings{armengol2022exebench,\n abstract = {Machine-learning promises to transform compilation and software engineering, yet is frequently limited by the scope of available datasets. In particular, there is a lack of runnable, real-world datasets required for a range of tasks ranging from neural program synthesis to machine learning-guided program optimization. We introduce a new dataset, ExeBench, which attempts to address this. It tackles two key issues with real-world code: references to external types and functions and scalable generation of IO examples. ExeBench is the first publicly available dataset that pairs real-world C code taken from GitHub with IO examples that allow these programs to be run. We develop a toolchain that scrapes GitHub, analyzes the code, and generates runnable snippets of code. We analyze our benchmark suite using several metrics, and show it is representative of real-world code. ExeBench contains 4.5M compilable and 700k executable C functions. This scale of executable, real functions will enable the next generation of machine learning-based programming tasks.},\n author = {Armengol-Estap{\\'e}, Jordi and Woodruff, Jackson and Brauckmann, Alexander and Magalh{\\~a}es, Jos{\\'e} Wesley de Souza and O'Boyle, Michael FP},\n booktitle = {Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},\n keywords = {dataset},\n link = {https://dl.acm.org/doi/abs/10.1145/3520312.3534867},\n pages = {50--59},\n title = {ExeBench: an ML-scale dataset of executable C functions},\n year = {2022}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3520312.3534867"
  },
  {
    "id": "lu2021codexglue",
    "title": "Codexglue: A machine learning benchmark dataset for code understanding and generation",
    "authors": "Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and others",
    "year": "2021",
    "source": "arXiv preprint arXiv:2102.04664",
    "category": "benchmarks&datasets",
    "keywords": [
      "dataset"
    ],
    "abstract": "Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.",
    "bibtex": "@article{lu2021codexglue,\n abstract = {Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.},\n author = {Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and others},\n journal = {arXiv preprint arXiv:2102.04664},\n keywords = {dataset},\n link = {https://arxiv.org/abs/2102.04664},\n title = {Codexglue: A machine learning benchmark dataset for code understanding and generation},\n year = {2021}\n}\n",
    "link": "https://arxiv.org/abs/2102.04664"
  },
  {
    "id": "cummins2017end",
    "title": "End-to-end deep learning of optimization heuristics",
    "authors": "Cummins, Chris and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh",
    "year": "2017",
    "source": "2017 26th International Conference on Parallel Architectures and Compilation Techniques (PACT)",
    "category": "intrapass-tuning",
    "keywords": [
      "herustic learning"
    ],
    "abstract": "Accurate automatic optimization heuristics are necessary for dealing with thecomplexity and diversity of modern hardware and software. Machine learning is aproven technique for learning such heuristics, but its success is bound by thequality of the features used. These features must be hand crafted by developersthrough a combination of expert domain knowledge and trial and error. This makesthe quality of the final model directly dependent on the skill and availabletime of the system architect. Our work introduces a better way for building heuristics. We develop a deepneural network that learns heuristics over raw code, entirely without using codefeatures. The neural network simultaneously constructs appropriaterepresentations of the code and learns how best to optimize, removing the needfor manual feature creation. Further, we show that our neural nets can transferlearning from one optimization problem to another, improving the accuracy of newmodels, without the help of human experts. We compare the effectiveness of our automatically generated heuristics againstones with features hand-picked by experts. We examine two challenging tasks:predicting optimal mapping for heterogeneous parallelism and GPU threadcoarsening factors. In 89% of the cases, the quality of our fully automaticheuristics matches or surpasses that of state-of-the-art predictive models usinghand-crafted features, providing on average 14% and 12% more performance withno human effort expended on designing features.",
    "bibtex": "@inproceedings{cummins2017end,\n abstract = {Accurate automatic optimization heuristics are necessary for dealing with thecomplexity and diversity of modern hardware and software. Machine learning is aproven technique for learning such heuristics, but its success is bound by thequality of the features used. These features must be hand crafted by developersthrough a combination of expert domain knowledge and trial and error. This makesthe quality of the final model directly dependent on the skill and availabletime of the system architect. Our work introduces a better way for building heuristics. We develop a deepneural network that learns heuristics over raw code, entirely without using codefeatures. The neural network simultaneously constructs appropriaterepresentations of the code and learns how best to optimize, removing the needfor manual feature creation. Further, we show that our neural nets can transferlearning from one optimization problem to another, improving the accuracy of newmodels, without the help of human experts. We compare the effectiveness of our automatically generated heuristics againstones with features hand-picked by experts. We examine two challenging tasks:predicting optimal mapping for heterogeneous parallelism and GPU threadcoarsening factors. In 89% of the cases, the quality of our fully automaticheuristics matches or surpasses that of state-of-the-art predictive models usinghand-crafted features, providing on average 14% and 12% more performance withno human effort expended on designing features.},\n author = {Cummins, Chris and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},\n booktitle = {2017 26th International Conference on Parallel Architectures and Compilation Techniques (PACT)},\n keywords = {herustic learning},\n link = {https://doi.org/10.1109/PACT.2017.24},\n organization = {IEEE},\n pages = {219--232},\n title = {End-to-end deep learning of optimization heuristics},\n year = {2017}\n}\n",
    "link": "https://doi.org/10.1109/PACT.2017.24"
  },
  {
    "id": "cummins2021programl",
    "title": "Programl: A graph-based program representation for data flow analysis and compiler optimizations",
    "authors": "Cummins, Chris and Fisches, Zacharias V and Ben-Nun, Tal and Hoefler, Torsten and O’Boyle, Michael FP and Leather, Hugh",
    "year": "2021",
    "source": "International Conference on Machine Learning",
    "category": "intrapass-tuning",
    "keywords": [
      "loop",
      "dfg",
      "SVM"
    ],
    "abstract": "Machine learning (ML) is increasingly seen as a viable approach for building compiler optimization heuristics, but many ML methods cannot replicate even the simplest of the data flow analyses that are critical to making good optimization decisions. We posit that if ML cannot do that, then it is insufficiently able to reason about programs. We formulate data flow analyses as supervised learning tasks and introduce a large open dataset of programs and their corresponding labels from several analyses. We use this dataset to benchmark ML methods and show that they struggle on these fundamental program reasoning tasks. We propose ProGraML - Program Graphs for Machine Learning - a language-independent, portable representation of program semantics. ProGraML overcomes the limitations of prior works and yields improved performance on downstream optimization tasks.",
    "bibtex": "@inproceedings{cummins2021programl,\n abstract = {Machine learning (ML) is increasingly seen as a viable approach for building compiler optimization heuristics, but many ML methods cannot replicate even the simplest of the data flow analyses that are critical to making good optimization decisions. We posit that if ML cannot do that, then it is insufficiently able to reason about programs. We formulate data flow analyses as supervised learning tasks and introduce a large open dataset of programs and their corresponding labels from several analyses. We use this dataset to benchmark ML methods and show that they struggle on these fundamental program reasoning tasks. We propose ProGraML - Program Graphs for Machine Learning - a language-independent, portable representation of program semantics. ProGraML overcomes the limitations of prior works and yields improved performance on downstream optimization tasks.},\n author = {Cummins, Chris and Fisches, Zacharias V and Ben-Nun, Tal and Hoefler, Torsten and O’Boyle, Michael FP and Leather, Hugh},\n booktitle = {International Conference on Machine Learning},\n keywords = {loop,dfg,SVM},\n link = {https://proceedings.mlr.press/v139/cummins21a.html},\n organization = {PMLR},\n pages = {2244--2253},\n title = {Programl: A graph-based program representation for data flow analysis and compiler optimizations},\n year = {2021}\n}\n",
    "link": "https://proceedings.mlr.press/v139/cummins21a.html"
  },
  {
    "id": "haj2020neurovectorizer",
    "title": "Neurovectorizer: End-to-end vectorization with deep reinforcement learning",
    "authors": "Haj-Ali, Ameer and Ahmed, Nesreen K and Willke, Ted and Shao, Yakun Sophia and Asanovic, Krste and Stoica, Ion",
    "year": "2020",
    "source": "Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization",
    "category": "intrapass-tuning",
    "keywords": [
      "loop",
      "rl"
    ],
    "abstract": "One of the key challenges arising when compilers vectorize loops for today’s SIMD-compatible architectures is to decide if vectorization or interleaving is beneficial. Then, the compiler has to determine the number of instructions to pack together and the interleaving level (stride). Compilers are designed today to use fixed-cost models that are based on heuristics to make vectorization decisions on loops. However, these models are unable to capture the data dependency, the computation graph, or the organization of instructions. Alternatively, software engineers often hand-write the vectorization factors of every loop. This, however, places a huge burden on them, since it requires prior experience and significantly increases the development time.\nIn this work, we explore a novel approach for handling loop vectorization and propose an end-to-end solution using deep reinforcement learning (RL). We conjecture that deep RL can capture different instructions, dependencies, and data structures to enable learning a sophisticated model that can better predict the actual performance cost and determine the optimal vectorization factors. We develop an end-to-end framework, from code to vectorization, that integrates deep RL in the LLVM compiler. Our proposed framework takes benchmark codes as input and extracts the loop codes. These loop codes are then fed to a loop embedding generator that learns an embedding for these loops. Finally, the learned embeddings are used as input to a Deep RL agent, which dynamically determines the vectorization factors for all the loops. We further extend our framework to support random search, decision trees, supervised neural networks, and nearest-neighbor search. We evaluate our approaches against the currently used LLVM vectorizer and loop polyhedral optimization techniques. Our experiments show 1.29×−4.73× performance speedup compared to baseline and only 3% worse than the brute-force search on a wide range of benchmarks.",
    "bibtex": "@inproceedings{haj2020neurovectorizer,\n abstract = {One of the key challenges arising when compilers vectorize loops for today’s SIMD-compatible architectures is to decide if vectorization or interleaving is beneficial. Then, the compiler has to determine the number of instructions to pack together and the interleaving level (stride). Compilers are designed today to use fixed-cost models that are based on heuristics to make vectorization decisions on loops. However, these models are unable to capture the data dependency, the computation graph, or the organization of instructions. Alternatively, software engineers often hand-write the vectorization factors of every loop. This, however, places a huge burden on them, since it requires prior experience and significantly increases the development time.\nIn this work, we explore a novel approach for handling loop vectorization and propose an end-to-end solution using deep reinforcement learning (RL). We conjecture that deep RL can capture different instructions, dependencies, and data structures to enable learning a sophisticated model that can better predict the actual performance cost and determine the optimal vectorization factors. We develop an end-to-end framework, from code to vectorization, that integrates deep RL in the LLVM compiler. Our proposed framework takes benchmark codes as input and extracts the loop codes. These loop codes are then fed to a loop embedding generator that learns an embedding for these loops. Finally, the learned embeddings are used as input to a Deep RL agent, which dynamically determines the vectorization factors for all the loops. We further extend our framework to support random search, decision trees, supervised neural networks, and nearest-neighbor search. We evaluate our approaches against the currently used LLVM vectorizer and loop polyhedral optimization techniques. Our experiments show 1.29×−4.73× performance speedup compared to baseline and only 3% worse than the brute-force search on a wide range of benchmarks.},\n author = {Haj-Ali, Ameer and Ahmed, Nesreen K and Willke, Ted and Shao, Yakun Sophia and Asanovic, Krste and Stoica, Ion},\n booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},\n keywords = {loop,rl},\n link = {https://dl.acm.org/doi/abs/10.1145/3368826.3377928},\n pages = {242--255},\n title = {Neurovectorizer: End-to-end vectorization with deep reinforcement learning},\n year = {2020}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3368826.3377928"
  },
  {
    "id": "brauckmann2021polygym",
    "title": "Polygym: Polyhedral optimizations as an environment for reinforcement learning",
    "authors": "Brauckmann, Alexander and Goens, Andr{\\'e}s and Castrillon, Jeronimo",
    "year": "2021",
    "source": "2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)",
    "category": "intrapass-tuning",
    "keywords": [
      "rl",
      "poly"
    ],
    "abstract": "The polyhedral model allows a structured way of defining semantics-preserving transformations to improve the performance of a large class of loops. Finding profitable points in this space is a hard problem which is usually approached by heuristics that generalize from domain-expert knowledge. Existing search space formulations in state-of-the-art heuristics depend on the shape of particular loops, making it hard to leverage generic and more powerful optimization techniques from the machine learning domain. In this paper, we propose a shape-agnostic formulation for the space of legal transformations in the polyhedral model as a Markov Decision Process (MDP). Instead of using transformations, the formulation is based on an abstract space of possible schedules. In this formulation, states model partial schedules, which are constructed by actions that are reusable across different loops. With a simple heuristic to traverse the space, we demonstrate that our formulation is powerful enough to match and outperform state-of-the-art heuristics. On the Polybench benchmark suite, we found the search space to contain transformations that lead to a speedup of 3.39x over LLVM O3, which is 1.34x better than the best transformations found in the search space of isl, and 1.83x better than the speedup achieved by the default heuristics of isl. Our generic MDP formulation enables future work to use reinforcement learning to learn optimization heuristics over a wide range of loops. This also contributes to the emerging field of machine learning in compilers, as it exposes a novel problem formulation that can push the limits of existing methods.",
    "bibtex": "@inproceedings{brauckmann2021polygym,\n abstract = {The polyhedral model allows a structured way of defining semantics-preserving transformations to improve the performance of a large class of loops. Finding profitable points in this space is a hard problem which is usually approached by heuristics that generalize from domain-expert knowledge. Existing search space formulations in state-of-the-art heuristics depend on the shape of particular loops, making it hard to leverage generic and more powerful optimization techniques from the machine learning domain. In this paper, we propose a shape-agnostic formulation for the space of legal transformations in the polyhedral model as a Markov Decision Process (MDP). Instead of using transformations, the formulation is based on an abstract space of possible schedules. In this formulation, states model partial schedules, which are constructed by actions that are reusable across different loops. With a simple heuristic to traverse the space, we demonstrate that our formulation is powerful enough to match and outperform state-of-the-art heuristics. On the Polybench benchmark suite, we found the search space to contain transformations that lead to a speedup of 3.39x over LLVM O3, which is 1.34x better than the best transformations found in the search space of isl, and 1.83x better than the speedup achieved by the default heuristics of isl. Our generic MDP formulation enables future work to use reinforcement learning to learn optimization heuristics over a wide range of loops. This also contributes to the emerging field of machine learning in compilers, as it exposes a novel problem formulation that can push the limits of existing methods.},\n author = {Brauckmann, Alexander and Goens, Andr{\\'e}s and Castrillon, Jeronimo},\n booktitle = {2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)},\n keywords = {rl,poly},\n link = {https://doi.org/10.1109/PACT52795.2021.00009},\n organization = {IEEE},\n pages = {17--29},\n title = {Polygym: Polyhedral optimizations as an environment for reinforcement learning},\n year = {2021}\n}\n",
    "link": "https://doi.org/10.1109/PACT52795.2021.00009"
  },
  {
    "id": "trofin2021mlgo",
    "title": "Mlgo: a machine learning guided compiler optimizations framework",
    "authors": "Trofin, Mircea and Qian, Yundi and Brevdo, Eugene and Lin, Zinan and Choromanski, Krzysztof and Li, David",
    "year": "2021",
    "source": "arXiv preprint arXiv:2101.04808",
    "category": "intrapass-tuning",
    "keywords": [
      "codesize",
      "inline"
    ],
    "abstract": "Leveraging machine-learning (ML) techniques for compiler optimizations has been widely studied and explored in academia. However, the adoption of ML in general-purpose, industry strength compilers has yet to happen. We propose MLGO, a framework for integrating ML techniques systematically in an industrial compiler -- LLVM. As a case study, we present the details and results of replacing the heuristics-based inlining-for-size optimization in LLVM with machine learned models. To the best of our knowledge, this work is the first full integration of ML in a complex compiler pass in a real-world setting. It is available in the main LLVM repository. We use two different ML algorithms: Policy Gradient and Evolution Strategies, to train the inlining-for-size model, and achieve up to 7\\% size reduction, when compared to state of the art LLVM -Oz. The same model, trained on one corpus, generalizes well to a diversity of real-world targets, as well as to the same set of targets after months of active development. This property of the trained models is beneficial to deploy ML techniques in real-world settings.",
    "bibtex": "@article{trofin2021mlgo,\n abstract = {Leveraging machine-learning (ML) techniques for compiler optimizations has been widely studied and explored in academia. However, the adoption of ML in general-purpose, industry strength compilers has yet to happen. We propose MLGO, a framework for integrating ML techniques systematically in an industrial compiler -- LLVM. As a case study, we present the details and results of replacing the heuristics-based inlining-for-size optimization in LLVM with machine learned models. To the best of our knowledge, this work is the first full integration of ML in a complex compiler pass in a real-world setting. It is available in the main LLVM repository. We use two different ML algorithms: Policy Gradient and Evolution Strategies, to train the inlining-for-size model, and achieve up to 7\\% size reduction, when compared to state of the art LLVM -Oz. The same model, trained on one corpus, generalizes well to a diversity of real-world targets, as well as to the same set of targets after months of active development. This property of the trained models is beneficial to deploy ML techniques in real-world settings.},\n author = {Trofin, Mircea and Qian, Yundi and Brevdo, Eugene and Lin, Zinan and Choromanski, Krzysztof and Li, David},\n journal = {arXiv preprint arXiv:2101.04808},\n keywords = {codesize,inline},\n link = {https://arxiv.org/abs/2101.04808},\n title = {Mlgo: a machine learning guided compiler optimizations framework},\n year = {2021}\n}\n",
    "link": "https://arxiv.org/abs/2101.04808"
  },
  {
    "id": "venkatakeerthy2023rl4real",
    "title": "Rl4real: Reinforcement learning for register allocation",
    "authors": "VenkataKeerthy, S and Jain, Siddharth and Kundu, Anilava and Aggarwal, Rohit and Cohen, Albert and Upadrasta, Ramakrishna",
    "year": "2023",
    "source": "Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction",
    "category": "intrapass-tuning",
    "keywords": [
      "rl",
      "register alloc",
      "llvm-mca"
    ],
    "abstract": "We aim to automate decades of research and experience in register allocation, leveraging machine learning. We tackle this problem by embedding a multi-agent reinforcement learning algorithm within LLVM, training it with the state of the art techniques. We formalize the constraints that precisely define the problem for a given instruction-set architecture, while ensuring that the generated code preserves semantic correctness. We also develop a gRPC based framework providing a modular and efficient compiler interface for training and inference. Our approach is architecture independent: we show experimental results targeting Intel x86 and ARM AArch64. Our results match or out-perform the heavily tuned, production-grade register allocators of LLVM.",
    "bibtex": "@inproceedings{venkatakeerthy2023rl4real,\n abstract = {We aim to automate decades of research and experience in register allocation, leveraging machine learning. We tackle this problem by embedding a multi-agent reinforcement learning algorithm within LLVM, training it with the state of the art techniques. We formalize the constraints that precisely define the problem for a given instruction-set architecture, while ensuring that the generated code preserves semantic correctness. We also develop a gRPC based framework providing a modular and efficient compiler interface for training and inference. Our approach is architecture independent: we show experimental results targeting Intel x86 and ARM AArch64. Our results match or out-perform the heavily tuned, production-grade register allocators of LLVM.},\n author = {VenkataKeerthy, S and Jain, Siddharth and Kundu, Anilava and Aggarwal, Rohit and Cohen, Albert and Upadrasta, Ramakrishna},\n booktitle = {Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction},\n keywords = {rl,register alloc,llvm-mca},\n link = {https://dl.acm.org/doi/abs/10.1145/3578360.3580273},\n pages = {133--144},\n title = {Rl4real: Reinforcement learning for register allocation},\n year = {2023}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3578360.3580273"
  },
  {
    "id": "zheng2024mloop",
    "title": "mLOOP: Optimize Loop Unrolling in Compilation with a ML-based Approach",
    "authors": "Zheng, Zhongchun and Wu, Yuan and Zhang, Xianwei",
    "year": "2024",
    "source": "2024 International Conference on Networking, Architecture and Storage (NAS)",
    "category": "intrapass-tuning",
    "keywords": [
      "loop"
    ],
    "abstract": "Loops are a fundamental component of programs, providing an structured and efficient way to execute repetitive tasks. Given their prevalence and significance, the performance of loops has a direct impact on the overall execution of a program. Predicting loop unroll factor holds remarkable importance in the domain of loop optimization and vectorization parallelism. With the rapid advancements in this field, leveraging machine learning (ML) methods for compilation optimization has emerged as a new research focus. Whereas traditional heuristic algorithms lack precision and Profile-Guided Optimization (PGO) techniques incur considerable compilation overhead, ML method serve as a more balanced approach with respect to accuracy and compilation time. Nonetheless, existing ML approaches are commonly confined to individual optimizations and fail to consider the interplay between multiple optimizations. Additionally, there is inadequate utilization of compilation optimization parameters, resulting in redundant calculations across different optimization processes. This paper proposes mLOOP, a method that employs the XGBoost model to predict loop unroll factors which are integrated into the metadata for use throughout the compilation pipeline. To facilitate deployment and testing in practices, mLOOP is encapsulated into a LLVM optimization pass. By testing on multiple loop-intensive benchmarks, mLOOP achieves 7% speedup on X86 platform and 12% on ARM.",
    "bibtex": "@inproceedings{zheng2024mloop,\n abstract = {Loops are a fundamental component of programs, providing an structured and efficient way to execute repetitive tasks. Given their prevalence and significance, the performance of loops has a direct impact on the overall execution of a program. Predicting loop unroll factor holds remarkable importance in the domain of loop optimization and vectorization parallelism. With the rapid advancements in this field, leveraging machine learning (ML) methods for compilation optimization has emerged as a new research focus. Whereas traditional heuristic algorithms lack precision and Profile-Guided Optimization (PGO) techniques incur considerable compilation overhead, ML method serve as a more balanced approach with respect to accuracy and compilation time. Nonetheless, existing ML approaches are commonly confined to individual optimizations and fail to consider the interplay between multiple optimizations. Additionally, there is inadequate utilization of compilation optimization parameters, resulting in redundant calculations across different optimization processes. This paper proposes mLOOP, a method that employs the XGBoost model to predict loop unroll factors which are integrated into the metadata for use throughout the compilation pipeline. To facilitate deployment and testing in practices, mLOOP is encapsulated into a LLVM optimization pass. By testing on multiple loop-intensive benchmarks, mLOOP achieves 7% speedup on X86 platform and 12% on ARM.},\n author = {Zheng, Zhongchun and Wu, Yuan and Zhang, Xianwei},\n booktitle = {2024 International Conference on Networking, Architecture and Storage (NAS)},\n keywords = {loop},\n link = {https://doi.org/10.1109/NAS63802.2024.10781373},\n organization = {IEEE},\n pages = {1--8},\n title = {mLOOP: Optimize Loop Unrolling in Compilation with a ML-based Approach},\n year = {2024}\n}\n",
    "link": "https://doi.org/10.1109/NAS63802.2024.10781373"
  },
  {
    "id": "ashouri2022mlgoperf",
    "title": "Mlgoperf: An ml guided inliner to optimize performance",
    "authors": "Ashouri, Amir H and Elhoushi, Mostafa and Hua, Yuzhe and Wang, Xiang and Manzoor, Muhammad Asif and Chan, Bryan and Gao, Yaoqing",
    "year": "2022",
    "source": "arXiv preprint arXiv:2207.08389",
    "category": "intrapass-tuning",
    "keywords": [
      "inline"
    ],
    "abstract": "For the past 25 years, we have witnessed an extensive application of Machine Learning to the Compiler space; the selection and the phase-ordering problem. However, limited works have been upstreamed into the state-of-the-art compilers, i.e., LLVM, to seamlessly integrate the former into the optimization pipeline of a compiler to be readily deployed by the user. MLGO was among the first of such projects and it only strives to reduce the code size of a binary with an ML-based Inliner using Reinforcement Learning.This paper presents MLGOPerf; the first end-to-end framework capable of optimizing performance using LLVM's ML-Inliner. It employs a secondary ML model to generate rewards used for training a retargeted Reinforcement learning agent, previously used as the primary model by MLGO. It does so by predicting the post-inlining speedup of a function under analysis and it enables a fast training framework for the primary model which otherwise wouldn't be practical. The experimental results show MLGOPerf is able to gain up to 1.8% and 2.2% with respect to LLVM's optimization at O3 when trained for performance on SPEC CPU2006 and Cbench benchmarks, respectively. Furthermore, the proposed approach provides up to 26% increased opportunities to autotune code regions for our benchmarks which can be translated into an additional 3.7% speedup value.",
    "bibtex": "@article{ashouri2022mlgoperf,\n abstract = {For the past 25 years, we have witnessed an extensive application of Machine Learning to the Compiler space; the selection and the phase-ordering problem. However, limited works have been upstreamed into the state-of-the-art compilers, i.e., LLVM, to seamlessly integrate the former into the optimization pipeline of a compiler to be readily deployed by the user. MLGO was among the first of such projects and it only strives to reduce the code size of a binary with an ML-based Inliner using Reinforcement Learning.This paper presents MLGOPerf; the first end-to-end framework capable of optimizing performance using LLVM's ML-Inliner. It employs a secondary ML model to generate rewards used for training a retargeted Reinforcement learning agent, previously used as the primary model by MLGO. It does so by predicting the post-inlining speedup of a function under analysis and it enables a fast training framework for the primary model which otherwise wouldn't be practical. The experimental results show MLGOPerf is able to gain up to 1.8% and 2.2% with respect to LLVM's optimization at O3 when trained for performance on SPEC CPU2006 and Cbench benchmarks, respectively. Furthermore, the proposed approach provides up to 26% increased opportunities to autotune code regions for our benchmarks which can be translated into an additional 3.7% speedup value.},\n author = {Ashouri, Amir H and Elhoushi, Mostafa and Hua, Yuzhe and Wang, Xiang and Manzoor, Muhammad Asif and Chan, Bryan and Gao, Yaoqing},\n journal = {arXiv preprint arXiv:2207.08389},\n keywords = {inline},\n link = {https://arxiv.org/abs/2207.08389},\n title = {Mlgoperf: An ml guided inliner to optimize performance},\n year = {2022}\n}\n",
    "link": "https://arxiv.org/abs/2207.08389"
  },
  {
    "id": "mendis2019compiler",
    "title": "Compiler auto-vectorization with imitation learning",
    "authors": "Mendis, Charith and Yang, Cambridge and Pu, Yewen and Amarasinghe, Dr Saman and Carbin, Michael",
    "year": "2019",
    "source": "Advances in Neural Information Processing Systems",
    "category": "intrapass-tuning",
    "keywords": [
      "loop",
      "slp"
    ],
    "abstract": "Modern microprocessors are equipped with single instruction multiple data (SIMD) or vector instruction sets which allow compilers to exploit fine-grained data level parallelism. To exploit this parallelism, compilers employ auto-vectorization techniques to automatically convert scalar code into vector code. Larsen & Amarasinghe (2000) first introduced superword level parallelism (SLP) based vectorization, which is one form of vectorization popularly used by compilers. Current compilers employ hand-crafted heuristics and typically only follow one SLP vectorization strategy which can be suboptimal. Recently, Mendis & Amarasinghe (2018) formulated the instruction packing problem of SLP vectorization by leveraging an integer linear programming (ILP) solver, achieving superior runtime performance. In this work, we explore whether it is feasible to imitate optimal decisions made by their ILP solution by fitting a graph neural network policy. We show that the learnt policy produces a vectorization scheme which is better than industry standard compiler heuristics both in terms of static measures and runtime performance. More specifically, the learnt agent produces a vectorization scheme which has a 22.6% higher average reduction in cost compared to LLVM compiler when measured using its own cost model and achieves a geometric mean runtime speedup of 1.015× on the NAS benchmark suite when compared to LLVM’s SLP vectorizer.",
    "bibtex": "@article{mendis2019compiler,\n abstract = {Modern microprocessors are equipped with single instruction multiple data (SIMD) or vector instruction sets which allow compilers to exploit fine-grained data level parallelism. To exploit this parallelism, compilers employ auto-vectorization techniques to automatically convert scalar code into vector code. Larsen & Amarasinghe (2000) first introduced superword level parallelism (SLP) based vectorization, which is one form of vectorization popularly used by compilers. Current compilers employ hand-crafted heuristics and typically only follow one SLP vectorization strategy which can be suboptimal. Recently, Mendis & Amarasinghe (2018) formulated the instruction packing problem of SLP vectorization by leveraging an integer linear programming (ILP) solver, achieving superior runtime performance. In this work, we explore whether it is feasible to imitate optimal decisions made by their ILP solution by fitting a graph neural network policy. We show that the learnt policy produces a vectorization scheme which is better than industry standard compiler heuristics both in terms of static measures and runtime performance. More specifically, the learnt agent produces a vectorization scheme which has a 22.6% higher average reduction in cost compared to LLVM compiler when measured using its own cost model and achieves a geometric mean runtime speedup of 1.015× on the NAS benchmark suite when compared to LLVM’s SLP vectorizer.},\n author = {Mendis, Charith and Yang, Cambridge and Pu, Yewen and Amarasinghe, Dr Saman and Carbin, Michael},\n journal = {Advances in Neural Information Processing Systems},\n keywords = {loop,slp},\n link = {https://proceedings.neurips.cc/paper/2019/hash/d1d5923fc822531bbfd9d87d4760914b-Abstract.html},\n title = {Compiler auto-vectorization with imitation learning},\n volume = {32},\n year = {2019}\n}\n",
    "link": "https://proceedings.neurips.cc/paper/2019/hash/d1d5923fc822531bbfd9d87d4760914b-Abstract.html"
  },
  {
    "id": "wang2014integrating",
    "title": "Integrating profile-driven parallelism detection and machine-learning-based mapping",
    "authors": "Wang, Zheng and Tournavitis, Georgios and Franke, Bj{\\\"o}rn and O'boyle, Michael FP",
    "year": "2014",
    "source": "ACM Transactions on Architecture and Code Optimization (TACO)",
    "category": "intrapass-tuning",
    "keywords": [
      "Loop",
      "Openmp"
    ],
    "abstract": "Compiler-based auto-parallelization is a much-studied area but has yet to find widespread application. This is largely due to the poor identification and exploitation of application parallelism, resulting in disappointing performance far below that which a skilled expert programmer could achieve. We have identified two weaknesses in traditional parallelizing compilers and propose a novel, integrated approach resulting in significant performance improvements of the generated parallel code. Using profile-driven parallelism detection, we overcome the limitations of static analysis, enabling the identification of more application parallelism, and only rely on the user for final approval. We then replace the traditional target-specific and inflexible mapping heuristics with a machine-learning-based prediction mechanism, resulting in better mapping decisions while automating adaptation to different target architectures. We have evaluated our parallelization strategy on the NAS and SPEC CPU2000 benchmarks and two different multicore platforms (dual quad-core Intel Xeon SMP and dual-socket QS20 Cell blade). We demonstrate that our approach not only yields significant improvements when compared with state-of-the-art parallelizing compilers but also comes close to and sometimes exceeds the performance of manually parallelized codes. On average, our methodology achieves 96% of the performance of the hand-tuned OpenMP NAS and SPEC parallel benchmarks on the Intel Xeon platform and gains a significant speedup for the IBM Cell platform, demonstrating the potential of profile-guided and machine-learning- based parallelization for complex multicore platforms.",
    "bibtex": "@article{wang2014integrating,\n abstract = {Compiler-based auto-parallelization is a much-studied area but has yet to find widespread application. This is largely due to the poor identification and exploitation of application parallelism, resulting in disappointing performance far below that which a skilled expert programmer could achieve. We have identified two weaknesses in traditional parallelizing compilers and propose a novel, integrated approach resulting in significant performance improvements of the generated parallel code. Using profile-driven parallelism detection, we overcome the limitations of static analysis, enabling the identification of more application parallelism, and only rely on the user for final approval. We then replace the traditional target-specific and inflexible mapping heuristics with a machine-learning-based prediction mechanism, resulting in better mapping decisions while automating adaptation to different target architectures. We have evaluated our parallelization strategy on the NAS and SPEC CPU2000 benchmarks and two different multicore platforms (dual quad-core Intel Xeon SMP and dual-socket QS20 Cell blade). We demonstrate that our approach not only yields significant improvements when compared with state-of-the-art parallelizing compilers but also comes close to and sometimes exceeds the performance of manually parallelized codes. On average, our methodology achieves 96% of the performance of the hand-tuned OpenMP NAS and SPEC parallel benchmarks on the Intel Xeon platform and gains a significant speedup for the IBM Cell platform, demonstrating the potential of profile-guided and machine-learning- based parallelization for complex multicore platforms.},\n author = {Wang, Zheng and Tournavitis, Georgios and Franke, Bj{\\\"o}rn and O'boyle, Michael FP},\n journal = {ACM Transactions on Architecture and Code Optimization (TACO)},\n keywords = {Loop,Openmp},\n link = {https://dl.acm.org/doi/abs/10.1145/2579561},\n number = {1},\n pages = {1--26},\n publisher = {ACM New York, NY, USA},\n title = {Integrating profile-driven parallelism detection and machine-learning-based mapping},\n volume = {11},\n year = {2014}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/2579561"
  },
  {
    "id": "wood2021artemis",
    "title": "Artemis: Automatic runtime tuning of parallel execution parameters using machine learning",
    "authors": "Wood, Chad and Georgakoudis, Giorgis and Beckingsale, David and Poliakoff, David and Gimenez, Alfredo and Huck, Kevin and Malony, Allen and Gamblin, Todd",
    "year": "2021",
    "source": "High Performance Computing: 36th International Conference, ISC High Performance 2021, Virtual Event, June 24--July 2, 2021, Proceedings 36",
    "category": "intrapass-tuning",
    "keywords": [
      "loop"
    ],
    "abstract": "Portable parallel programming models provide the potential for high performance and productivity, however they come with a multitude of runtime parameters that can have significant impact on execution performance. Selecting the optimal set of those parameters is non-trivial, so that HPC applications perform well in different system environments and on different input data sets, without the need of time consuming parameter exploration or major algorithmic adjustments. We present Artemis, a method for online, feedback-driven, automatic parameter tuning using machine learning that is generalizable and suitable for integration into high-performance codes. Artemis monitors execution at runtime and creates adaptive models for tuning execution parameters, while being minimally invasive in application development and runtime overhead. We demonstrate the effectiveness of Artemis by optimizing the execution times of three HPC proxy applications: Cleverleaf, LULESH, and Kokkos Kernels SpMV. Evaluation shows that Artemis selects the optimal execution policy with over 85% accuracy, has modest monitoring overhead of less than 9%, and increases execution speed by up to 47% despite its runtime overhead.",
    "bibtex": "@inproceedings{wood2021artemis,\n abstract = {Portable parallel programming models provide the potential for high performance and productivity, however they come with a multitude of runtime parameters that can have significant impact on execution performance. Selecting the optimal set of those parameters is non-trivial, so that HPC applications perform well in different system environments and on different input data sets, without the need of time consuming parameter exploration or major algorithmic adjustments. We present Artemis, a method for online, feedback-driven, automatic parameter tuning using machine learning that is generalizable and suitable for integration into high-performance codes. Artemis monitors execution at runtime and creates adaptive models for tuning execution parameters, while being minimally invasive in application development and runtime overhead. We demonstrate the effectiveness of Artemis by optimizing the execution times of three HPC proxy applications: Cleverleaf, LULESH, and Kokkos Kernels SpMV. Evaluation shows that Artemis selects the optimal execution policy with over 85% accuracy, has modest monitoring overhead of less than 9%, and increases execution speed by up to 47% despite its runtime overhead.},\n author = {Wood, Chad and Georgakoudis, Giorgis and Beckingsale, David and Poliakoff, David and Gimenez, Alfredo and Huck, Kevin and Malony, Allen and Gamblin, Todd},\n booktitle = {High Performance Computing: 36th International Conference, ISC High Performance 2021, Virtual Event, June 24--July 2, 2021, Proceedings 36},\n keywords = {loop},\n link = {https://link.springer.com/chapter/10.1007/978-3-030-78713-4_24},\n organization = {Springer},\n pages = {453--472},\n title = {Artemis: Automatic runtime tuning of parallel execution parameters using machine learning},\n year = {2021}\n}\n",
    "link": "https://link.springer.com/chapter/10.1007/978-3-030-78713-4_24"
  },
  {
    "id": "wu2022autotuning",
    "title": "Autotuning polybench benchmarks with llvm clang/polly loop optimization pragmas using bayesian optimization",
    "authors": "Wu, Xingfu and Kruse, Michael and Balaprakash, Prasanna and Finkel, Hal and Hovland, Paul and Taylor, Valerie and Hall, Mary",
    "year": "2022",
    "source": "Concurrency and Computation: Practice and Experience",
    "category": "intrapass-tuning",
    "keywords": [
      "loop",
      "bayes"
    ],
    "abstract": "We develop a ytopt autotuning framework that leverages Bayesian optimization to explore the parameter space search and compare four different supervised learning methods within Bayesian optimization and evaluate their effectiveness. We select six of the most complex PolyBench benchmarks and apply the newly developed LLVM Clang/Polly loop optimization pragmas to the benchmarks to optimize them. We then use the autotuning framework to optimize the pragma parameters to improve their performance. The experimental results show that our autotuning approach outperforms the other compiling methods to provide the smallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu, and covariance with two large datasets in 200 code evaluations for effectively searching the parameter spaces with up to 170,368 different configurations. We find that the Floyd–Warshall benchmark did not benefit from autotuning. To cope with this issue, we provide some compiler option solutions to improve the performance. Then we present loop autotuning without a user's knowledge using a simple mctree autotuning framework to further improve the performance of the Floyd–Warshall benchmark. We also extend the ytopt autotuning framework to tune a deep learning application.",
    "bibtex": "@article{wu2022autotuning,\n abstract = {We develop a ytopt autotuning framework that leverages Bayesian optimization to explore the parameter space search and compare four different supervised learning methods within Bayesian optimization and evaluate their effectiveness. We select six of the most complex PolyBench benchmarks and apply the newly developed LLVM Clang/Polly loop optimization pragmas to the benchmarks to optimize them. We then use the autotuning framework to optimize the pragma parameters to improve their performance. The experimental results show that our autotuning approach outperforms the other compiling methods to provide the smallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu, and covariance with two large datasets in 200 code evaluations for effectively searching the parameter spaces with up to 170,368 different configurations. We find that the Floyd–Warshall benchmark did not benefit from autotuning. To cope with this issue, we provide some compiler option solutions to improve the performance. Then we present loop autotuning without a user's knowledge using a simple mctree autotuning framework to further improve the performance of the Floyd–Warshall benchmark. We also extend the ytopt autotuning framework to tune a deep learning application.},\n author = {Wu, Xingfu and Kruse, Michael and Balaprakash, Prasanna and Finkel, Hal and Hovland, Paul and Taylor, Valerie and Hall, Mary},\n journal = {Concurrency and Computation: Practice and Experience},\n keywords = {loop,bayes},\n link = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6683},\n number = {20},\n pages = {e6683},\n publisher = {Wiley Online Library},\n title = {Autotuning polybench benchmarks with llvm clang/polly loop optimization pragmas using bayesian optimization},\n volume = {34},\n year = {2022}\n}\n",
    "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6683"
  },
  {
    "id": "roy2021bliss",
    "title": "Bliss: auto-tuning complex applications using a pool of diverse lightweight learning models",
    "authors": "Roy, Rohan Basu and Patel, Tirthak and Gadepally, Vijay and Tiwari, Devesh",
    "year": "2021",
    "source": "Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
    "category": "intrapass-tuning",
    "keywords": [
      "loop",
      "bayes"
    ],
    "abstract": "As parallel applications become more complex, auto-tuning becomes more desirable, challenging, and time-consuming. We propose, Bliss, a novel solution for auto-tuning parallel applications without requiring apriori information about applications, domain-specific knowledge, or instrumentation. Bliss demonstrates how to leverage a pool of Bayesian Optimization models to find the near-optimal parameter setting 1.64× faster than the state-of-the-art approaches.",
    "bibtex": "@inproceedings{roy2021bliss,\n abstract = {As parallel applications become more complex, auto-tuning becomes more desirable, challenging, and time-consuming. We propose, Bliss, a novel solution for auto-tuning parallel applications without requiring apriori information about applications, domain-specific knowledge, or instrumentation. Bliss demonstrates how to leverage a pool of Bayesian Optimization models to find the near-optimal parameter setting 1.64× faster than the state-of-the-art approaches.},\n author = {Roy, Rohan Basu and Patel, Tirthak and Gadepally, Vijay and Tiwari, Devesh},\n booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},\n keywords = {loop,bayes},\n link = {https://dl.acm.org/doi/abs/10.1145/3453483.3454109},\n pages = {1280--1295},\n title = {Bliss: auto-tuning complex applications using a pool of diverse lightweight learning models},\n year = {2021}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3453483.3454109"
  },
  {
    "id": "pan2025towards",
    "title": "Towards efficient compiler auto-tuning: Leveraging synergistic search spaces",
    "authors": "Pan, Haolin and Wei, Yuanyu and Xing, Mingjie and Wu, Yanjun and Zhao, Chen",
    "year": "2025",
    "source": "Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization",
    "category": "interpass-tuning",
    "keywords": [
      "kmeans",
      "codesize",
      "GA"
    ],
    "abstract": "Determining the optimal sequence of compiler optimization passes is challenging due to the extensive and intricate search space. Traditional auto-tuning techniques, such as iterative compilation and machine learning methods, are often limited by high computational costs and difficulties in generalizing to new programs. These approaches can be inefficient and may not fully address the varying optimization needs across different programs. This paper introduces a novel approach that leverages the synergistic relationships between optimization passes to effectively reduce the search space. By focusing on chained synergy pass pairs that jointly optimize a specific target, our method uses K-means clustering to capture common optimization patterns across programs and forms these pairs into coresets. Leveraging a supervised learning model trained on these coresets, we effectively predict the most beneficial coreset for new programs, streamlining the search for optimal sequences. By integrating various search strategies, our method quickly converges to near-optimal solutions. Our approach achieves state-of-the-art performance on ten benchmark datasets, including MiBench, CBench, NPB, and CHStone, demonstrating an average reduction of 7.5% in Intermediate Representation (IR) instruction count compared to Oz. Furthermore, this set of chained synergy pass pairs is also well-suited for iterative search studies by other researchers, as it enables achieving an average codesize reduction of 13.9% compared to Oz with a simple search strategy that takes only about 5 seconds, outperforming existing search-based techniques in the initial pass search space across five datasets.",
    "bibtex": "@inproceedings{pan2025towards,\n abstract = {Determining the optimal sequence of compiler optimization passes is challenging due to the extensive and intricate search space. Traditional auto-tuning techniques, such as iterative compilation and machine learning methods, are often limited by high computational costs and difficulties in generalizing to new programs. These approaches can be inefficient and may not fully address the varying optimization needs across different programs. This paper introduces a novel approach that leverages the synergistic relationships between optimization passes to effectively reduce the search space. By focusing on chained synergy pass pairs that jointly optimize a specific target, our method uses K-means clustering to capture common optimization patterns across programs and forms these pairs into coresets. Leveraging a supervised learning model trained on these coresets, we effectively predict the most beneficial coreset for new programs, streamlining the search for optimal sequences. By integrating various search strategies, our method quickly converges to near-optimal solutions. Our approach achieves state-of-the-art performance on ten benchmark datasets, including MiBench, CBench, NPB, and CHStone, demonstrating an average reduction of 7.5% in Intermediate Representation (IR) instruction count compared to Oz. Furthermore, this set of chained synergy pass pairs is also well-suited for iterative search studies by other researchers, as it enables achieving an average codesize reduction of 13.9% compared to Oz with a simple search strategy that takes only about 5 seconds, outperforming existing search-based techniques in the initial pass search space across five datasets.},\n author = {Pan, Haolin and Wei, Yuanyu and Xing, Mingjie and Wu, Yanjun and Zhao, Chen},\n booktitle = {Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},\n keywords = {kmeans,codesize,GA,},\n link = {https://doi.org/10.1145/3696443.3708961},\n pages = {614--627},\n title = {Towards efficient compiler auto-tuning: Leveraging synergistic search spaces},\n year = {2025}\n}\n",
    "link": "https://doi.org/10.1145/3696443.3708961"
  },
  {
    "id": "ogilvie2017minimizing",
    "title": "Minimizing the cost of iterative compilation with active learning",
    "authors": "Ogilvie, William F and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh",
    "year": "2017",
    "source": "2017 IEEE/ACM international symposium on code generation and optimization (CGO)",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Since performance is not portable between platforms, engineers must fine-tune heuristics for each processor in turn. This is such a laborious task that high-profile compilers, supporting many architectures, cannot keep up with hardware innovation and are actually out-of-date. Iterative compilation driven by machine learning has been shown to be efficient at generating portable optimization models automatically. However, good quality models require costly, repetitive, and extensive training which greatly hinders the wide adoption of this powerful technique. In this work, we show that much of this cost is spent collecting training data, runtime measurements for different optimization decisions, which contribute little to the final heuristic. Current implementations evaluate randomly chosen, often redundant, training examples a pre-configured, almost always excessive, number of times - a large source of wasted effort. Our approach optimizes not only the selection of training examples but also the number of samples per example, independently. To evaluate, we construct 11 high-quality models which use a combination of optimization settings to predict the runtime of benchmarks from the SPAPT suite. Our novel, broadly applicable, methodology is able to reduce the training overhead by up to 26x compared to an approach with a fixed number of sample runs, transforming what is potentially months of work into days.",
    "bibtex": "@inproceedings{ogilvie2017minimizing,\n abstract = {Since performance is not portable between platforms, engineers must fine-tune heuristics for each processor in turn. This is such a laborious task that high-profile compilers, supporting many architectures, cannot keep up with hardware innovation and are actually out-of-date. Iterative compilation driven by machine learning has been shown to be efficient at generating portable optimization models automatically. However, good quality models require costly, repetitive, and extensive training which greatly hinders the wide adoption of this powerful technique. In this work, we show that much of this cost is spent collecting training data, runtime measurements for different optimization decisions, which contribute little to the final heuristic. Current implementations evaluate randomly chosen, often redundant, training examples a pre-configured, almost always excessive, number of times - a large source of wasted effort. Our approach optimizes not only the selection of training examples but also the number of samples per example, independently. To evaluate, we construct 11 high-quality models which use a combination of optimization settings to predict the runtime of benchmarks from the SPAPT suite. Our novel, broadly applicable, methodology is able to reduce the training overhead by up to 26x compared to an approach with a fixed number of sample runs, transforming what is potentially months of work into days.},\n author = {Ogilvie, William F and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},\n booktitle = {2017 IEEE/ACM international symposium on code generation and optimization (CGO)},\n link = {https://doi.org/10.1109/CGO.2017.7863744},\n organization = {IEEE},\n pages = {245--256},\n title = {Minimizing the cost of iterative compilation with active learning},\n year = {2017}\n}\n",
    "link": "https://doi.org/10.1109/CGO.2017.7863744"
  },
  {
    "id": "liang2023learning",
    "title": "Learning compiler pass orders using coreset and normalized value prediction",
    "authors": "Liang, Youwei and Stone, Kevin and Shameli, Ali and Cummins, Chris and Elhoushi, Mostafa and Guo, Jiadong and Steiner, Benoit and Yang, Xiaomeng and Xie, Pengtao and Leather, Hugh James and others",
    "year": "2023",
    "source": "International Conference on Machine Learning",
    "category": "interpass-tuning",
    "keywords": [
      "codesize",
      "opensource"
    ],
    "abstract": "Finding the optimal pass sequence of compilation can lead to a significant reduction in program size. Prior works on compilation pass ordering have two major drawbacks. They either require an excessive budget (in terms of the number of compilation passes) at compile time or fail to generalize to unseen programs. In this work, instead of predicting passes sequentially, we directly learn a policy on the pass sequence space, which outperforms the default -Oz flag by an average of 4.5% over a large collection (4683) of unseen code repositories from diverse domains across 14 datasets. To achieve this, we first identify a small set (termed coreset) of pass sequences that generally optimize the size of most programs. Then, a policy is learned to pick the optimal sequences by predicting the normalized values of the pass sequences in the coreset. Our results demonstrate that existing human-designed compiler passes can be improved with a simple yet effective technique that leverages pass sequence space which contains dense rewards, while approaches operating on the individual pass space may suffer from issues of sparse reward, and do not generalize well to held-out programs from different domains. Website: https://rlcompopt.github.io.",
    "bibtex": "@inproceedings{liang2023learning,\n abstract = {Finding the optimal pass sequence of compilation can lead to a significant reduction in program size. Prior works on compilation pass ordering have two major drawbacks. They either require an excessive budget (in terms of the number of compilation passes) at compile time or fail to generalize to unseen programs. In this work, instead of predicting passes sequentially, we directly learn a policy on the pass sequence space, which outperforms the default -Oz flag by an average of 4.5% over a large collection (4683) of unseen code repositories from diverse domains across 14 datasets. To achieve this, we first identify a small set (termed coreset) of pass sequences that generally optimize the size of most programs. Then, a policy is learned to pick the optimal sequences by predicting the normalized values of the pass sequences in the coreset. Our results demonstrate that existing human-designed compiler passes can be improved with a simple yet effective technique that leverages pass sequence space which contains dense rewards, while approaches operating on the individual pass space may suffer from issues of sparse reward, and do not generalize well to held-out programs from different domains. Website: https://rlcompopt.github.io.},\n author = {Liang, Youwei and Stone, Kevin and Shameli, Ali and Cummins, Chris and Elhoushi, Mostafa and Guo, Jiadong and Steiner, Benoit and Yang, Xiaomeng and Xie, Pengtao and Leather, Hugh James and others},\n booktitle = {International Conference on Machine Learning},\n keywords = {codesize,opensource},\n link = {https://proceedings.mlr.press/v202/liang23f.html},\n organization = {PMLR},\n pages = {20746--20762},\n title = {Learning compiler pass orders using coreset and normalized value prediction},\n year = {2023}\n}\n",
    "link": "https://proceedings.mlr.press/v202/liang23f.html"
  },
  {
    "id": "liu2022compiler",
    "title": "Compiler optimization parameter selection method based on ensemble learning",
    "authors": "Liu, Hui and Xu, Jinlong and Chen, Sen and Guo, Te",
    "year": "2022",
    "source": "Electronics",
    "category": "interpass-tuning",
    "keywords": [
      "PSO"
    ],
    "abstract": "Iterative compilation based on machine learning can effectively predict a program’s compiler optimization parameters. Although having some limits, such as the low efficiency of optimization parameter search and prediction accuracy, machine learning-based solutions have been a frontier research field in the field of iterative compilation and have gained increasing attention. The research challenges are focused on learning algorithm selection, optimal parameter search, and program feature representation. For the existing problems, we propose an ensemble learning-based optimization parameter selection (ELOPS) method for the compiler. First, in order to further improve the optimization parameter search efficiency and accuracy, we proposed a multi-objective particle swarm optimization (PSO) algorithm to determine the optimal compiler parameters of the program. Second, we extracted the mixed features of the program through the feature-class relevance method, rather than using static or dynamic features alone. Finally, as the existing research usually uses a separate machine learning algorithm to build prediction models, an ensemble learning model using program features and optimization parameters was constructed to effectively predict compiler optimization parameters of the new program. Using standard performance evaluation corporation 2006 (SPEC2006) and NAS parallel benchmark (NPB) benchmarks as well as some typical scientific computing programs, we compared ELOPS with the existing methods. The experimental results showed that we can respectively achieve 1.29× and 1.26× speedup when using our method on two platforms, which are better results than those of existing methods.",
    "bibtex": "@article{liu2022compiler,\n abstract = {Iterative compilation based on machine learning can effectively predict a program’s compiler optimization parameters. Although having some limits, such as the low efficiency of optimization parameter search and prediction accuracy, machine learning-based solutions have been a frontier research field in the field of iterative compilation and have gained increasing attention. The research challenges are focused on learning algorithm selection, optimal parameter search, and program feature representation. For the existing problems, we propose an ensemble learning-based optimization parameter selection (ELOPS) method for the compiler. First, in order to further improve the optimization parameter search efficiency and accuracy, we proposed a multi-objective particle swarm optimization (PSO) algorithm to determine the optimal compiler parameters of the program. Second, we extracted the mixed features of the program through the feature-class relevance method, rather than using static or dynamic features alone. Finally, as the existing research usually uses a separate machine learning algorithm to build prediction models, an ensemble learning model using program features and optimization parameters was constructed to effectively predict compiler optimization parameters of the new program. Using standard performance evaluation corporation 2006 (SPEC2006) and NAS parallel benchmark (NPB) benchmarks as well as some typical scientific computing programs, we compared ELOPS with the existing methods. The experimental results showed that we can respectively achieve 1.29× and 1.26× speedup when using our method on two platforms, which are better results than those of existing methods.},\n author = {Liu, Hui and Xu, Jinlong and Chen, Sen and Guo, Te},\n journal = {Electronics},\n keywords = {PSO},\n number = {15},\n pages = {2452},\n publisher = {MDPI},\n title = {Compiler optimization parameter selection method based on ensemble learning},\n volume = {11},\n year = {2022}\n}\n",
    "link": ""
  },
  {
    "id": "zhu2024compiler",
    "title": "Compiler autotuning through multiple-phase learning",
    "authors": "Zhu, Mingxuan and Hao, Dan and Chen, Junjie",
    "year": "2024",
    "source": "ACM Transactions on Software Engineering and Methodology",
    "category": "interpass-tuning",
    "keywords": [
      "PSO"
    ],
    "abstract": "Widely used compilers like GCC and LLVM usually have hundreds of optimizations controlled by optimization flags, which are enabled or disabled during compilation to improve the runtime performance (e.g., small execution time) of the compiler program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to manually tune compiler optimization flags. In the literature, a number of autotuning techniques have been proposed, which tune optimization flags for a compiled program by comparing its actual runtime performance with different optimization flag combinations. Due to the huge search space and heavy actual runtime cost, these techniques suffer from the widely recognized efficiency problem. To reduce the heavy runtime cost, in this article we propose a lightweight learning approach that uses a small number of actual runtime performance data to predict the runtime performance of a compiled program with various optimization flag combinations. Furthermore, to reduce the search space, we design a novel particle swarm algorithm that tunes compiler optimization flags with the prediction model. To evaluate the performance of the proposed approach, CompTuner, we conduct an extensive experimental study on two popular C compilers, GCC and LLVM, with two widely used benchmarks, cBench and PolyBench. The experimental results show that CompTuner significantly outperforms the six compared techniques, including the state-of-the-art technique BOCA.",
    "bibtex": "@article{zhu2024compiler,\n abstract = {Widely used compilers like GCC and LLVM usually have hundreds of optimizations controlled by optimization flags, which are enabled or disabled during compilation to improve the runtime performance (e.g., small execution time) of the compiler program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to manually tune compiler optimization flags. In the literature, a number of autotuning techniques have been proposed, which tune optimization flags for a compiled program by comparing its actual runtime performance with different optimization flag combinations. Due to the huge search space and heavy actual runtime cost, these techniques suffer from the widely recognized efficiency problem. To reduce the heavy runtime cost, in this article we propose a lightweight learning approach that uses a small number of actual runtime performance data to predict the runtime performance of a compiled program with various optimization flag combinations. Furthermore, to reduce the search space, we design a novel particle swarm algorithm that tunes compiler optimization flags with the prediction model. To evaluate the performance of the proposed approach, CompTuner, we conduct an extensive experimental study on two popular C compilers, GCC and LLVM, with two widely used benchmarks, cBench and PolyBench. The experimental results show that CompTuner significantly outperforms the six compared techniques, including the state-of-the-art technique BOCA.},\n author = {Zhu, Mingxuan and Hao, Dan and Chen, Junjie},\n journal = {ACM Transactions on Software Engineering and Methodology},\n keywords = {PSO},\n link = {https://dl.acm.org/doi/abs/10.1145/3640330},\n number = {4},\n pages = {1--38},\n publisher = {ACM New York, NY},\n title = {Compiler autotuning through multiple-phase learning},\n volume = {33},\n year = {2024}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3640330"
  },
  {
    "id": "ni2024tsoa",
    "title": "Tsoa: a two-stage optimization approach for GCC compilation options to minimize execution time",
    "authors": "Ni, Youcong and Du, Xin and Yuan, Yuan and Xiao, Ruliang and Chen, Gaolin",
    "year": "2024",
    "source": "Automated Software Engineering",
    "category": "interpass-tuning",
    "keywords": [
      "randomforest"
    ],
    "abstract": "The open-source compiler GCC offers numerous options to improve execution time. Two categories of approaches, machine learning-based and design space exploration, have emerged for selecting the optimal set of options. However, they continue to face challenge in quickly obtaining high-quality solutions due to the large and discrete optimization space, time-consuming utility evaluation for selected options, and complex interactions among options. To address these challenges, we propose TSOA, a Two-Stage Optimization Approach for GCC compilation options to minimize execution time. In the first stage, we present OPPM, an Option Preselection algorithm based on Pattern Mining. OPPM generates diverse samples to cover a wide range of option interactions. It subsequently mines frequent options from both objective-improved and non-improved samples. The mining results are further validated using CRC codes to precisely preselect options and reduce the optimization space. Transitioning to the second stage, we present OSEA, an Option Selection Evolutionary optimization Algorithm. OSEA is grounded in solution preselection and an option interaction graph. The solution preselection employs a random forest to build a classifier, efficiently identifying promising solutions for the next-generation population and thereby reducing the time spent on utility evaluation. Simultaneously, the option interaction graph is built to capture option interplays and their influence on objectives from evaluated solutions. Then, high-quality solutions are generated based on the option interaction graph. We evaluate the performance of TSOA by comparing it with representative machine learning-based and design space exploration approaches across a diverse set of 20 problem instances from two benchmark platforms. Additionally, we validate the effectiveness of OPPM and conduct related ablation experiments. The experimental results show that TSOA outperforms state-of-the-art approaches significantly in both optimization time and solution quality. Moreover, OPPM outperforms other option preselection algorithms, while the effectiveness of random forest-assisted solution preselection, along with new solution generation based on the option interaction graph, has been verified.",
    "bibtex": "@article{ni2024tsoa,\n abstract = {The open-source compiler GCC offers numerous options to improve execution time. Two categories of approaches, machine learning-based and design space exploration, have emerged for selecting the optimal set of options. However, they continue to face challenge in quickly obtaining high-quality solutions due to the large and discrete optimization space, time-consuming utility evaluation for selected options, and complex interactions among options. To address these challenges, we propose TSOA, a Two-Stage Optimization Approach for GCC compilation options to minimize execution time. In the first stage, we present OPPM, an Option Preselection algorithm based on Pattern Mining. OPPM generates diverse samples to cover a wide range of option interactions. It subsequently mines frequent options from both objective-improved and non-improved samples. The mining results are further validated using CRC codes to precisely preselect options and reduce the optimization space. Transitioning to the second stage, we present OSEA, an Option Selection Evolutionary optimization Algorithm. OSEA is grounded in solution preselection and an option interaction graph. The solution preselection employs a random forest to build a classifier, efficiently identifying promising solutions for the next-generation population and thereby reducing the time spent on utility evaluation. Simultaneously, the option interaction graph is built to capture option interplays and their influence on objectives from evaluated solutions. Then, high-quality solutions are generated based on the option interaction graph. We evaluate the performance of TSOA by comparing it with representative machine learning-based and design space exploration approaches across a diverse set of 20 problem instances from two benchmark platforms. Additionally, we validate the effectiveness of OPPM and conduct related ablation experiments. The experimental results show that TSOA outperforms state-of-the-art approaches significantly in both optimization time and solution quality. Moreover, OPPM outperforms other option preselection algorithms, while the effectiveness of random forest-assisted solution preselection, along with new solution generation based on the option interaction graph, has been verified.},\n author = {Ni, Youcong and Du, Xin and Yuan, Yuan and Xiao, Ruliang and Chen, Gaolin},\n journal = {Automated Software Engineering},\n keywords = {randomforest},\n link = {https://link.springer.com/article/10.1007/s10515-024-00437-w},\n number = {2},\n pages = {39},\n publisher = {Springer},\n title = {Tsoa: a two-stage optimization approach for GCC compilation options to minimize execution time},\n volume = {31},\n year = {2024}\n}\n",
    "link": "https://link.springer.com/article/10.1007/s10515-024-00437-w"
  },
  {
    "id": "park2022srtuner",
    "title": "Srtuner: Effective compiler optimization customization by exposing synergistic relations",
    "authors": "Park, Sunghyun and Latifi, Salar and Park, Yongjun and Behroozi, Armand and Jeon, Byungsoo and Mahlke, Scott",
    "year": "2022",
    "source": "2022 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Despite ceaseless efforts, extremely large and complex optimization space makes even the state-of-the-art compilers fail in delivering the most performant setting that can fully utilize the underlying hardware. Although this inefficiency suggests opportunity for tuning, it has been challenging for prior tuning methods to consider the complex interactions between optimizations and maximize the tuning quality while handling local optima efficiently. To tackle this problem, we suggest an intelligent auto-tuning strategy, called SRTuner, which searches for the best optimization setting by exposing important optimization interactions and directly using them to focus on promising subspaces. To reveal high-impact inter-optimization relations, SRTuner proposes a multistage structure and a distribution-based estimation method that approximates the impact of an optimization effectively. Besides, to efficiently handle local optima, our technique defines optimization decisions as a series of multi-armed bandit problems to formulate the exploration-exploitation dilemma. SRTuner is evaluated with three representative compilers from various domains on different target hardware: GCC (traditional C/ C++ compiler) on CPU, TVM (domain-specific machine learning compiler) on GPU, and OpenCL compilers (kernel compiler for heterogeneous computing) on both CPU/GPU. Results show that SRTuner accelerates target executions by 1.24×, 2.03× and 34.4× compared to the highest level of optimization provided by each compiler and outperforms state-of-the-art works by 1.04×−1.14×. As a byproduct of our unique tuning strategy, SRTuner can offer synergistic optimizations for each workload, which allows it to in part identify why it outperformed current compilers. With this information, we are able to find important optimizations that each compiler misused and demonstrate how this information can benefit future tuning strategies.",
    "bibtex": "@inproceedings{park2022srtuner,\n abstract = {Despite ceaseless efforts, extremely large and complex optimization space makes even the state-of-the-art compilers fail in delivering the most performant setting that can fully utilize the underlying hardware. Although this inefficiency suggests opportunity for tuning, it has been challenging for prior tuning methods to consider the complex interactions between optimizations and maximize the tuning quality while handling local optima efficiently. To tackle this problem, we suggest an intelligent auto-tuning strategy, called SRTuner, which searches for the best optimization setting by exposing important optimization interactions and directly using them to focus on promising subspaces. To reveal high-impact inter-optimization relations, SRTuner proposes a multistage structure and a distribution-based estimation method that approximates the impact of an optimization effectively. Besides, to efficiently handle local optima, our technique defines optimization decisions as a series of multi-armed bandit problems to formulate the exploration-exploitation dilemma. SRTuner is evaluated with three representative compilers from various domains on different target hardware: GCC (traditional C/ C++ compiler) on CPU, TVM (domain-specific machine learning compiler) on GPU, and OpenCL compilers (kernel compiler for heterogeneous computing) on both CPU/GPU. Results show that SRTuner accelerates target executions by 1.24×, 2.03× and 34.4× compared to the highest level of optimization provided by each compiler and outperforms state-of-the-art works by 1.04×−1.14×. As a byproduct of our unique tuning strategy, SRTuner can offer synergistic optimizations for each workload, which allows it to in part identify why it outperformed current compilers. With this information, we are able to find important optimizations that each compiler misused and demonstrate how this information can benefit future tuning strategies.},\n author = {Park, Sunghyun and Latifi, Salar and Park, Yongjun and Behroozi, Armand and Jeon, Byungsoo and Mahlke, Scott},\n booktitle = {2022 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},\n link = {https://ieeexplore.ieee.org/abstract/document/9741263},\n organization = {IEEE},\n pages = {118--130},\n title = {Srtuner: Effective compiler optimization customization by exposing synergistic relations},\n year = {2022}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/9741263"
  },
  {
    "id": "taugtekin2021foga",
    "title": "Foga: Flag optimization with genetic algorithm",
    "authors": "Ta{\\u{g}}tekin, Burak and H{\\\"o}ke, Berkan and Sezer, Mert Kutay and {\\\"O}zt{\\\"u}rk, Mahiye Uluya{\\u{g}}mur",
    "year": "2021",
    "source": "2021 International Conference on INnovations in Intelligent SysTems and Applications (INISTA)",
    "category": "interpass-tuning",
    "keywords": [
      "GA"
    ],
    "abstract": "Recently, program autotuning has become very popular especially in embedded systems, when we have limited resources such as computing power and memory where these systems run generally time-critical applications. Compiler optimization space gradually expands with the renewed compiler options and inclusion of new architectures. These advancements bring autotuning even more important position. In this paper, we introduced Flag Optimization with Genetic Algorithm (FOGA) as an autotuning solution for GCC flag optimization. FOGA has two main advantages over the other autotuning approaches: the first one is the hyperparameter tuning of the genetic algorithm (GA), the second one is the maximum iteration parameter to stop when no further improvement occurs. We demonstrated remarkable speedup in the execution time of C++ source codes with the help of optimization flags provided by FOGA when compared to the state of the art framework OpenTuner.",
    "bibtex": "@inproceedings{taugtekin2021foga,\n abstract = {Recently, program autotuning has become very popular especially in embedded systems, when we have limited resources such as computing power and memory where these systems run generally time-critical applications. Compiler optimization space gradually expands with the renewed compiler options and inclusion of new architectures. These advancements bring autotuning even more important position. In this paper, we introduced Flag Optimization with Genetic Algorithm (FOGA) as an autotuning solution for GCC flag optimization. FOGA has two main advantages over the other autotuning approaches: the first one is the hyperparameter tuning of the genetic algorithm (GA), the second one is the maximum iteration parameter to stop when no further improvement occurs. We demonstrated remarkable speedup in the execution time of C++ source codes with the help of optimization flags provided by FOGA when compared to the state of the art framework OpenTuner.},\n author = {Ta{\\u{g}}tekin, Burak and H{\\\"o}ke, Berkan and Sezer, Mert Kutay and {\\\"O}zt{\\\"u}rk, Mahiye Uluya{\\u{g}}mur},\n booktitle = {2021 International Conference on INnovations in Intelligent SysTems and Applications (INISTA)},\n keywords = {GA},\n link = {https://doi.org/10.1109/INISTA52262.2021.9548573},\n organization = {IEEE},\n pages = {1--6},\n title = {Foga: Flag optimization with genetic algorithm},\n year = {2021}\n}\n",
    "link": "https://doi.org/10.1109/INISTA52262.2021.9548573"
  },
  {
    "id": "liu2021iterative",
    "title": "Iterative compilation optimization based on metric learning and collaborative filtering",
    "authors": "Liu, Hongzhi and Luo, Jie and Li, Ying and Wu, Zhonghai",
    "year": "2021",
    "source": "ACM Transactions on Architecture and Code Optimization (TACO)",
    "category": "interpass-tuning",
    "keywords": [
      "PCA",
      "EFA"
    ],
    "abstract": "Pass selection and phase ordering are two critical compiler auto-tuning problems. Traditional heuristic methods cannot effectively address these NP-hard problems especially given the increasing number of compiler passes and diverse hardware architectures. Recent research efforts have attempted to address these problems through machine learning. However, the large search space of candidate pass sequences, the large numbers of redundant and irrelevant features, and the lack of training program instances make it difficult to learn models well. Several methods have tried to use expert knowledge to simplify the problems, such as using only the compiler passes or subsequences in the standard levels (e.g., -O1, -O2, and -O3) provided by compiler designers. However, these methods ignore other useful compiler passes that are not contained in the standard levels. Principal component analysis (PCA) and exploratory factor analysis (EFA) have been utilized to reduce the redundancy of feature data. However, these unsupervised methods retain all the information irrelevant to the performance of compilation optimization, which may mislead the subsequent model learning.\nTo solve these problems, we propose a compiler pass selection and phase ordering approach, called Iterative Compilation based on Metric learning and Collaborative filtering (ICMC). First, we propose a data-driven method to construct pass subsequences according to the observed collaborative interactions and dependency among passes on a given program set. Therefore, we can make use of all available compiler passes and prune the search space. Then, a supervised metric learning method is utilized to retain useful feature information for compilation optimization while removing both the irrelevant and the redundant information. Based on the learned similarity metric, a neighborhood-based collaborative filtering method is employed to iteratively recommend a few superior compiler passes for each target program. Last, an iterative data enhancement method is designed to alleviate the problem of lacking training program instances and to enhance the performance of iterative pass recommendations. The experimental results using the LLVM compiler on all 32 cBench programs show the following: (1) ICMC significantly outperforms several state-of-the-art compiler phase ordering methods, (2) it performs the same or better than the standard level -O3 on all the test programs, and (3) it can reach an average performance speedup of 1.20 (up to 1.46) compared with the standard level -O3.",
    "bibtex": "@article{liu2021iterative,\n abstract = {Pass selection and phase ordering are two critical compiler auto-tuning problems. Traditional heuristic methods cannot effectively address these NP-hard problems especially given the increasing number of compiler passes and diverse hardware architectures. Recent research efforts have attempted to address these problems through machine learning. However, the large search space of candidate pass sequences, the large numbers of redundant and irrelevant features, and the lack of training program instances make it difficult to learn models well. Several methods have tried to use expert knowledge to simplify the problems, such as using only the compiler passes or subsequences in the standard levels (e.g., -O1, -O2, and -O3) provided by compiler designers. However, these methods ignore other useful compiler passes that are not contained in the standard levels. Principal component analysis (PCA) and exploratory factor analysis (EFA) have been utilized to reduce the redundancy of feature data. However, these unsupervised methods retain all the information irrelevant to the performance of compilation optimization, which may mislead the subsequent model learning.\nTo solve these problems, we propose a compiler pass selection and phase ordering approach, called Iterative Compilation based on Metric learning and Collaborative filtering (ICMC). First, we propose a data-driven method to construct pass subsequences according to the observed collaborative interactions and dependency among passes on a given program set. Therefore, we can make use of all available compiler passes and prune the search space. Then, a supervised metric learning method is utilized to retain useful feature information for compilation optimization while removing both the irrelevant and the redundant information. Based on the learned similarity metric, a neighborhood-based collaborative filtering method is employed to iteratively recommend a few superior compiler passes for each target program. Last, an iterative data enhancement method is designed to alleviate the problem of lacking training program instances and to enhance the performance of iterative pass recommendations. The experimental results using the LLVM compiler on all 32 cBench programs show the following: (1) ICMC significantly outperforms several state-of-the-art compiler phase ordering methods, (2) it performs the same or better than the standard level -O3 on all the test programs, and (3) it can reach an average performance speedup of 1.20 (up to 1.46) compared with the standard level -O3.},\n author = {Liu, Hongzhi and Luo, Jie and Li, Ying and Wu, Zhonghai},\n journal = {ACM Transactions on Architecture and Code Optimization (TACO)},\n keywords = {PCA,EFA},\n link = {https://dl.acm.org/doi/full/10.1145/3480250},\n number = {1},\n pages = {1--25},\n publisher = {ACM New York, NY},\n title = {Iterative compilation optimization based on metric learning and collaborative filtering},\n volume = {19},\n year = {2021}\n}\n",
    "link": "https://dl.acm.org/doi/full/10.1145/3480250"
  },
  {
    "id": "jayatilaka2021towards",
    "title": "Towards compile-time-reducing compiler optimization selection via machine learning",
    "authors": "Jayatilaka, Tarindu and Ueno, Hideto and Georgakoudis, Giorgis and Park, EunJung and Doerfert, Johannes",
    "year": "2021",
    "source": "50th International Conference on Parallel Processing Workshop",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Compilers come with a multitude of optimizations to choose from, and the chosen optimizations significantly affect the performance of the code being optimized. Selecting the optimal set of optimizations to apply and determining the order to run them is non-trivial. All of these optimizations closely interact with each other, and an optimization’s ability to improve the code heavily depends on how the previous optimizations modified it. The current approach to solve this is to use a one-size-fits-all optimization sequence, that is designed to perform reasonably well for any given source code. In other words, they are not designed to optimize depending on the nature of the code, which usually results in sub-optimal performance. In this paper, we present preliminary work tackling the problem from the perspective of compile-time by adapting the optimization sequence to cater to different program types. We start by analyzing how the source code interacts with the passes, as well as how the passes interact with each other. We use this information and propose two potential methods driven by machine learning to run customized optimization sequences on the source code. First, we look at how we can use a neural network to predict and skip passes that do not optimize the code to improve compilation time. Second, we look at how we can use clustering and predictive models to select custom pass pipelines. We believe that our approach has the potential to replace the current one-size-fits-all approach, with better optimization sequences that are tailored to perform better depending on the code. At the same time, it will allow testing the potential pipelines thoroughly, a practical requirement to gain confidence in the correctness of the compiler.",
    "bibtex": "@inproceedings{jayatilaka2021towards,\n abstract = {Compilers come with a multitude of optimizations to choose from, and the chosen optimizations significantly affect the performance of the code being optimized. Selecting the optimal set of optimizations to apply and determining the order to run them is non-trivial. All of these optimizations closely interact with each other, and an optimization’s ability to improve the code heavily depends on how the previous optimizations modified it. The current approach to solve this is to use a one-size-fits-all optimization sequence, that is designed to perform reasonably well for any given source code. In other words, they are not designed to optimize depending on the nature of the code, which usually results in sub-optimal performance. In this paper, we present preliminary work tackling the problem from the perspective of compile-time by adapting the optimization sequence to cater to different program types. We start by analyzing how the source code interacts with the passes, as well as how the passes interact with each other. We use this information and propose two potential methods driven by machine learning to run customized optimization sequences on the source code. First, we look at how we can use a neural network to predict and skip passes that do not optimize the code to improve compilation time. Second, we look at how we can use clustering and predictive models to select custom pass pipelines. We believe that our approach has the potential to replace the current one-size-fits-all approach, with better optimization sequences that are tailored to perform better depending on the code. At the same time, it will allow testing the potential pipelines thoroughly, a practical requirement to gain confidence in the correctness of the compiler.},\n author = {Jayatilaka, Tarindu and Ueno, Hideto and Georgakoudis, Giorgis and Park, EunJung and Doerfert, Johannes},\n booktitle = {50th International Conference on Parallel Processing Workshop},\n link = {https://dl.acm.org/doi/abs/10.1145/3458744.3473355},\n pages = {1--6},\n title = {Towards compile-time-reducing compiler optimization selection via machine learning},\n year = {2021}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3458744.3473355"
  },
  {
    "id": "colucci2021mlcomp",
    "title": "MLComp: A methodology for machine learning-based performance estimation and adaptive selection of Pareto-optimal compiler optimization sequences",
    "authors": "Colucci, Alessio and Juh{\\'a}sz, D{\\'a}vid and Mosbeck, Martin and Marchisio, Alberto and Rehman, Semeen and Kreutzer, Manfred and Nadbath, G{\\\"u}nther and Jantsch, Axel and Shafique, Muhammad",
    "year": "2021",
    "source": "2021 Design, Automation \\& Test in Europe Conference \\& Exhibition (DATE)",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Embedded systems have proliferated in various consumer and industrial applications with the evolution of Cyber-Physical Systems and the Internet of Things. These systems are subjected to stringent constraints so that embedded software must be optimized for multiple objectives simultaneously, namely reduced energy consumption, execution time, and code size. Compilers offer optimization phases to improve these metrics. However, proper selection and ordering of them depends on multiple factors and typically requires expert knowledge. State-of-the-art optimizers facilitate different platforms and applications case by case, and they are limited by optimizing one metric at a time, as well as requiring a time-consuming adaptation for different targets through dynamic profiling. To address these problems, we propose the novel MLComp methodology, in which optimization phases are sequenced by a Reinforcement Learning-based policy. Training of the policy is supported by Machine Learning-based analytical models for quick performance estimation, thereby drastically reducing the time spent for dynamic profiling. In our framework, different Machine Learning models are automatically tested to choose the best-fitting one. The trained Performance Estimator model is leveraged to efficiently devise Reinforcement Learning-based multi-objective policies for creating quasi-optimal phase sequences. Compared to state-of-the-art estimation models, our Performance Estimator model achieves lower relative error (< 2%) with up to 50 × faster training time over multiple platforms and application domains. Our Phase Selection Policy improves execution time and energy consumption of a given code by up to 12% and 6%, respectively. The Performance Estimator and the Phase Selection Policy can be trained efficiently for any target platform and application domain.",
    "bibtex": "@inproceedings{colucci2021mlcomp,\n abstract = {Embedded systems have proliferated in various consumer and industrial applications with the evolution of Cyber-Physical Systems and the Internet of Things. These systems are subjected to stringent constraints so that embedded software must be optimized for multiple objectives simultaneously, namely reduced energy consumption, execution time, and code size. Compilers offer optimization phases to improve these metrics. However, proper selection and ordering of them depends on multiple factors and typically requires expert knowledge. State-of-the-art optimizers facilitate different platforms and applications case by case, and they are limited by optimizing one metric at a time, as well as requiring a time-consuming adaptation for different targets through dynamic profiling. To address these problems, we propose the novel MLComp methodology, in which optimization phases are sequenced by a Reinforcement Learning-based policy. Training of the policy is supported by Machine Learning-based analytical models for quick performance estimation, thereby drastically reducing the time spent for dynamic profiling. In our framework, different Machine Learning models are automatically tested to choose the best-fitting one. The trained Performance Estimator model is leveraged to efficiently devise Reinforcement Learning-based multi-objective policies for creating quasi-optimal phase sequences. Compared to state-of-the-art estimation models, our Performance Estimator model achieves lower relative error (< 2%) with up to 50 × faster training time over multiple platforms and application domains. Our Phase Selection Policy improves execution time and energy consumption of a given code by up to 12% and 6%, respectively. The Performance Estimator and the Phase Selection Policy can be trained efficiently for any target platform and application domain.},\n author = {Colucci, Alessio and Juh{\\'a}sz, D{\\'a}vid and Mosbeck, Martin and Marchisio, Alberto and Rehman, Semeen and Kreutzer, Manfred and Nadbath, G{\\\"u}nther and Jantsch, Axel and Shafique, Muhammad},\n booktitle = {2021 Design, Automation \\& Test in Europe Conference \\& Exhibition (DATE)},\n link = {https://ieeexplore.ieee.org/abstract/document/9474158},\n organization = {IEEE},\n pages = {108--113},\n title = {MLComp: A methodology for machine learning-based performance estimation and adaptive selection of Pareto-optimal compiler optimization sequences},\n year = {2021}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/9474158"
  },
  {
    "id": "chen2021efficient",
    "title": "Efficient compiler autotuning via bayesian optimization",
    "authors": "Chen, Junjie and Xu, Ningxin and Chen, Peiqi and Zhang, Hongyu",
    "year": "2021",
    "source": "2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)",
    "category": "interpass-tuning",
    "keywords": [
      "Bayesian"
    ],
    "abstract": "A typical compiler such as GCC supports hundreds of optimizations controlled by compilation flags for improving the runtime performance of the compiled program. Due to the large number of compilation flags and the exponential number of flag combinations, it is impossible for compiler users to manually tune these optimization flags in order to achieve the required runtime performance of the compiled programs. Over the years, many compiler autotuning approaches have been proposed to automatically tune optimization flags, but they still suffer from the efficiency problem due to the huge search space. In this paper, we propose the first Bayesian optimization based approach, called BOCA, for efficient compiler autotuning. In BOCA, we leverage a tree-based model for approximating the objective function in order to make Bayesian optimization scalable to a large number of optimization flags. Moreover, we design a novel searching strategy to improve the efficiency of Bayesian optimization by incorporating the impact of each optimization flag measured by the tree-based model and a decay function to strike a balance between exploitation and exploration. We conduct extensive experiments to investigate the effectiveness of BOCA on two most popular C compilers (i.e., GCC and LLVM) and two widely-used C benchmarks (i.e., cBench and PolyBench). The results show that BOCA significantly outperforms the state-of-the-art compiler autotuning approaches and Bayesion optimization methods in terms of the time spent on achieving specified speedups, demonstrating the effectiveness of BOCA.",
    "bibtex": "@inproceedings{chen2021efficient,\n abstract = {A typical compiler such as GCC supports hundreds of optimizations controlled by compilation flags for improving the runtime performance of the compiled program. Due to the large number of compilation flags and the exponential number of flag combinations, it is impossible for compiler users to manually tune these optimization flags in order to achieve the required runtime performance of the compiled programs. Over the years, many compiler autotuning approaches have been proposed to automatically tune optimization flags, but they still suffer from the efficiency problem due to the huge search space. In this paper, we propose the first Bayesian optimization based approach, called BOCA, for efficient compiler autotuning. In BOCA, we leverage a tree-based model for approximating the objective function in order to make Bayesian optimization scalable to a large number of optimization flags. Moreover, we design a novel searching strategy to improve the efficiency of Bayesian optimization by incorporating the impact of each optimization flag measured by the tree-based model and a decay function to strike a balance between exploitation and exploration. We conduct extensive experiments to investigate the effectiveness of BOCA on two most popular C compilers (i.e., GCC and LLVM) and two widely-used C benchmarks (i.e., cBench and PolyBench). The results show that BOCA significantly outperforms the state-of-the-art compiler autotuning approaches and Bayesion optimization methods in terms of the time spent on achieving specified speedups, demonstrating the effectiveness of BOCA.},\n author = {Chen, Junjie and Xu, Ningxin and Chen, Peiqi and Zhang, Hongyu},\n booktitle = {2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},\n keywords = {Bayesian},\n link = {https://ieeexplore.ieee.org/abstract/document/9401979},\n organization = {IEEE},\n pages = {1198--1209},\n title = {Efficient compiler autotuning via bayesian optimization},\n year = {2021}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/9401979"
  },
  {
    "id": "huang2019autophase",
    "title": "Autophase: Compiler phase-ordering for hls with deep reinforcement learning",
    "authors": "Huang, Qijing and Haj-Ali, Ameer and Moses, William and Xiang, John and Stoica, Ion and Asanovic, Krste and Wawrzynek, John",
    "year": "2019",
    "source": "2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "The performance of the code generated by a compiler depends on the order in which the optimization passes are applied. In high-level synthesis, the quality of the generated circuit relates directly to the code generated by the front-end compiler. Choosing a good order-often referred to as the phase-ordering problem-is an NP-hard problem. In this paper, we evaluate a new technique to address the phase-ordering problem: deep reinforcement learning. We implement a framework in the context of the LLVM compiler to optimize the ordering for HLS programs and compare the performance of deep reinforcement learning to state-of-the-art algorithms that address the phase-ordering problem. Overall, our framework runs one to two orders of magnitude faster than these algorithms, and achieves a 16% improvement in circuit performance over the -O3 compiler flag.",
    "bibtex": "@inproceedings{huang2019autophase,\n abstract = {The performance of the code generated by a compiler depends on the order in which the optimization passes are applied. In high-level synthesis, the quality of the generated circuit relates directly to the code generated by the front-end compiler. Choosing a good order-often referred to as the phase-ordering problem-is an NP-hard problem. In this paper, we evaluate a new technique to address the phase-ordering problem: deep reinforcement learning. We implement a framework in the context of the LLVM compiler to optimize the ordering for HLS programs and compare the performance of deep reinforcement learning to state-of-the-art algorithms that address the phase-ordering problem. Overall, our framework runs one to two orders of magnitude faster than these algorithms, and achieves a 16% improvement in circuit performance over the -O3 compiler flag.},\n author = {Huang, Qijing and Haj-Ali, Ameer and Moses, William and Xiang, John and Stoica, Ion and Asanovic, Krste and Wawrzynek, John},\n booktitle = {2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},\n link = {https://doi.org/10.1109/FCCM.2019.00049},\n organization = {IEEE},\n pages = {308--308},\n title = {Autophase: Compiler phase-ordering for hls with deep reinforcement learning},\n year = {2019}\n}\n",
    "link": "https://doi.org/10.1109/FCCM.2019.00049"
  },
  {
    "id": "ashouri2017micomp",
    "title": "Micomp: Mitigating the compiler phase-ordering problem using optimization sub-sequences and machine learning",
    "authors": "Ashouri, Amir H and Bignoli, Andrea and Palermo, Gianluca and Silvano, Cristina and Kulkarni, Sameer and Cavazos, John",
    "year": "2017",
    "source": "ACM Transactions on Architecture and Code Optimization (TACO)",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Recent compilers offer a vast number of multilayered optimizations targeting different code segments of an application. Choosing among these optimizations can significantly impact the performance of the code being optimized. The selection of the right set of compiler optimizations for a particular code segment is a very hard problem, but finding the best ordering of these optimizations adds further complexity. Finding the best ordering represents a long standing problem in compilation research, named the phase-ordering problem. The traditional approach of constructing compiler heuristics to solve this problem simply cannot cope with the enormous complexity of choosing the right ordering of optimizations for every code segment in an application.This article proposes an automatic optimization framework we call MiCOMP, which <u>Mi</u>tigates the <u>Com</u>piler <u>P</u>hase-ordering problem. We perform phase ordering of the optimizations in LLVM’s highest optimization level using optimization sub-sequences and machine learning. The idea is to cluster the optimization passes of LLVM’s O3 setting into different clusters to predict the speedup of a complete sequence of all the optimization clusters instead of having to deal with the ordering of more than 60 different individual optimizations. The predictive model uses (1) dynamic features, (2) an encoded version of the compiler sequence, and (3) an exploration heuristic to tackle the problem.Experimental results using the LLVM compiler framework and the Cbench suite show the effectiveness of the proposed clustering and encoding techniques to application-based reordering of passes, while using a number of predictive models. We perform statistical analysis on the results and compare against (1) random iterative compilation, (2) standard optimization levels, and (3) two recent prediction approaches. We show that MiCOMP’s iterative compilation using its sub-sequences can reach an average performance speedup of 1.31 (up to 1.51). Additionally, we demonstrate that MiCOMP’s prediction model outperforms the -O1, -O2, and -O3 optimization levels within using just a few predictions and reduces the prediction error rate down to only 5%. Overall, it achieves 90% of the available speedup by exploring less than 0.001% of the optimization space.",
    "bibtex": "@article{ashouri2017micomp,\n abstract = {Recent compilers offer a vast number of multilayered optimizations targeting different code segments of an application. Choosing among these optimizations can significantly impact the performance of the code being optimized. The selection of the right set of compiler optimizations for a particular code segment is a very hard problem, but finding the best ordering of these optimizations adds further complexity. Finding the best ordering represents a long standing problem in compilation research, named the phase-ordering problem. The traditional approach of constructing compiler heuristics to solve this problem simply cannot cope with the enormous complexity of choosing the right ordering of optimizations for every code segment in an application.This article proposes an automatic optimization framework we call MiCOMP, which <u>Mi</u>tigates the <u>Com</u>piler <u>P</u>hase-ordering problem. We perform phase ordering of the optimizations in LLVM’s highest optimization level using optimization sub-sequences and machine learning. The idea is to cluster the optimization passes of LLVM’s O3 setting into different clusters to predict the speedup of a complete sequence of all the optimization clusters instead of having to deal with the ordering of more than 60 different individual optimizations. The predictive model uses (1) dynamic features, (2) an encoded version of the compiler sequence, and (3) an exploration heuristic to tackle the problem.Experimental results using the LLVM compiler framework and the Cbench suite show the effectiveness of the proposed clustering and encoding techniques to application-based reordering of passes, while using a number of predictive models. We perform statistical analysis on the results and compare against (1) random iterative compilation, (2) standard optimization levels, and (3) two recent prediction approaches. We show that MiCOMP’s iterative compilation using its sub-sequences can reach an average performance speedup of 1.31 (up to 1.51). Additionally, we demonstrate that MiCOMP’s prediction model outperforms the -O1, -O2, and -O3 optimization levels within using just a few predictions and reduces the prediction error rate down to only 5%. Overall, it achieves 90% of the available speedup by exploring less than 0.001% of the optimization space.},\n author = {Ashouri, Amir H and Bignoli, Andrea and Palermo, Gianluca and Silvano, Cristina and Kulkarni, Sameer and Cavazos, John},\n journal = {ACM Transactions on Architecture and Code Optimization (TACO)},\n link = {https://dl.acm.org/doi/abs/10.1145/3124452},\n number = {3},\n pages = {1--28},\n publisher = {ACM New York, NY, USA},\n title = {Micomp: Mitigating the compiler phase-ordering problem using optimization sub-sequences and machine learning},\n volume = {14},\n year = {2017}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3124452"
  },
  {
    "id": "seeker2024revealing",
    "title": "Revealing compiler heuristics through automated discovery and optimization",
    "authors": "Seeker, Volker and Cummins, Chris and Cole, Murray and Franke, Bj{\\\"o}rn and Hazelwood, Kim and Leather, Hugh",
    "year": "2024",
    "source": "2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)",
    "category": "interpass-tuning",
    "keywords": [
      "search space"
    ],
    "abstract": "Tuning compiler heuristics and parameters is well known to improve optimization outcomes dramatically. Prior works have tuned command line flags and a few expert identified heuristics. However, there are an unknown number of heuristics buried, unmarked and unexposed inside the compiler as a consequence of decades of development without auto-tuning being foremost in the minds of developers. Many may not even have been considered heuristics by the developers who wrote them. The result is that auto-tuning search and machine learning can optimize only a tiny fraction of what could be possible if all heuristics were available to tune. Manually discovering all of these heuristics hidden among millions of lines of code and exposing them to auto-tuning tools is a Herculean task that is simply not practical. What is needed is a method of automatically finding these heuristics to extract every last drop of potential optimization. In this work, we propose Heureka, a framework that automatically identifies potential heuristics in the compiler that are highly profitable optimization targets and then automatically finds available tuning parameters for those heuristics with minimal human involvement. Our work is based on the following key insight: When modifying the output of a heuristic within an acceptable value range, the calling code using that output will still function correctly and produce semantically correct results. Building on that, we automatically manipulate the output of potential heuristic code in the compiler and decide using a Differential Testing approach if we found a heuristic or not. During output manipulation, we also explore acceptable value ranges of the targeted code. Heuristics identified in this way can then be tuned to optimize an objective function. We used Heureka to search for heuristics among eight thousand functions from the LLVM optimization passes, which is about 2% of all available functions. We then use identified heuristics to tune the compilation of 38 applications from the NAS and Polybench benchmark suites. Compared to an -ozbaseline we reduce binary sizes by up to 11.6% considering single heuristics only and up to 19.5% when stacking the effects of multiple identified tuning targets and applying a random search with minimal search effort. Generalizing from existing analysis results, Heureka needs, on average, a little under an hour on a single machine to identify relevant heuristic targets for a previously unseen application.",
    "bibtex": "@inproceedings{seeker2024revealing,\n abstract = {Tuning compiler heuristics and parameters is well known to improve optimization outcomes dramatically. Prior works have tuned command line flags and a few expert identified heuristics. However, there are an unknown number of heuristics buried, unmarked and unexposed inside the compiler as a consequence of decades of development without auto-tuning being foremost in the minds of developers. Many may not even have been considered heuristics by the developers who wrote them. The result is that auto-tuning search and machine learning can optimize only a tiny fraction of what could be possible if all heuristics were available to tune. Manually discovering all of these heuristics hidden among millions of lines of code and exposing them to auto-tuning tools is a Herculean task that is simply not practical. What is needed is a method of automatically finding these heuristics to extract every last drop of potential optimization. In this work, we propose Heureka, a framework that automatically identifies potential heuristics in the compiler that are highly profitable optimization targets and then automatically finds available tuning parameters for those heuristics with minimal human involvement. Our work is based on the following key insight: When modifying the output of a heuristic within an acceptable value range, the calling code using that output will still function correctly and produce semantically correct results. Building on that, we automatically manipulate the output of potential heuristic code in the compiler and decide using a Differential Testing approach if we found a heuristic or not. During output manipulation, we also explore acceptable value ranges of the targeted code. Heuristics identified in this way can then be tuned to optimize an objective function. We used Heureka to search for heuristics among eight thousand functions from the LLVM optimization passes, which is about 2% of all available functions. We then use identified heuristics to tune the compilation of 38 applications from the NAS and Polybench benchmark suites. Compared to an -ozbaseline we reduce binary sizes by up to 11.6% considering single heuristics only and up to 19.5% when stacking the effects of multiple identified tuning targets and applying a random search with minimal search effort. Generalizing from existing analysis results, Heureka needs, on average, a little under an hour on a single machine to identify relevant heuristic targets for a previously unseen application.},\n author = {Seeker, Volker and Cummins, Chris and Cole, Murray and Franke, Bj{\\\"o}rn and Hazelwood, Kim and Leather, Hugh},\n booktitle = {2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},\n keywords = {search space},\n link = {https://doi.org/10.1109/CGO57630.2024.10444847},\n organization = {IEEE},\n pages = {55--66},\n title = {Revealing compiler heuristics through automated discovery and optimization},\n year = {2024}\n}\n",
    "link": "https://doi.org/10.1109/CGO57630.2024.10444847"
  },
  {
    "id": "burgstaller2024optimization",
    "title": "Optimization Space Learning: A Lightweight, Noniterative Technique for Compiler Autotuning",
    "authors": "Burgstaller, Tamim and Garber, Damian and Le, Viet-Man and Felfernig, Alexander",
    "year": "2024",
    "source": "Proceedings of the 28th ACM International Systems and Software Product Line Conference",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Compilers are highly configurable systems. One can influence the performance of a compiled program by activating and deactivating selected compiler optimizations. However, automatically finding well-performing configurations is a challenging task. We consider expensive iteration, paired with recompilation of the program to optimize, as one of the main shortcomings of state-of-the-art approaches. Therefore, we propose Optimization Space Learning, a lightweight and noniterative technique. It exploits concepts known from configuration space learning and recommender systems to discover well-performing compiler configurations. This reduces the overhead induced by the approach significantly, compared to existing approaches. The process of finding a well-performing configuration is 800k times faster than with the state-of-the-art techniques.",
    "bibtex": "@inproceedings{burgstaller2024optimization,\n abstract = {Compilers are highly configurable systems. One can influence the performance of a compiled program by activating and deactivating selected compiler optimizations. However, automatically finding well-performing configurations is a challenging task. We consider expensive iteration, paired with recompilation of the program to optimize, as one of the main shortcomings of state-of-the-art approaches. Therefore, we propose Optimization Space Learning, a lightweight and noniterative technique. It exploits concepts known from configuration space learning and recommender systems to discover well-performing compiler configurations. This reduces the overhead induced by the approach significantly, compared to existing approaches. The process of finding a well-performing configuration is 800k times faster than with the state-of-the-art techniques.},\n author = {Burgstaller, Tamim and Garber, Damian and Le, Viet-Man and Felfernig, Alexander},\n booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},\n link = {https://dl.acm.org/doi/abs/10.1145/3646548.3672588},\n pages = {36--46},\n title = {Optimization Space Learning: A Lightweight, Noniterative Technique for Compiler Autotuning},\n year = {2024}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3646548.3672588"
  },
  {
    "id": "garciarena2016evolutionary",
    "title": "Evolutionary optimization of compiler flag selection by learning and exploiting flags interactions",
    "authors": "Garciarena, Unai and Santana, Roberto",
    "year": "2016",
    "source": "Proceedings of the 2016 on Genetic and Evolutionary Computation Conference Companion",
    "category": "interpass-tuning",
    "keywords": [
      "EA"
    ],
    "abstract": "Compiler flag selection can be an effective way to increase the quality of executable code according to different code quality criteria. Evolutionary algorithms have been successfully applied to this optimization problem. However, previous approaches have only partially addressed the question of capturing and exploiting the interactions between compilation options to improve the search. In this paper we deal with this question comparing estimation of distribution algorithms (EDAs) and a traditional genetic algorithm approach. We show that EDAs that learn bivariate interactions can improve the results of GAs for some of the programs considered. We also show that the probabilistic models generated as a result of the search for optimal flag combinations can be used to unveil the (problem-dependent) interactions between the flags, allowing the user a more informed choice of compilation options.",
    "bibtex": "@inproceedings{garciarena2016evolutionary,\n abstract = {Compiler flag selection can be an effective way to increase the quality of executable code according to different code quality criteria. Evolutionary algorithms have been successfully applied to this optimization problem. However, previous approaches have only partially addressed the question of capturing and exploiting the interactions between compilation options to improve the search. In this paper we deal with this question comparing estimation of distribution algorithms (EDAs) and a traditional genetic algorithm approach. We show that EDAs that learn bivariate interactions can improve the results of GAs for some of the programs considered. We also show that the probabilistic models generated as a result of the search for optimal flag combinations can be used to unveil the (problem-dependent) interactions between the flags, allowing the user a more informed choice of compilation options.},\n author = {Garciarena, Unai and Santana, Roberto},\n booktitle = {Proceedings of the 2016 on Genetic and Evolutionary Computation Conference Companion},\n keywords = {EA},\n link = {https://dl.acm.org/doi/abs/10.1145/2908961.2931696},\n pages = {1159--1166},\n title = {Evolutionary optimization of compiler flag selection by learning and exploiting flags interactions},\n year = {2016}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/2908961.2931696"
  },
  {
    "id": "Gong2025OptimizingWB",
    "title": "Optimizing WebAssembly Bytecode for IoT Devices Using Deep Reinforcement Learning",
    "authors": "Kaijie Gong and Ruiqi Yang and Haoyu Li and Yi Gao and Wei Dong",
    "year": "2025",
    "source": "ACM Transactions on Internet Technology",
    "category": "interpass-tuning",
    "keywords": [
      "drl"
    ],
    "abstract": "WebAssembly has shown promising potential on various IoT devices to achieve the desired features such as multi-language support and seamless device-cloud integration. The execution performance of WebAssembly bytecode is directly influenced by compilation sequences. While existing research has explored the optimization of compilation sequences for native code, these approaches are not suitable to WebAssembly bytecode due to its unique instruction format and control flow graph structure. In this work, we propose WasmRL, a novel efficient deep reinforcement learning (DRL)-based compiler optimization framework tailored for WebAssembly bytecode. We conduct a fine-grained analysis of the characteristics of WebAssembly instructions and associated compilation flags. We observe that the same compilation sequence may yield contrasting performance outcomes in WebAssembly and native code. Motivated by our observation, we introduce a WebAssembly-specific DRL state representation that simultaneously captures the impact of various compilation sequences on the WebAssembly bytecode and its runtime performance. To enhance the training efficiency of the DRL model, we propose a tree-based action space refinement method. Furthermore, we develop a pluggable cross-platform training strategy to optimize WebAssembly bytecode across different IoT devices. We evaluate the performance of WasmRL extensively on PolybenchC, MiBench, Shootout public datasets and real-world IoT applications. Experimental results show: (1) The DRL model trained on a specific device achieves 1.4x/1.1x speedups over -O3 for seen/unseen programs; (2) The DRL model trained on different devices simultaneously achieves 1.21x/1.06x improvements respectively. The code has been available at https://github.com/CarrollAdmin/WasmRL.",
    "bibtex": "@article{Gong2025OptimizingWB,\n abstract = {WebAssembly has shown promising potential on various IoT devices to achieve the desired features such as multi-language support and seamless device-cloud integration. The execution performance of WebAssembly bytecode is directly influenced by compilation sequences. While existing research has explored the optimization of compilation sequences for native code, these approaches are not suitable to WebAssembly bytecode due to its unique instruction format and control flow graph structure. In this work, we propose WasmRL, a novel efficient deep reinforcement learning (DRL)-based compiler optimization framework tailored for WebAssembly bytecode. We conduct a fine-grained analysis of the characteristics of WebAssembly instructions and associated compilation flags. We observe that the same compilation sequence may yield contrasting performance outcomes in WebAssembly and native code. Motivated by our observation, we introduce a WebAssembly-specific DRL state representation that simultaneously captures the impact of various compilation sequences on the WebAssembly bytecode and its runtime performance. To enhance the training efficiency of the DRL model, we propose a tree-based action space refinement method. Furthermore, we develop a pluggable cross-platform training strategy to optimize WebAssembly bytecode across different IoT devices. We evaluate the performance of WasmRL extensively on PolybenchC, MiBench, Shootout public datasets and real-world IoT applications. Experimental results show: (1) The DRL model trained on a specific device achieves 1.4x/1.1x speedups over -O3 for seen/unseen programs; (2) The DRL model trained on different devices simultaneously achieves 1.21x/1.06x improvements respectively. The code has been available at https://github.com/CarrollAdmin/WasmRL.},\n author = {Kaijie Gong and Ruiqi Yang and Haoyu Li and Yi Gao and Wei Dong},\n journal = {ACM Transactions on Internet Technology},\n keywords = {drl},\n link = {https://dl.acm.org/doi/abs/10.1145/3731451},\n pages = {1 - 26},\n title = {Optimizing WebAssembly Bytecode for IoT Devices Using Deep Reinforcement Learning},\n volume = {25},\n year = {2025}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3731451"
  },
  {
    "id": "zhu2023compiler",
    "title": "Compiler auto-tuning via critical flag selection",
    "authors": "Zhu, Mingxuan and Hao, Dan",
    "year": "2023",
    "source": "2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
    "category": "interpass-tuning",
    "keywords": [
      "gcc"
    ],
    "abstract": "Widely used compilers like GCC usually have hundreds of optimizations controlled by optimization flags, which can be enabled or disabled during compilation to improve the runtime performance of a compiled program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to tune compiler optimization flags manually. In the literature, many auto-tuning techniques have been proposed, which find a desired setting on all optimization flags (i.e., an optimization sequence) by designing different search strategies in the entire optimization space. Due to the huge search space, these techniques suffer from the widely-recognized efficiency problem. To reduce the search space, in this paper, we propose a critical-flag selection based approach CFSCA which first finds flags potentially relevant to the target program by analyzing program structure and compiler documentation, and then identifies critical flags through statistical analysis on the program's predicted runtime performance with various optimization sequences. With the reduced search space, CFSCA selects a desired optimization sequence. To evaluate the performance of the proposed approach CFSCA, we conduct an extensive experimental study on the latest version of the compiler GCC with a widely used benchmark cBench. The experimental results show that CFSCA significantly outperforms the four compared techniques, including the state-of-art technique BOCA.",
    "bibtex": "@inproceedings{zhu2023compiler,\n abstract = {Widely used compilers like GCC usually have hundreds of optimizations controlled by optimization flags, which can be enabled or disabled during compilation to improve the runtime performance of a compiled program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to tune compiler optimization flags manually. In the literature, many auto-tuning techniques have been proposed, which find a desired setting on all optimization flags (i.e., an optimization sequence) by designing different search strategies in the entire optimization space. Due to the huge search space, these techniques suffer from the widely-recognized efficiency problem. To reduce the search space, in this paper, we propose a critical-flag selection based approach CFSCA which first finds flags potentially relevant to the target program by analyzing program structure and compiler documentation, and then identifies critical flags through statistical analysis on the program's predicted runtime performance with various optimization sequences. With the reduced search space, CFSCA selects a desired optimization sequence. To evaluate the performance of the proposed approach CFSCA, we conduct an extensive experimental study on the latest version of the compiler GCC with a widely used benchmark cBench. The experimental results show that CFSCA significantly outperforms the four compared techniques, including the state-of-art technique BOCA.},\n author = {Zhu, Mingxuan and Hao, Dan},\n booktitle = {2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)},\n keywords = {gcc},\n link = {https://ieeexplore.ieee.org/abstract/document/10298446},\n organization = {IEEE},\n pages = {1000--1011},\n title = {Compiler auto-tuning via critical flag selection},\n year = {2023}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/10298446"
  },
  {
    "id": "cereda2020collaborative",
    "title": "A collaborative filtering approach for the automatic tuning of compiler optimisations",
    "authors": "Cereda, Stefano and Palermo, Gianluca and Cremonesi, Paolo and Doni, Stefano",
    "year": "2020",
    "source": "The 21st ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Selecting the right compiler optimisations has a severe impact on programs' performance. Still, the available optimisations keep increasing, and their effect depends on the specific program, making the task human intractable. Researchers proposed several techniques to search in the space of compiler optimisations. Some approaches focus on finding better search algorithms, while others try to speed up the search by leveraging previously collected knowledge. The possibility to effectively reuse previous compilation results inspired us toward the investigation of techniques derived from the Recommender Systems field. The proposed approach exploits previously collected knowledge and improves its characterisation over time. Differently from current state-of-the-art solutions, our approach is not based on performance counters but relies on Reaction Matching, an algorithm able to characterise programs looking at how they react to different optimisation sets. The proposed approach has been validated using two widely used benchmark suites, cBench and PolyBench, including 54 different programs. Our solution, on average, extracted 90% of the available performance improvement 10 iterations before current state-of-the-art solutions,which corresponds to 40% fewer compilations and performance tests to perform.",
    "bibtex": "@inproceedings{cereda2020collaborative,\n abstract = {Selecting the right compiler optimisations has a severe impact on programs' performance. Still, the available optimisations keep increasing, and their effect depends on the specific program, making the task human intractable. Researchers proposed several techniques to search in the space of compiler optimisations. Some approaches focus on finding better search algorithms, while others try to speed up the search by leveraging previously collected knowledge. The possibility to effectively reuse previous compilation results inspired us toward the investigation of techniques derived from the Recommender Systems field. The proposed approach exploits previously collected knowledge and improves its characterisation over time. Differently from current state-of-the-art solutions, our approach is not based on performance counters but relies on Reaction Matching, an algorithm able to characterise programs looking at how they react to different optimisation sets. The proposed approach has been validated using two widely used benchmark suites, cBench and PolyBench, including 54 different programs. Our solution, on average, extracted 90% of the available performance improvement 10 iterations before current state-of-the-art solutions,which corresponds to 40% fewer compilations and performance tests to perform.},\n author = {Cereda, Stefano and Palermo, Gianluca and Cremonesi, Paolo and Doni, Stefano},\n booktitle = {The 21st ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems},\n keywords = {},\n link = {https://dl.acm.org/doi/abs/10.1145/3372799.3394361},\n pages = {15--25},\n title = {A collaborative filtering approach for the automatic tuning of compiler optimisations},\n year = {2020}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3372799.3394361"
  },
  {
    "id": "shahzad2022reinforcement",
    "title": "Reinforcement learning strategies for compiler optimization in high level synthesis",
    "authors": "Shahzad, Hafsah and Sanaullah, Ahmed and Arora, Sanjay and Munafo, Robert and Yao, Xiteng and Drepper, Ulrich and Herbordt, Martin",
    "year": "2022",
    "source": "2022 IEEE/ACM Eighth Workshop on the LLVM Compiler Infrastructure in HPC (LLVM-HPC)",
    "category": "interpass-tuning",
    "keywords": [
      "ML",
      "llvm",
      "HLS"
    ],
    "abstract": "High Level Synthesis (HLS) offers a possible programmability solution for FPGAs by automatically compiling CPU codes to custom hardware configurations, but currently delivers far lower hardware quality than circuits written using Hardware Description Languages (HDLs). One reason is because the standard set of code optimizations used by CPU compilers, such as LLVM, are not well suited for a FPGA back end. Code performance is impacted largely by the order in which passes are applied. Similarly, it is also imperative to find a reasonable number of passes to apply and the optimum pass parameter values. In order to bridge the gap between hand tuned and automatically generated hardware, it is thus important to determine the optimal sequence of passes for HLS compilations, which could vary substantially across different workloads. Machine learning (ML) offers one popular approach to automate finding optimal compiler passes but requires selecting the right method. Supervised ML is not ideal since it requires labeled data mapping workload to optimal (or close to optimal) sequence of passes, which is computationally prohibitive. Unsupervised ML techniques don’t take into account the requirement that a quantity representing performance needs to be maximized. Reinforcement learning, which represents the problem of maximizing longterm rewards without requiring labeled data has been used for such planning problems before. While much work has been done along these lines for compilers in general, that directed towards HLS has been limited and conservative. In this paper, we address these limitations by expanding both the number of learning strategies for HLS compiler tuning and the metrics used to evaluate their impact. Our results show improvements over state-of-art for each standard benchmark evaluated and learning quality metric investigated. Choosing just the right strategy can give an improvement of 23× in learning speed, 4× in performance potential, 3×...",
    "bibtex": "@inproceedings{shahzad2022reinforcement,\n abstract = {High Level Synthesis (HLS) offers a possible programmability solution for FPGAs by automatically compiling CPU codes to custom hardware configurations, but currently delivers far lower hardware quality than circuits written using Hardware Description Languages (HDLs). One reason is because the standard set of code optimizations used by CPU compilers, such as LLVM, are not well suited for a FPGA back end. Code performance is impacted largely by the order in which passes are applied. Similarly, it is also imperative to find a reasonable number of passes to apply and the optimum pass parameter values. In order to bridge the gap between hand tuned and automatically generated hardware, it is thus important to determine the optimal sequence of passes for HLS compilations, which could vary substantially across different workloads. Machine learning (ML) offers one popular approach to automate finding optimal compiler passes but requires selecting the right method. Supervised ML is not ideal since it requires labeled data mapping workload to optimal (or close to optimal) sequence of passes, which is computationally prohibitive. Unsupervised ML techniques don’t take into account the requirement that a quantity representing performance needs to be maximized. Reinforcement learning, which represents the problem of maximizing longterm rewards without requiring labeled data has been used for such planning problems before. While much work has been done along these lines for compilers in general, that directed towards HLS has been limited and conservative. In this paper, we address these limitations by expanding both the number of learning strategies for HLS compiler tuning and the metrics used to evaluate their impact. Our results show improvements over state-of-art for each standard benchmark evaluated and learning quality metric investigated. Choosing just the right strategy can give an improvement of 23× in learning speed, 4× in performance potential, 3×...},\n author = {Shahzad, Hafsah and Sanaullah, Ahmed and Arora, Sanjay and Munafo, Robert and Yao, Xiteng and Drepper, Ulrich and Herbordt, Martin},\n booktitle = {2022 IEEE/ACM Eighth Workshop on the LLVM Compiler Infrastructure in HPC (LLVM-HPC)},\n keywords = {ML,llvm,HLS},\n link = {https://ieeexplore.ieee.org/abstract/document/10027131},\n organization = {IEEE},\n pages = {13--22},\n title = {Reinforcement learning strategies for compiler optimization in high level synthesis},\n year = {2022}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/10027131"
  },
  {
    "id": "xiao2024eatuner",
    "title": "EAtuner: Comparative Study of Evolutionary Algorithms for Compiler Auto-tuning",
    "authors": "Xiao, Guojian and Qin, Siyuan and Li, Kuan and Chen, Juan and Yin, Jianping",
    "year": "2024",
    "source": "2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)",
    "category": "interpass-tuning",
    "keywords": [
      "DE",
      "llvm"
    ],
    "abstract": "The manual adjustment of compilation flags by compiler users is impractical due to the exponential size of the search space. To address this, machine learning-based compiler auto-tuning methods, particularly evolutionary algorithms, have been proposed. However, existing works use different benchmarks and experimental setups, making it difficult to compare the strengths and weaknesses of various algorithms. To address this, we present EAtuner, an evolutionary algorithm-based framework for compiler auto-tuning, with the goal of benchmarking and identifying suitable algorithms for compiler auto-tuning. We implement ten discrete binary evolutionary algorithms and evaluate their effectiveness on the LLVM compiler through experiments. Notably, eight of these algorithms have not been previously applied to compiler flag optimization problems before our work. The results show that all ten algorithms can effectively achieve compiler auto-tuning, resulting in an average speedup of 1.204. However, there are notable differences in the effectiveness and efficiency of each algorithm, particularly in optimization efficiency, which is positively correlated with the number of program compilations. Based on this, we classify the algorithms into three levels, with Differential Evolution (DE) showing significant advantages in optimization effectiveness and efficiency. Additionally, we provide a comprehensive summary of the applicability of compiler flags, the correlation between them, and their relationship with programs.",
    "bibtex": "@inproceedings{xiao2024eatuner,\n abstract = {The manual adjustment of compilation flags by compiler users is impractical due to the exponential size of the search space. To address this, machine learning-based compiler auto-tuning methods, particularly evolutionary algorithms, have been proposed. However, existing works use different benchmarks and experimental setups, making it difficult to compare the strengths and weaknesses of various algorithms. To address this, we present EAtuner, an evolutionary algorithm-based framework for compiler auto-tuning, with the goal of benchmarking and identifying suitable algorithms for compiler auto-tuning. We implement ten discrete binary evolutionary algorithms and evaluate their effectiveness on the LLVM compiler through experiments. Notably, eight of these algorithms have not been previously applied to compiler flag optimization problems before our work. The results show that all ten algorithms can effectively achieve compiler auto-tuning, resulting in an average speedup of 1.204. However, there are notable differences in the effectiveness and efficiency of each algorithm, particularly in optimization efficiency, which is positively correlated with the number of program compilations. Based on this, we classify the algorithms into three levels, with Differential Evolution (DE) showing significant advantages in optimization effectiveness and efficiency. Additionally, we provide a comprehensive summary of the applicability of compiler flags, the correlation between them, and their relationship with programs.},\n author = {Xiao, Guojian and Qin, Siyuan and Li, Kuan and Chen, Juan and Yin, Jianping},\n booktitle = {2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)},\n keywords = {DE,llvm},\n link = {https://ieeexplore.ieee.org/abstract/document/10580120},\n organization = {IEEE},\n pages = {419--426},\n title = {EAtuner: Comparative Study of Evolutionary Algorithms for Compiler Auto-tuning},\n year = {2024}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/10580120"
  },
  {
    "id": "gao2025grouptuner",
    "title": "Grouptuner: Efficient Group-Aware Compiler Auto-tuning",
    "authors": "Gao, Bingyu and Yao, Mengyu and Wang, Ziming and Liu, Dong and Li, Ding and Chen, Xiangqun and Guo, Yao",
    "year": "2025",
    "source": "Proceedings of the 26th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Modern compilers typically provide hundreds of options to optimize program performance, but users often cannot fully leverage them due to the huge number of options. While standard optimization combinations (e.g., -O3) provide reasonable defaults, they often fail to deliver near-peak performance across diverse programs and architectures. To address this challenge, compiler auto-tuning techniques have emerged to automate the discovery of improved option combinations. Existing techniques typically focus on identifying critical options and prioritizing them during the search to improve efficiency. However, due to limited tuning iterations, the resulting data is often sparse and noisy, making it highly challenging to accurately identify critical options. As a result, these algorithms are prone to being trapped in local optima. To address this limitation, we propose GroupTuner, a group-aware auto-tuning technique that directly applies localized mutation to coherent option groups based on historically best-performing combinations, thus avoiding explicitly identifying critical options. By forgoing the need to know precisely which options are most important, GroupTuner maximizes the use of existing performance data, ensuring more targeted exploration. Extensive experiments demonstrate that GroupTuner can efficiently discover competitive option combinations, achieving an average performance improvement of 12.39% over -O3 while requiring only 77.21% of the time compared to the random search algorithm, significantly outperforming state-of-the-art methods.",
    "bibtex": "@inproceedings{gao2025grouptuner,\n abstract = {Modern compilers typically provide hundreds of options to optimize program performance, but users often cannot fully leverage them due to the huge number of options. While standard optimization combinations (e.g., -O3) provide reasonable defaults, they often fail to deliver near-peak performance across diverse programs and architectures. To address this challenge, compiler auto-tuning techniques have emerged to automate the discovery of improved option combinations. Existing techniques typically focus on identifying critical options and prioritizing them during the search to improve efficiency. However, due to limited tuning iterations, the resulting data is often sparse and noisy, making it highly challenging to accurately identify critical options. As a result, these algorithms are prone to being trapped in local optima. To address this limitation, we propose GroupTuner, a group-aware auto-tuning technique that directly applies localized mutation to coherent option groups based on historically best-performing combinations, thus avoiding explicitly identifying critical options. By forgoing the need to know precisely which options are most important, GroupTuner maximizes the use of existing performance data, ensuring more targeted exploration. Extensive experiments demonstrate that GroupTuner can efficiently discover competitive option combinations, achieving an average performance improvement of 12.39% over -O3 while requiring only 77.21% of the time compared to the random search algorithm, significantly outperforming state-of-the-art methods.},\n author = {Gao, Bingyu and Yao, Mengyu and Wang, Ziming and Liu, Dong and Li, Ding and Chen, Xiangqun and Guo, Yao},\n booktitle = {Proceedings of the 26th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},\n keywords = {},\n link = {https://doi.org/10.1145/3735452.3735530},\n pages = {122--133},\n title = {Grouptuner: Efficient Group-Aware Compiler Auto-tuning},\n year = {2025}\n}\n",
    "link": "https://doi.org/10.1145/3735452.3735530"
  },
  {
    "id": "ansel2014opentuner",
    "title": "Opentuner: An extensible framework for program autotuning",
    "authors": "Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and Ragan-Kelley, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman",
    "year": "2014",
    "source": "Proceedings of the 23rd international conference on Parallel architectures and compilation",
    "category": "interpass-tuning",
    "keywords": [],
    "abstract": "Program autotuning has been shown to achieve better or more portable performance in a number of domains. However, autotuners themselves are rarely portable between projects, for a number of reasons: using a domain-informed search space representation is critical to achieving good results; search spaces can be intractably large and require advanced machine learning techniques; and the landscape of search spaces can vary greatly between different problems, sometimes requiring domain specific search techniques to explore efficiently. This paper introduces OpenTuner, a new open source framework for building domain-specific multi-objective program autotuners. OpenTuner supports fully-customizable configuration representations, an extensible technique representation to allow for domain-specific techniques, and an easy to use interface for communicating with the program to be autotuned. A key capability inside OpenTuner is the use of ensembles of disparate search techniques simultaneously; techniques that perform well will dynamically be allocated a larger proportion of tests. We demonstrate the efficacy and generality of OpenTuner by building autotuners for 7 distinct projects and 16 total benchmarks, showing speedups over prior techniques of these projects of up to 2.8x with little programmer effort.",
    "bibtex": "@inproceedings{ansel2014opentuner,\n abstract = {Program autotuning has been shown to achieve better or more portable performance in a number of domains. However, autotuners themselves are rarely portable between projects, for a number of reasons: using a domain-informed search space representation is critical to achieving good results; search spaces can be intractably large and require advanced machine learning techniques; and the landscape of search spaces can vary greatly between different problems, sometimes requiring domain specific search techniques to explore efficiently. This paper introduces OpenTuner, a new open source framework for building domain-specific multi-objective program autotuners. OpenTuner supports fully-customizable configuration representations, an extensible technique representation to allow for domain-specific techniques, and an easy to use interface for communicating with the program to be autotuned. A key capability inside OpenTuner is the use of ensembles of disparate search techniques simultaneously; techniques that perform well will dynamically be allocated a larger proportion of tests. We demonstrate the efficacy and generality of OpenTuner by building autotuners for 7 distinct projects and 16 total benchmarks, showing speedups over prior techniques of these projects of up to 2.8x with little programmer effort.},\n author = {Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and Ragan-Kelley, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman},\n booktitle = {Proceedings of the 23rd international conference on Parallel architectures and compilation},\n keywords = {},\n link = {https://dl.acm.org/doi/abs/10.1145/2628071.2628092},\n pages = {303--316},\n title = {Opentuner: An extensible framework for program autotuning},\n year = {2014}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/2628071.2628092"
  },
  {
    "id": "li2022unleashing",
    "title": "Unleashing the power of compiler intermediate representation to enhance neural program embeddings",
    "authors": "Li, Zongjie and Ma, Pingchuan and Wang, Huaijin and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi",
    "year": "2022",
    "source": "Proceedings of the 44th International Conference on Software Engineering",
    "category": "representation-learning",
    "keywords": [
      "GA",
      "embedding"
    ],
    "abstract": "Neural program embeddings have demonstrated considerable promise in a range of program analysis tasks, including clone identification, program repair, code completion, and program synthesis. However, most existing methods generate neural program embeddings directly from the program source codes, by learning from features such as tokens, abstract syntax trees, and control flow graphs. This paper takes a fresh look at how to improve program embeddings by leveraging compiler intermediate representation (IR). We first demonstrate simple yet highly effective methods for enhancing embedding quality by training embedding models alongside source code and LLVM IR generated by default optimization levels (e.g., -O2). We then introduce IRGen, a framework based on genetic algorithms (GA), to identify (near-)optimal sequences of optimization flags that can significantly improve embedding quality. We use IRGen to find optimal sequences of LLVM optimization flags by performing GA on source code datasets. We then extend a popular code embedding model, CodeCMR, by adding a new objective based on triplet loss to enable a joint learning over source code and LLVM IR. We benchmark the quality of embedding using a representative downstream application, code clone detection. When CodeCMR was trained with source code and LLVM IRs optimized by findings of IRGen, the embedding quality was significantly improved, outperforming the state-of-the-art model, CodeBERT, which was trained only with source code. Our augmented CodeCMR also outperformed CodeCMR trained over source code and IR optimized with default optimization levels. We investigate the properties of optimization flags that increase embedding quality, demonstrate IRGen's generalization in boosting other embedding models, and establish IRGen's use in settings with extremely limited training data. Our research and findings demonstrate that a straightforward addition to modern neural code embedding models can provide a highly effective enhancement.",
    "bibtex": "@inproceedings{li2022unleashing,\n abstract = {Neural program embeddings have demonstrated considerable promise in a range of program analysis tasks, including clone identification, program repair, code completion, and program synthesis. However, most existing methods generate neural program embeddings directly from the program source codes, by learning from features such as tokens, abstract syntax trees, and control flow graphs. This paper takes a fresh look at how to improve program embeddings by leveraging compiler intermediate representation (IR). We first demonstrate simple yet highly effective methods for enhancing embedding quality by training embedding models alongside source code and LLVM IR generated by default optimization levels (e.g., -O2). We then introduce IRGen, a framework based on genetic algorithms (GA), to identify (near-)optimal sequences of optimization flags that can significantly improve embedding quality. We use IRGen to find optimal sequences of LLVM optimization flags by performing GA on source code datasets. We then extend a popular code embedding model, CodeCMR, by adding a new objective based on triplet loss to enable a joint learning over source code and LLVM IR. We benchmark the quality of embedding using a representative downstream application, code clone detection. When CodeCMR was trained with source code and LLVM IRs optimized by findings of IRGen, the embedding quality was significantly improved, outperforming the state-of-the-art model, CodeBERT, which was trained only with source code. Our augmented CodeCMR also outperformed CodeCMR trained over source code and IR optimized with default optimization levels. We investigate the properties of optimization flags that increase embedding quality, demonstrate IRGen's generalization in boosting other embedding models, and establish IRGen's use in settings with extremely limited training data. Our research and findings demonstrate that a straightforward addition to modern neural code embedding models can provide a highly effective enhancement.},\n author = {Li, Zongjie and Ma, Pingchuan and Wang, Huaijin and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi},\n booktitle = {Proceedings of the 44th International Conference on Software Engineering},\n keywords = {GA,embedding},\n link = {https://dl.acm.org/doi/10.1145/3510003.3510217},\n pages = {2253--2265},\n title = {Unleashing the power of compiler intermediate representation to enhance neural program embeddings},\n year = {2022}\n}\n",
    "link": "https://dl.acm.org/doi/10.1145/3510003.3510217"
  },
  {
    "id": "dutta2024mirencoder",
    "title": "Mirencoder: Multi-modal ir-based pretrained embeddings for performance optimizations",
    "authors": "Dutta, Akash and Jannesari, Ali",
    "year": "2024",
    "source": "Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques",
    "category": "representation-learning",
    "keywords": [
      "multi-modal",
      "IR"
    ],
    "abstract": "One of the primary areas of interest in High Performance Computing is the improvement of performance of parallel workloads. Nowadays, compilable source code-based optimization tasks that employ deep learning often exploit LLVM Intermediate Representations (IRs) for extracting features from source code. Most such works target specific tasks, or are designed with a pre-defined set of heuristics. So far, pre-trained models are rare in this domain, but the possibilities have been widely discussed. Especially approaches mimicking large-language models (LLMs) have been proposed. But these have prohibitively large training costs. In this paper, we propose MIREncoder, a Multi-modal IR-based Auto-Encoder that can be pre-trained to generate a learned embedding space to be used for downstream tasks by machine learning-based approaches. A multi-modal approach enables us to better extract features from compilable programs. It allows us to better model code syntax, semantics and structure. For code-based performance optimizations, these features are very important while making optimization decisions. A pre-trained model/embedding implicitly enables the usage of transfer learning, and helps move away from task-specific trained models. Additionally, a pre-trained model used for downstream performance optimization should itself have reduced overhead, and be easily usable. These considerations have led us to propose a modeling approach that i) understands code semantics and structure, ii) enables use of transfer learning, and iii) is small and simple enough to be easily re-purposed or reused even with low resource availability. Our evaluations will show that our proposed approach can outperform the state of the art while reducing overhead.",
    "bibtex": "@inproceedings{dutta2024mirencoder,\n abstract = {One of the primary areas of interest in High Performance Computing is the improvement of performance of parallel workloads. Nowadays, compilable source code-based optimization tasks that employ deep learning often exploit LLVM Intermediate Representations (IRs) for extracting features from source code. Most such works target specific tasks, or are designed with a pre-defined set of heuristics. So far, pre-trained models are rare in this domain, but the possibilities have been widely discussed. Especially approaches mimicking large-language models (LLMs) have been proposed. But these have prohibitively large training costs. In this paper, we propose MIREncoder, a Multi-modal IR-based Auto-Encoder that can be pre-trained to generate a learned embedding space to be used for downstream tasks by machine learning-based approaches. A multi-modal approach enables us to better extract features from compilable programs. It allows us to better model code syntax, semantics and structure. For code-based performance optimizations, these features are very important while making optimization decisions. A pre-trained model/embedding implicitly enables the usage of transfer learning, and helps move away from task-specific trained models. Additionally, a pre-trained model used for downstream performance optimization should itself have reduced overhead, and be easily usable. These considerations have led us to propose a modeling approach that i) understands code semantics and structure, ii) enables use of transfer learning, and iii) is small and simple enough to be easily re-purposed or reused even with low resource availability. Our evaluations will show that our proposed approach can outperform the state of the art while reducing overhead.},\n author = {Dutta, Akash and Jannesari, Ali},\n booktitle = {Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques},\n keywords = {multi-modal,IR},\n link = {https://dl.acm.org/doi/abs/10.1145/3656019.3676895},\n pages = {156--167},\n title = {Mirencoder: Multi-modal ir-based pretrained embeddings for performance optimizations},\n year = {2024}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3656019.3676895"
  },
  {
    "id": "ben2018neural",
    "title": "Neural code comprehension: A learnable representation of code semantics",
    "authors": "Ben-Nun, Tal and Jakobovits, Alice Shoshana and Hoefler, Torsten",
    "year": "2018",
    "source": "Advances in neural information processing systems",
    "category": "representation-learning",
    "keywords": [
      "inst2vec",
      "IR"
    ],
    "abstract": "With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.",
    "bibtex": "@article{ben2018neural,\n abstract = {With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.},\n author = {Ben-Nun, Tal and Jakobovits, Alice Shoshana and Hoefler, Torsten},\n journal = {Advances in neural information processing systems},\n keywords = {inst2vec,IR},\n link = {https://proceedings.neurips.cc/paper/2018/hash/17c3433fecc21b57000debdf7ad5c930-Abstract.html},\n title = {Neural code comprehension: A learnable representation of code semantics},\n volume = {31},\n year = {2018}\n}\n",
    "link": "https://proceedings.neurips.cc/paper/2018/hash/17c3433fecc21b57000debdf7ad5c930-Abstract.html"
  },
  {
    "id": "brauckmann2020compiler",
    "title": "Compiler-based graph representations for deep learning models of code",
    "authors": "Brauckmann, Alexander and Goens, Andr{\\'e}s and Ertel, Sebastian and Castrillon, Jeronimo",
    "year": "2020",
    "source": "Proceedings of the 29th International Conference on Compiler Construction",
    "category": "representation-learning",
    "keywords": [
      "RNN",
      "AST"
    ],
    "abstract": "In natural language processing, novel methods in deep learning, like recurrent neural networks (RNNs) on sequences of words, have been very successful. In contrast to natural languages, programming languages usually have a well-defined structure. With this structure compilers can reason about programs, using graphs such as abstract syntax trees (ASTs) or control-data flow graphs (CDFGs). In this paper, we argue that we should use these graph structures instead of sequences for learning compiler optimization tasks. To this end, we use graph neural networks (GNNs) for learning predictive compiler tasks on two representations based on ASTs and CDFGs. Experiments show that this improves upon the state-of-the-art in the task of heterogeneous OpenCL mapping, while providing orders of magnitude faster inference times, crucial for compiler optimizations. When testing on benchmark suites not included for training, our AST-based model significantly outperforms the state-of-the-art by over 12 percentage points in terms of accuracy. It is the only one to perform clearly better than a random mapping. On the task of predicting thread coarsening factors, we show that all of the methods fail to produce an overall speedup.",
    "bibtex": "@inproceedings{brauckmann2020compiler,\n abstract = {In natural language processing, novel methods in deep learning, like recurrent neural networks (RNNs) on sequences of words, have been very successful. In contrast to natural languages, programming languages usually have a well-defined structure. With this structure compilers can reason about programs, using graphs such as abstract syntax trees (ASTs) or control-data flow graphs (CDFGs). In this paper, we argue that we should use these graph structures instead of sequences for learning compiler optimization tasks. To this end, we use graph neural networks (GNNs) for learning predictive compiler tasks on two representations based on ASTs and CDFGs. Experiments show that this improves upon the state-of-the-art in the task of heterogeneous OpenCL mapping, while providing orders of magnitude faster inference times, crucial for compiler optimizations. When testing on benchmark suites not included for training, our AST-based model significantly outperforms the state-of-the-art by over 12 percentage points in terms of accuracy. It is the only one to perform clearly better than a random mapping. On the task of predicting thread coarsening factors, we show that all of the methods fail to produce an overall speedup.},\n author = {Brauckmann, Alexander and Goens, Andr{\\'e}s and Ertel, Sebastian and Castrillon, Jeronimo},\n booktitle = {Proceedings of the 29th International Conference on Compiler Construction},\n keywords = {RNN,AST},\n link = {https://dl.acm.org/doi/abs/10.1145/3377555.3377894},\n pages = {201--211},\n title = {Compiler-based graph representations for deep learning models of code},\n year = {2020}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3377555.3377894"
  },
  {
    "id": "venkatakeerthy2020ir2vec",
    "title": "Ir2vec: Llvm ir based scalable program embeddings",
    "authors": "VenkataKeerthy, S and Aggarwal, Rohit and Jain, Shalini and Desarkar, Maunendra Sankar and Upadrasta, Ramakrishna and Srikant, YN",
    "year": "2020",
    "source": "ACM Transactions on Architecture and Code Optimization (TACO)",
    "category": "representation-learning",
    "keywords": [
      "IR"
    ],
    "abstract": "We propose IR2VEC, a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with flow information to capture the syntax as well as the semantics of the input programs. As our infrastructure is based on the Intermediate Representation (IR) of the source code, obtained embeddings are both language and machine independent. The entities of the IR are modeled as relationships, and their representations are learned to form a seed embedding vocabulary. Using this infrastructure, we propose two incremental encodings: Symbolic and Flow-Aware. Symbolic encodings are obtained from the seed embedding vocabulary, and Flow-Aware encodings are obtained by augmenting the Symbolic encodings with the flow information. We show the effectiveness of our methodology on two optimization tasks (Heterogeneous device mapping and Thread coarsening). Our way of representing the programs enables us to use non-sequential models resulting in orders of magnitude of faster training time. Both the encodings generated by IR2VEC outperform the existing methods in both the tasks, even while using simple machine learning models. In particular, our results improve or match the state-of-the-art speedup in 11/14 benchmark-suites in the device mapping task across two platforms and 53/68 benchmarks in the thread coarsening task across four different platforms. When compared to the other methods, our embeddings are more scalable, is non-data-hungry, and has better Out-Of-Vocabulary (OOV) characteristics.",
    "bibtex": "@article{venkatakeerthy2020ir2vec,\n abstract = {We propose IR2VEC, a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with flow information to capture the syntax as well as the semantics of the input programs. As our infrastructure is based on the Intermediate Representation (IR) of the source code, obtained embeddings are both language and machine independent. The entities of the IR are modeled as relationships, and their representations are learned to form a seed embedding vocabulary. Using this infrastructure, we propose two incremental encodings: Symbolic and Flow-Aware. Symbolic encodings are obtained from the seed embedding vocabulary, and Flow-Aware encodings are obtained by augmenting the Symbolic encodings with the flow information. We show the effectiveness of our methodology on two optimization tasks (Heterogeneous device mapping and Thread coarsening). Our way of representing the programs enables us to use non-sequential models resulting in orders of magnitude of faster training time. Both the encodings generated by IR2VEC outperform the existing methods in both the tasks, even while using simple machine learning models. In particular, our results improve or match the state-of-the-art speedup in 11/14 benchmark-suites in the device mapping task across two platforms and 53/68 benchmarks in the thread coarsening task across four different platforms. When compared to the other methods, our embeddings are more scalable, is non-data-hungry, and has better Out-Of-Vocabulary (OOV) characteristics.},\n author = {VenkataKeerthy, S and Aggarwal, Rohit and Jain, Shalini and Desarkar, Maunendra Sankar and Upadrasta, Ramakrishna and Srikant, YN},\n journal = {ACM Transactions on Architecture and Code Optimization (TACO)},\n keywords = {IR},\n link = {https://dl.acm.org/doi/abs/10.1145/3418463},\n number = {4},\n pages = {1--27},\n publisher = {ACM New York, NY, USA},\n title = {Ir2vec: Llvm ir based scalable program embeddings},\n volume = {17},\n year = {2020}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3418463"
  },
  {
    "id": "alon2019code2vec",
    "title": "code2vec: Learning distributed representations of code",
    "authors": "Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran",
    "year": "2019",
    "source": "Proceedings of the ACM on Programming Languages",
    "category": "representation-learning",
    "keywords": [
      "code2vec"
    ],
    "abstract": "We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. To this end, code is first decomposed to a collection of paths in its abstract syntax tree. Then, the network learns the atomic representation of each path while simultaneously learning how to aggregate a set of them. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 12M methods. We show that code vectors trained on this dataset can predict method names from files that were unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. A comparison of our approach to previous techniques over the same dataset shows an improvement of more than 75%, making it the first to successfully predict method names based on a large, cross-project corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at http://code2vec.org. The code, data and trained models are available at https://github.com/tech-srl/code2vec.",
    "bibtex": "@article{alon2019code2vec,\n abstract = {We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. To this end, code is first decomposed to a collection of paths in its abstract syntax tree. Then, the network learns the atomic representation of each path while simultaneously learning how to aggregate a set of them. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 12M methods. We show that code vectors trained on this dataset can predict method names from files that were unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. A comparison of our approach to previous techniques over the same dataset shows an improvement of more than 75%, making it the first to successfully predict method names based on a large, cross-project corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at http://code2vec.org. The code, data and trained models are available at https://github.com/tech-srl/code2vec.},\n author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},\n journal = {Proceedings of the ACM on Programming Languages},\n keywords = {code2vec},\n link = {https://dl.acm.org/doi/abs/10.1145/3290353},\n number = {POPL},\n pages = {1--29},\n publisher = {ACM New York, NY, USA},\n title = {code2vec: Learning distributed representations of code},\n volume = {3},\n year = {2019}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3290353"
  },
  {
    "id": "dutta2023performance",
    "title": "Performance optimization using multimodal modeling and heterogeneous GNN",
    "authors": "Dutta, Akash and Alcaraz, Jordi and TehraniJamsaz, Ali and Cesar, Eduardo and Sikora, Anna and Jannesari, Ali",
    "year": "2023",
    "source": "Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing",
    "category": "representation-learning",
    "keywords": [
      "GNN"
    ],
    "abstract": "Growing heterogeneity and configurability in HPC architectures has made auto-tuning applications and runtime parameters on these systems very complex. Users are presented with a multitude of options to configure parameters. In addition to application specific solutions, a common approach is to use general purpose search strategies, which often might not identify the best configurations or their time to convergence is a significant barrier. There is, thus, a need for a general purpose and efficient tuning approach that can be easily scaled and adapted to various tuning tasks. We propose a technique for tuning parallel code regions that is general enough to be adapted to multiple tasks. In this paper, we analyze IR-based programming models to make task-specific performance optimizations. To this end, we propose the Multimodal Graph Neural Network and Autoencoder (MGA) tuner, a multimodal deep learning based approach that adapts Heterogeneous Graph Neural Networks and Denoising Autoencoders for modeling IR-based code representations that serve as separate modalities. This approach is used as part of our pipeline to model a syntax, semantics, and structure-aware IR-based code representation for tuning parallel code regions/kernels. We extensively experiment on OpenMP and OpenCL code regions/kernels obtained from PolyBench, Rodinia, STREAM, DataRaceBench, AMD SDK, NPB, NVIDIA SDK, Parboil, SHOC, LULESH, XSBench, RSBench, miniFE, miniAMR, and Quicksilver benchmarks and applications. We apply our multimodal learning techniques to the tasks of (i) optimizing the number of threads, scheduling policy and chunk size in OpenMP loops and, (ii) identifying the best device for heterogeneous device mapping of OpenCL kernels. Our experiments show that this multimodal learning based approach outperforms the state-of-the-art in almost all experiments.",
    "bibtex": "@inproceedings{dutta2023performance,\n abstract = {Growing heterogeneity and configurability in HPC architectures has made auto-tuning applications and runtime parameters on these systems very complex. Users are presented with a multitude of options to configure parameters. In addition to application specific solutions, a common approach is to use general purpose search strategies, which often might not identify the best configurations or their time to convergence is a significant barrier. There is, thus, a need for a general purpose and efficient tuning approach that can be easily scaled and adapted to various tuning tasks. We propose a technique for tuning parallel code regions that is general enough to be adapted to multiple tasks. In this paper, we analyze IR-based programming models to make task-specific performance optimizations. To this end, we propose the Multimodal Graph Neural Network and Autoencoder (MGA) tuner, a multimodal deep learning based approach that adapts Heterogeneous Graph Neural Networks and Denoising Autoencoders for modeling IR-based code representations that serve as separate modalities. This approach is used as part of our pipeline to model a syntax, semantics, and structure-aware IR-based code representation for tuning parallel code regions/kernels. We extensively experiment on OpenMP and OpenCL code regions/kernels obtained from PolyBench, Rodinia, STREAM, DataRaceBench, AMD SDK, NPB, NVIDIA SDK, Parboil, SHOC, LULESH, XSBench, RSBench, miniFE, miniAMR, and Quicksilver benchmarks and applications. We apply our multimodal learning techniques to the tasks of (i) optimizing the number of threads, scheduling policy and chunk size in OpenMP loops and, (ii) identifying the best device for heterogeneous device mapping of OpenCL kernels. Our experiments show that this multimodal learning based approach outperforms the state-of-the-art in almost all experiments.},\n author = {Dutta, Akash and Alcaraz, Jordi and TehraniJamsaz, Ali and Cesar, Eduardo and Sikora, Anna and Jannesari, Ali},\n booktitle = {Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing},\n keywords = {GNN},\n link = {https://dl.acm.org/doi/abs/10.1145/3588195.3592984},\n pages = {45--57},\n title = {Performance optimization using multimodal modeling and heterogeneous GNN},\n year = {2023}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3588195.3592984"
  },
  {
    "id": "dutta2022pattern",
    "title": "Pattern-based autotuning of openmp loops using graph neural networks",
    "authors": "Dutta, Akash and Alcaraz, Jordi and TehraniJamsaz, Ali and Sikora, Anna and Cesar, Eduardo and Jannesari, Ali",
    "year": "2022",
    "source": "2022 IEEE/ACM International Workshop on Artificial Intelligence and Machine Learning for Scientific Applications (AI4S)",
    "category": "representation-learning",
    "keywords": [
      "Openmp",
      "GNN"
    ],
    "abstract": "Stagnation of Moore's law has led to the increased adoption of parallel programming for enhancing performance of scientific applications. Frequently occurring code and design patterns in scientific applications are often used for transforming serial code to parallel. But, identifying these patterns is not easy. To this end, we propose using Graph Neural Networks for modeling code flow graphs to identify patterns in such parallel code. Additionally, identifying the runtime parameters for best performing parallel code is also challenging. We propose a pattern-guided deep learning based tuning approach, to help identify the best runtime parameters for OpenMP loops. Overall, we aim to identify commonly occurring patterns in parallel loops and use these patterns to guide auto-tuning efforts. We validate our hypothesis on 20 different applications from Polybench, and STREAM benchmark suites. This deep learning-based approach can identify the considered patterns with an overall accuracy of 91%. We validate the usefulness of using patterns for auto-tuning on tuning the number of threads, scheduling policies and chunk size on a single socket system, and the thread count and affinity on a multi-socket machine. Our approach achieves geometric mean speedups of 1.1× and 4.7× respectively over default OpenMP configurations, compared to brute-force speedups of 1.27× and 4.93× respectively.",
    "bibtex": "@inproceedings{dutta2022pattern,\n abstract = {Stagnation of Moore's law has led to the increased adoption of parallel programming for enhancing performance of scientific applications. Frequently occurring code and design patterns in scientific applications are often used for transforming serial code to parallel. But, identifying these patterns is not easy. To this end, we propose using Graph Neural Networks for modeling code flow graphs to identify patterns in such parallel code. Additionally, identifying the runtime parameters for best performing parallel code is also challenging. We propose a pattern-guided deep learning based tuning approach, to help identify the best runtime parameters for OpenMP loops. Overall, we aim to identify commonly occurring patterns in parallel loops and use these patterns to guide auto-tuning efforts. We validate our hypothesis on 20 different applications from Polybench, and STREAM benchmark suites. This deep learning-based approach can identify the considered patterns with an overall accuracy of 91%. We validate the usefulness of using patterns for auto-tuning on tuning the number of threads, scheduling policies and chunk size on a single socket system, and the thread count and affinity on a multi-socket machine. Our approach achieves geometric mean speedups of 1.1× and 4.7× respectively over default OpenMP configurations, compared to brute-force speedups of 1.27× and 4.93× respectively.},\n author = {Dutta, Akash and Alcaraz, Jordi and TehraniJamsaz, Ali and Sikora, Anna and Cesar, Eduardo and Jannesari, Ali},\n booktitle = {2022 IEEE/ACM International Workshop on Artificial Intelligence and Machine Learning for Scientific Applications (AI4S)},\n keywords = {Openmp,GNN},\n link = {https://ieeexplore.ieee.org/abstract/document/10027572/},\n organization = {IEEE},\n pages = {26--31},\n title = {Pattern-based autotuning of openmp loops using graph neural networks},\n year = {2022}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/10027572/"
  },
  {
    "id": "tehranijamsaz2023perfograph",
    "title": "Perfograph: A numerical aware program graph representation for performance optimization and program analysis",
    "authors": "TehraniJamsaz, Ali and Mahmud, Quazi Ishtiaque and Chen, Le and Ahmed, Nesreen K and Jannesari, Ali",
    "year": "2023",
    "source": "Advances in Neural Information Processing Systems",
    "category": "representation-learning",
    "keywords": [],
    "abstract": "The remarkable growth and significant success of machine learning have expanded its applications into programming languages and program analysis. However, a key challenge in adopting the latest machine learning methods is the representation of programming languages which has a direct impact on the ability of machine learning methods to reason about programs. The absence of numerical awareness, aggregate data structure information, and improper way of presenting variables in previous representation works have limited their performances. To overcome the limitations and challenges of current program representations, we propose a novel graph-based program representation called PERFOGRAPH. PERFOGRAPH can capture numerical information and the aggregate data structure by introducing new nodes and edges. Furthermore, we propose an adapted embedding method to incorporate numerical awareness.These enhancements make PERFOGRAPH a highly flexible and scalable representation that can effectively capture programs' intricate dependencies and semantics. Consequently, it serves as a powerful tool for various applications such as program analysis, performance optimization, and parallelism discovery. Our experimental results demonstrate that PERFOGRAPH outperforms existing representations and sets new state-of-the-art results by reducing the error rate by 7.4% (AMD dataset) and 10% (NVIDIA dataset) in the well-known Device Mapping challenge. It also sets new state-of-the-art results in various performance optimization tasks like Parallelism Discovery and Numa and Prefetchers Configuration prediction.",
    "bibtex": "@article{tehranijamsaz2023perfograph,\n abstract = {The remarkable growth and significant success of machine learning have expanded its applications into programming languages and program analysis. However, a key challenge in adopting the latest machine learning methods is the representation of programming languages which has a direct impact on the ability of machine learning methods to reason about programs. The absence of numerical awareness, aggregate data structure information, and improper way of presenting variables in previous representation works have limited their performances. To overcome the limitations and challenges of current program representations, we propose a novel graph-based program representation called PERFOGRAPH. PERFOGRAPH can capture numerical information and the aggregate data structure by introducing new nodes and edges. Furthermore, we propose an adapted embedding method to incorporate numerical awareness.These enhancements make PERFOGRAPH a highly flexible and scalable representation that can effectively capture programs' intricate dependencies and semantics. Consequently, it serves as a powerful tool for various applications such as program analysis, performance optimization, and parallelism discovery. Our experimental results demonstrate that PERFOGRAPH outperforms existing representations and sets new state-of-the-art results by reducing the error rate by 7.4% (AMD dataset) and 10% (NVIDIA dataset) in the well-known Device Mapping challenge. It also sets new state-of-the-art results in various performance optimization tasks like Parallelism Discovery and Numa and Prefetchers Configuration prediction.},\n author = {TehraniJamsaz, Ali and Mahmud, Quazi Ishtiaque and Chen, Le and Ahmed, Nesreen K and Jannesari, Ali},\n journal = {Advances in Neural Information Processing Systems},\n keywords = {},\n link = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/b41907dd4df5c60f86216b73fe0c7465-Abstract-Conference.html},\n pages = {57783--57794},\n title = {Perfograph: A numerical aware program graph representation for performance optimization and program analysis},\n volume = {36},\n year = {2023}\n}\n",
    "link": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/b41907dd4df5c60f86216b73fe0c7465-Abstract-Conference.html"
  },
  {
    "id": "mosaner2022machine",
    "title": "Machine-Learning-Based Self-Optimizing Compiler Heuristics✱",
    "authors": "Mosaner, Raphael and Leopoldseder, David and Kisling, Wolfgang and Stadler, Lukas and M{\\\"o}ssenb{\\\"o}ck, Hanspeter",
    "year": "2022",
    "source": "Proceedings of the 19th International Conference on Managed Programming Languages and Runtimes",
    "category": "jit",
    "keywords": [
      "GraalVM"
    ],
    "abstract": "Compiler optimizations are often based on hand-crafted heuristics to guide the optimization process. These heuristics are designed to benefit the average program and are otherwise static or only customized by profiling information. We propose machine-learning-based self-optimizing compiler heuristics, a novel approach for fitting optimization decisions in a dynamic compiler to specific environments. This is done by updating a machine learning model with extracted performance data at run time. Related work—which primarily targets static compilers—has already shown that machine learning can outperform hand-crafted heuristics. Our approach is specifically designed for dynamic compilation and uses concepts such as deoptimization for transparently switching between generating data and performing machine learning decisions in single program runs. We implemented our approach in the GraalVM, a high-performance production VM for dynamic compilation. When evaluating our approach by replacing loop peeling heuristics with learned models we encountered speedups larger than 30% for several benchmarks and only few slowdowns of up to 7%.",
    "bibtex": "@inproceedings{mosaner2022machine,\n abstract = {Compiler optimizations are often based on hand-crafted heuristics to guide the optimization process. These heuristics are designed to benefit the average program and are otherwise static or only customized by profiling information. We propose machine-learning-based self-optimizing compiler heuristics, a novel approach for fitting optimization decisions in a dynamic compiler to specific environments. This is done by updating a machine learning model with extracted performance data at run time. Related work—which primarily targets static compilers—has already shown that machine learning can outperform hand-crafted heuristics. Our approach is specifically designed for dynamic compilation and uses concepts such as deoptimization for transparently switching between generating data and performing machine learning decisions in single program runs. We implemented our approach in the GraalVM, a high-performance production VM for dynamic compilation. When evaluating our approach by replacing loop peeling heuristics with learned models we encountered speedups larger than 30% for several benchmarks and only few slowdowns of up to 7%.},\n author = {Mosaner, Raphael and Leopoldseder, David and Kisling, Wolfgang and Stadler, Lukas and M{\\\"o}ssenb{\\\"o}ck, Hanspeter},\n booktitle = {Proceedings of the 19th International Conference on Managed Programming Languages and Runtimes},\n keywords = {GraalVM},\n link = {https://dl.acm.org/doi/abs/10.1145/3546918.3546921},\n pages = {98--111},\n title = {Machine-Learning-Based Self-Optimizing Compiler Heuristics✱},\n year = {2022}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3546918.3546921"
  },
  {
    "id": "fang2024stacksight",
    "title": "StackSight: Unveiling webassembly through large language models and neurosymbolic chain-of-thought decompilation",
    "authors": "Fang, Weike and Zhou, Zhejian and He, Junzhou and Wang, Weihang",
    "year": "2024",
    "source": "arXiv preprint arXiv:2406.04568",
    "category": "binary-optimization",
    "keywords": [
      "webassembly",
      "CoT",
      "decompile"
    ],
    "abstract": "WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics.",
    "bibtex": "@article{fang2024stacksight,\n abstract = {WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics.},\n author = {Fang, Weike and Zhou, Zhejian and He, Junzhou and Wang, Weihang},\n journal = {arXiv preprint arXiv:2406.04568},\n keywords = {webassembly,CoT,decompile},\n link = {https://arxiv.org/abs/2406.04568},\n title = {StackSight: Unveiling webassembly through large language models and neurosymbolic chain-of-thought decompilation},\n year = {2024}\n}\n",
    "link": "https://arxiv.org/abs/2406.04568"
  },
  {
    "id": "cao2022boosting",
    "title": "Boosting neural networks to decompile optimized binaries",
    "authors": "Cao, Ying and Liang, Ruigang and Chen, Kai and Hu, Peiwei",
    "year": "2022",
    "source": "proceedings of the 38th annual computer security applications conference",
    "category": "binary-optimization",
    "keywords": [
      "GNN"
    ],
    "abstract": "Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21% higher accuracy than state-of-the-art neural decompilation frameworks.",
    "bibtex": "@inproceedings{cao2022boosting,\n abstract = {Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21% higher accuracy than state-of-the-art neural decompilation frameworks.},\n author = {Cao, Ying and Liang, Ruigang and Chen, Kai and Hu, Peiwei},\n booktitle = {proceedings of the 38th annual computer security applications conference},\n keywords = {GNN},\n link = {https://dl.acm.org/doi/10.1145/3564625.3567998},\n pages = {508--518},\n title = {Boosting neural networks to decompile optimized binaries},\n year = {2022}\n}\n",
    "link": "https://dl.acm.org/doi/10.1145/3564625.3567998"
  },
  {
    "id": "she2024wadec",
    "title": "WaDec: Decompiling WebAssembly Using Large Language Model",
    "authors": "She, Xinyu and Zhao, Yanjie and Wang, Haoyu",
    "year": "2024",
    "source": "Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering",
    "category": "binary-optimization",
    "keywords": [
      "WebAssembly",
      "LLM"
    ],
    "abstract": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm. In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and an output consistency of 27.15%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185%, cyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average code similarity above 50%. In summary, WaDec enhances understanding of the code's structure and execution flow, facilitating automated code analysis, optimization, and security auditing.",
    "bibtex": "@inproceedings{she2024wadec,\n abstract = {WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm. In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and an output consistency of 27.15%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185%, cyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average code similarity above 50%. In summary, WaDec enhances understanding of the code's structure and execution flow, facilitating automated code analysis, optimization, and security auditing.},\n author = {She, Xinyu and Zhao, Yanjie and Wang, Haoyu},\n booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},\n keywords = {WebAssembly,LLM},\n link = {https://dl.acm.org/doi/10.1145/3691620.3695020},\n pages = {481--492},\n title = {WaDec: Decompiling WebAssembly Using Large Language Model},\n year = {2024}\n}\n",
    "link": "https://dl.acm.org/doi/10.1145/3691620.3695020"
  },
  {
    "id": "huang2024multi",
    "title": "Multi-modal Learning for WebAssembly Reverse Engineering",
    "authors": "Huang, Hanxian and Zhao, Jishen",
    "year": "2024",
    "source": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
    "category": "binary-optimization",
    "keywords": [
      "WebAssembly"
    ],
    "abstract": "The increasing adoption of WebAssembly (Wasm) for performance-critical and security-sensitive tasks drives the demand for WebAssembly program comprehension and reverse engineering. Recent studies have introduced machine learning (ML)-based WebAssembly reverse engineering tools. Yet, the generalization of task-specific ML solutions remains challenging, because their effectiveness hinges on the availability of an ample supply of high-quality task-specific labeled data. Moreover, previous works trained models only with features extracted from WebAssembly, overlooking the high-level semantics present in the corresponding source code and its documentation. Acknowledging the abundance of available source code with documentation, which can be compiled into WebAssembly, we propose to learn representations of them concurrently and harness their mutual relationships for effective WebAssembly reverse engineering. In this paper, we present WasmRev, the first multi-modal pre-trained language model for WebAssembly reverse engineering. WasmRev is pre-trained using self-supervised learning on a large-scale multi-modal corpus encompassing source code, code documentation and the compiled WebAssembly, without requiring labeled data. WasmRev incorporates three tailored multi-modal pre-training tasks to capture various characteristics of WebAssembly and cross-modal relationships. WasmRev is only trained once to produce general-purpose representations that can broadly support WebAssembly reverse engineering tasks through few-shot fine-tuning with much less labeled data, improving data efficiency. We fine-tune WasmRev onto three important reverse engineering tasks: type recovery, function purpose identification and WebAssembly summarization. Our results show that WasmRev pre-trained on the corpus of multi-modal samples establishes a robust foundation for these tasks, achieving high task accuracy and outperforming the state-of-the-art ML methods for WebAssembly reverse engineering.",
    "bibtex": "@inproceedings{huang2024multi,\n abstract = {The increasing adoption of WebAssembly (Wasm) for performance-critical and security-sensitive tasks drives the demand for WebAssembly program comprehension and reverse engineering. Recent studies have introduced machine learning (ML)-based WebAssembly reverse engineering tools. Yet, the generalization of task-specific ML solutions remains challenging, because their effectiveness hinges on the availability of an ample supply of high-quality task-specific labeled data. Moreover, previous works trained models only with features extracted from WebAssembly, overlooking the high-level semantics present in the corresponding source code and its documentation. Acknowledging the abundance of available source code with documentation, which can be compiled into WebAssembly, we propose to learn representations of them concurrently and harness their mutual relationships for effective WebAssembly reverse engineering. In this paper, we present WasmRev, the first multi-modal pre-trained language model for WebAssembly reverse engineering. WasmRev is pre-trained using self-supervised learning on a large-scale multi-modal corpus encompassing source code, code documentation and the compiled WebAssembly, without requiring labeled data. WasmRev incorporates three tailored multi-modal pre-training tasks to capture various characteristics of WebAssembly and cross-modal relationships. WasmRev is only trained once to produce general-purpose representations that can broadly support WebAssembly reverse engineering tasks through few-shot fine-tuning with much less labeled data, improving data efficiency. We fine-tune WasmRev onto three important reverse engineering tasks: type recovery, function purpose identification and WebAssembly summarization. Our results show that WasmRev pre-trained on the corpus of multi-modal samples establishes a robust foundation for these tasks, achieving high task accuracy and outperforming the state-of-the-art ML methods for WebAssembly reverse engineering.},\n author = {Huang, Hanxian and Zhao, Jishen},\n booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},\n keywords = {WebAssembly},\n link = {https://dl.acm.org/doi/10.1145/3650212.3652141},\n pages = {453--465},\n title = {Multi-modal Learning for WebAssembly Reverse Engineering},\n year = {2024}\n}\n",
    "link": "https://dl.acm.org/doi/10.1145/3650212.3652141"
  },
  {
    "id": "wong2025decllm",
    "title": "DecLLM: LLM-Augmented Recompilable Decompilation for Enabling Programmatic Use of Decompiled Code",
    "authors": "Wong, Wai Kin and Wu, Daoyuan and Wang, Huaijin and Li, Zongjie and Liu, Zhibo and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi",
    "year": "2025",
    "source": "Proceedings of the ACM on Software Engineering",
    "category": "binary-optimization",
    "keywords": [
      "LLM"
    ],
    "abstract": "Decompilers are widely used in reverse engineering (RE) to convert compiled executables into human-readable pseudocode and support various security analysis tasks. Existing decompilers, such as IDA Pro and Ghidra, focus on enhancing the readability of decompiled code rather than its recompilability, which limits further programmatic use, such as for CodeQL-based vulnerability analysis that requires compilable versions of the decompiled code. Recent LLM-based approaches for enhancing decompilation results, while useful for human RE analysts, unfortunately also follow the same path. In this paper, we explore, for the first time, how off-the-shelf large language models (LLMs) can be used to enable recompilable decompilation—automatically correcting decompiler outputs into compilable versions. We first show that this is non-trivial through a pilot study examining existing rule-based and LLM-based approaches. Based on the lessons learned, we design DecLLM, an iterative LLM-based repair loop that utilizes both static recompilation and dynamic runtime feedback as oracles to iteratively fix decompiler outputs. We test DecLLM on popular C benchmarks and real-world binaries using two mainstream LLMs, GPT-3.5 and GPT-4, and show that off-the-shelf LLMs can achieve an upper bound of around 70% recompilation success rate, i.e., 70 out of 100 originally non-recompilable decompiler outputs are now recompilable. We also demonstrate the practical applicability of the recompilable code for CodeQL-based vulnerability analysis, which is impossible to perform directly on binaries. For the remaining 30% of hard cases, we further delve into their errors to gain insights for future improvements in decompilation-oriented LLM design.",
    "bibtex": "@article{wong2025decllm,\n abstract = {Decompilers are widely used in reverse engineering (RE) to convert compiled executables into human-readable pseudocode and support various security analysis tasks. Existing decompilers, such as IDA Pro and Ghidra, focus on enhancing the readability of decompiled code rather than its recompilability, which limits further programmatic use, such as for CodeQL-based vulnerability analysis that requires compilable versions of the decompiled code. Recent LLM-based approaches for enhancing decompilation results, while useful for human RE analysts, unfortunately also follow the same path. In this paper, we explore, for the first time, how off-the-shelf large language models (LLMs) can be used to enable recompilable decompilation—automatically correcting decompiler outputs into compilable versions. We first show that this is non-trivial through a pilot study examining existing rule-based and LLM-based approaches. Based on the lessons learned, we design DecLLM, an iterative LLM-based repair loop that utilizes both static recompilation and dynamic runtime feedback as oracles to iteratively fix decompiler outputs. We test DecLLM on popular C benchmarks and real-world binaries using two mainstream LLMs, GPT-3.5 and GPT-4, and show that off-the-shelf LLMs can achieve an upper bound of around 70% recompilation success rate, i.e., 70 out of 100 originally non-recompilable decompiler outputs are now recompilable. We also demonstrate the practical applicability of the recompilable code for CodeQL-based vulnerability analysis, which is impossible to perform directly on binaries. For the remaining 30% of hard cases, we further delve into their errors to gain insights for future improvements in decompilation-oriented LLM design.},\n author = {Wong, Wai Kin and Wu, Daoyuan and Wang, Huaijin and Li, Zongjie and Liu, Zhibo and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi},\n journal = {Proceedings of the ACM on Software Engineering},\n keywords = {LLM},\n link = {https://dl.acm.org/doi/10.1145/3728958},\n number = {ISSTA},\n pages = {1841--1864},\n publisher = {ACM New York, NY, USA},\n title = {DecLLM: LLM-Augmented Recompilable Decompilation for Enabling Programmatic Use of Decompiled Code},\n volume = {2},\n year = {2025}\n}\n",
    "link": "https://dl.acm.org/doi/10.1145/3728958"
  },
  {
    "id": "cao2024evaluating",
    "title": "Evaluating the Effectiveness of Decompilers",
    "authors": "Cao, Ying and Zhang, Runze and Liang, Ruigang and Chen, Kai",
    "year": "2024",
    "source": "Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis",
    "category": "binary-optimization",
    "keywords": [
      "decompiler"
    ],
    "abstract": "In software security tasks like malware analysis and vulnerability mining, reverse engineering is pivotal, with C decompilers playing a crucial role in understanding program semantics. However, reverse engineers still predominantly rely on assembly code rather than decompiled code when analyzing complex binaries. This practice underlines the limitations of current decompiled code, which hinders its effectiveness in reverse engineering. Identifying and analyzing the problems of existing decompilers and making targeted improvements can effectively enhance the efficiency of software analysis. In this study, we systematically evaluate current mainstream decompilers’ semantic consistency and readability. Semantic evaluation results show that the state-of-the-art decompiler Hex-Rays has about 55% accuracy at almost all optimization, which contradicts the common belief among many reverse engineers that decompilers are usually accurate. Readability evaluation indicates that despite years of efforts to improve the readability of the decompiled code, decompilers’ template-based approach still predominantly yields code akin to binary structures rather than human coding patterns. Additionally, our human study indicates that to enhance decompilers’ accuracy and readability, introducing human or compiler-aware strategies like a speculate-verify-correct approach to obtain recompilable decompiled code and iteratively refine it to more closely resemble the original binary, potentially offers a more effective optimization method than relying on static analysis and rule expansion.",
    "bibtex": "@inproceedings{cao2024evaluating,\n abstract = {In software security tasks like malware analysis and vulnerability mining, reverse engineering is pivotal, with C decompilers playing a crucial role in understanding program semantics. However, reverse engineers still predominantly rely on assembly code rather than decompiled code when analyzing complex binaries. This practice underlines the limitations of current decompiled code, which hinders its effectiveness in reverse engineering. Identifying and analyzing the problems of existing decompilers and making targeted improvements can effectively enhance the efficiency of software analysis. In this study, we systematically evaluate current mainstream decompilers’ semantic consistency and readability. Semantic evaluation results show that the state-of-the-art decompiler Hex-Rays has about 55% accuracy at almost all optimization, which contradicts the common belief among many reverse engineers that decompilers are usually accurate. Readability evaluation indicates that despite years of efforts to improve the readability of the decompiled code, decompilers’ template-based approach still predominantly yields code akin to binary structures rather than human coding patterns. Additionally, our human study indicates that to enhance decompilers’ accuracy and readability, introducing human or compiler-aware strategies like a speculate-verify-correct approach to obtain recompilable decompiled code and iteratively refine it to more closely resemble the original binary, potentially offers a more effective optimization method than relying on static analysis and rule expansion.},\n author = {Cao, Ying and Zhang, Runze and Liang, Ruigang and Chen, Kai},\n booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},\n keywords = {decompiler},\n link = {https://dl.acm.org/doi/abs/10.1145/3650212.3652144},\n pages = {491--502},\n title = {Evaluating the Effectiveness of Decompilers},\n year = {2024}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3650212.3652144"
  },
  {
    "id": "hu2024degpt",
    "title": "Degpt: Optimizing decompiler output with llm",
    "authors": "Hu, Peiwei and Liang, Ruigang and Chen, Kai",
    "year": "2024",
    "source": "Proceedings 2024 Network and Distributed System Security Symposium",
    "category": "binary-optimization",
    "keywords": [
      "LLM"
    ],
    "abstract": "Decompilation plays a vital role in reverse engineering tasks such as malware analysis and vulnerability discovery by transforming low-level assembly into high-level source code to enhance binary comprehension. However, existing decompilers often produce outputs that suffer from issues like meaningless variable names, redundant variables, and a lack of explanatory comments, limiting their usefulness. Prior work has attempted to address this by training models on large datasets of decompiler outputs, but these datasets often fail to generalize to unseen binaries, leading to degraded performance during binary migration.To overcome these limitations, this paper introduces DeGPT, an end-to-end framework designed to refine and optimize decompiler outputs for improved readability and simplicity. Leveraging the powerful generalization capabilities of large language models (LLMs), DeGPT mitigates performance drop-off through a novel three-role mechanism: a referee (R_ref) that proposes optimization strategies, an advisor (R_adv) that applies corrective transformations, and an operator (R_ope) that ensures the semantics of the original binary remain unchanged. Evaluations across diverse datasets—including command-line tools, malware, audio libraries, and algorithm implementations—demonstrate that DeGPT significantly enhances the quality of decompiler outputs. Specifically, it reduces cognitive load by 24.4% and generates meaningful comments in 62.9% of cases. User studies further confirm that DeGPT effectively simplifies code and enriches it with semantic information, such as accurate variable names and contextual comments, thereby greatly assisting reverse engineers in understanding complex binaries.",
    "bibtex": "@inproceedings{hu2024degpt,\n abstract = {Decompilation plays a vital role in reverse engineering tasks such as malware analysis and vulnerability discovery by transforming low-level assembly into high-level source code to enhance binary comprehension. However, existing decompilers often produce outputs that suffer from issues like meaningless variable names, redundant variables, and a lack of explanatory comments, limiting their usefulness. Prior work has attempted to address this by training models on large datasets of decompiler outputs, but these datasets often fail to generalize to unseen binaries, leading to degraded performance during binary migration.To overcome these limitations, this paper introduces DeGPT, an end-to-end framework designed to refine and optimize decompiler outputs for improved readability and simplicity. Leveraging the powerful generalization capabilities of large language models (LLMs), DeGPT mitigates performance drop-off through a novel three-role mechanism: a referee (R_ref) that proposes optimization strategies, an advisor (R_adv) that applies corrective transformations, and an operator (R_ope) that ensures the semantics of the original binary remain unchanged. Evaluations across diverse datasets—including command-line tools, malware, audio libraries, and algorithm implementations—demonstrate that DeGPT significantly enhances the quality of decompiler outputs. Specifically, it reduces cognitive load by 24.4% and generates meaningful comments in 62.9% of cases. User studies further confirm that DeGPT effectively simplifies code and enriches it with semantic information, such as accurate variable names and contextual comments, thereby greatly assisting reverse engineers in understanding complex binaries.},\n author = {Hu, Peiwei and Liang, Ruigang and Chen, Kai},\n booktitle = {Proceedings 2024 Network and Distributed System Security Symposium},\n keywords = {LLM},\n link = {https://www.ndss-symposium.org/wp-content/uploads/2024-401-paper.pdf},\n title = {Degpt: Optimizing decompiler output with llm},\n volume = {267622140},\n year = {2024}\n}\n",
    "link": "https://www.ndss-symposium.org/wp-content/uploads/2024-401-paper.pdf"
  },
  {
    "id": "tan2024llm4decompile",
    "title": "LLM4Decompile: Decompiling Binary Code with Large Language Models",
    "authors": "Tan, Hanzhuo and Luo, Qi and Li, Jing and Zhang, Yuqun",
    "year": "2024",
    "source": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    "category": "binary-optimization",
    "keywords": [
      "LLM"
    ],
    "abstract": "Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100% in terms of re-executability rate. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results.",
    "bibtex": "@inproceedings{tan2024llm4decompile,\n abstract = {Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100% in terms of re-executability rate. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results.},\n author = {Tan, Hanzhuo and Luo, Qi and Li, Jing and Zhang, Yuqun},\n booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},\n keywords = {LLM},\n link = {https://aclanthology.org/2024.emnlp-main.203/},\n pages = {3473--3487},\n title = {LLM4Decompile: Decompiling Binary Code with Large Language Models},\n year = {2024}\n}\n",
    "link": "https://aclanthology.org/2024.emnlp-main.203/"
  },
  {
    "id": "armengol2024slade",
    "title": "Slade: A portable small language model decompiler for optimized assembly",
    "authors": "Armengol-Estap{\\'e}, Jordi and Woodruff, Jackson and Cummins, Chris and O'Boyle, Michael FP",
    "year": "2024",
    "source": "2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)",
    "category": "binary-optimization",
    "keywords": [],
    "abstract": "Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. Nevertheless, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence Transformer trained over real-world code and augmented with a type inference engine. We utilize a novel tokenizer, dropout-free regularization, and type inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code. We evaluate SLaDe on over 4,000 ExeBench functions on two ISAs and at two optimization levels. SLaDe is up to 6× more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4× more accurate than the large language model ChatGPT and generates significantly more readable code than both.",
    "bibtex": "@inproceedings{armengol2024slade,\n abstract = {Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. Nevertheless, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence Transformer trained over real-world code and augmented with a type inference engine. We utilize a novel tokenizer, dropout-free regularization, and type inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code. We evaluate SLaDe on over 4,000 ExeBench functions on two ISAs and at two optimization levels. SLaDe is up to 6× more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4× more accurate than the large language model ChatGPT and generates significantly more readable code than both.},\n author = {Armengol-Estap{\\'e}, Jordi and Woodruff, Jackson and Cummins, Chris and O'Boyle, Michael FP},\n booktitle = {2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},\n keywords = {},\n link = {https://ieeexplore.ieee.org/abstract/document/10444788},\n organization = {IEEE},\n pages = {67--80},\n title = {Slade: A portable small language model decompiler for optimized assembly},\n year = {2024}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/10444788"
  },
  {
    "id": "lacomis2019dire",
    "title": "Dire: A neural approach to decompiled identifier naming",
    "authors": "Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan",
    "year": "2019",
    "source": "2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)",
    "category": "binary-optimization",
    "keywords": [],
    "abstract": "The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.",
    "bibtex": "@inproceedings{lacomis2019dire,\n abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.},\n author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan},\n booktitle = {2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)},\n keywords = {},\n link = {https://ieeexplore.ieee.org/abstract/document/8952404},\n organization = {IEEE},\n pages = {628--639},\n title = {Dire: A neural approach to decompiled identifier naming},\n year = {2019}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/8952404"
  },
  {
    "id": "manuel2024enhancing",
    "title": "Enhancing Reverse Engineering: Investigating and Benchmarking Large Language Models for Vulnerability Analysis in Decompiled Binaries",
    "authors": "Manuel, Dylan and Islam, Nafis Tanveer and Khoury, Joseph and Nunez, Ana and Bou-Harb, Elias and Najafirad, Peyman",
    "year": "2024",
    "source": "arXiv preprint arXiv:2411.04981",
    "category": "binary-optimization",
    "keywords": [
      "LLM"
    ],
    "abstract": "Security experts reverse engineer (decompile) binary code to identify critical security vulnerabilities. The limited access to source code in vital systems - such as firmware, drivers, and proprietary software used in Critical Infrastructures (CI) - makes this analysis even more crucial on the binary level. Even with available source code, a semantic gap persists after compilation between the source and the binary code executed by the processor. This gap may hinder the detection of vulnerabilities in source code. That being said, current research on Large Language Models (LLMs) overlooks the significance of decompiled binaries in this area by focusing solely on source code. In this work, we are the first to empirically uncover the substantial semantic limitations of state-of-the-art LLMs when it comes to analyzing vulnerabilities in decompiled binaries, largely due to the absence of relevant datasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binary code vulnerability dataset. Our dataset is multi-architecture and multi-optimization, focusing on C/C++ due to their wide usage in CI and association with numerous vulnerabilities. Specifically, we curate 150,872 samples of vulnerable and non-vulnerable decompiled binary code for the task of (i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv) recovering function names in the domain of decompiled binaries. Subsequently, we fine-tune state-of-the-art LLMs using DeBinVul and report on a performance increase of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama3, and CodeGen2 respectively, in detecting binary code vulnerabilities. Additionally, using DeBinVul, we report a high performance of 80-90% on the vulnerability classification task. Furthermore, we report improved performance in function name recovery and vulnerability description tasks.",
    "bibtex": "@article{manuel2024enhancing,\n abstract = {Security experts reverse engineer (decompile) binary code to identify critical security vulnerabilities. The limited access to source code in vital systems - such as firmware, drivers, and proprietary software used in Critical Infrastructures (CI) - makes this analysis even more crucial on the binary level. Even with available source code, a semantic gap persists after compilation between the source and the binary code executed by the processor. This gap may hinder the detection of vulnerabilities in source code. That being said, current research on Large Language Models (LLMs) overlooks the significance of decompiled binaries in this area by focusing solely on source code. In this work, we are the first to empirically uncover the substantial semantic limitations of state-of-the-art LLMs when it comes to analyzing vulnerabilities in decompiled binaries, largely due to the absence of relevant datasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binary code vulnerability dataset. Our dataset is multi-architecture and multi-optimization, focusing on C/C++ due to their wide usage in CI and association with numerous vulnerabilities. Specifically, we curate 150,872 samples of vulnerable and non-vulnerable decompiled binary code for the task of (i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv) recovering function names in the domain of decompiled binaries. Subsequently, we fine-tune state-of-the-art LLMs using DeBinVul and report on a performance increase of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama3, and CodeGen2 respectively, in detecting binary code vulnerabilities. Additionally, using DeBinVul, we report a high performance of 80-90% on the vulnerability classification task. Furthermore, we report improved performance in function name recovery and vulnerability description tasks.},\n author = {Manuel, Dylan and Islam, Nafis Tanveer and Khoury, Joseph and Nunez, Ana and Bou-Harb, Elias and Najafirad, Peyman},\n journal = {arXiv preprint arXiv:2411.04981},\n keywords = {LLM},\n link = {https://arxiv.org/abs/2411.04981},\n title = {Enhancing Reverse Engineering: Investigating and Benchmarking Large Language Models for Vulnerability Analysis in Decompiled Binaries},\n year = {2024}\n}\n",
    "link": "https://arxiv.org/abs/2411.04981"
  },
  {
    "id": "al2023extending",
    "title": "Extending source code pre-trained language models to summarise decompiled binaries",
    "authors": "Al-Kaswan, Ali and Ahmed, Toufique and Izadi, Maliheh and Sawant, Anand Ashok and Devanbu, Premkumar and van Deursen, Arie",
    "year": "2023",
    "source": "2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",
    "category": "binary-optimization",
    "keywords": [
      "LLM"
    ],
    "abstract": "Binary reverse engineering is used to understand and analyse programs for which the source code is unavailable. Decompilers can help, transforming opaque binaries into a more readable source code-like representation. Still, reverse engineering is difficult and costly, involving considering effort in labelling code with helpful summaries. While the automated summarisation of decompiled code can help reverse engineers understand and analyse binaries, current work mainly focuses on summarising source code, and no suitable dataset exists for this task. In this work, we extend large pre-trained language models of source code to summarise de-compiled binary functions. Further-more, we investigate the impact of input and data properties on the performance of such models. Our approach consists of two main components; the data and the model. We first build CAPYBARA, a dataset of 214K decompiled function-documentation pairs across various compiler optimisations. We extend CAPYBARA further by removing identifiers, and deduplicating the data. Next, we fine-tune the CodeT5 base model with CAPYBARA to create BinT5. BinT5 achieves the state-of-the-art BLEU-4 score of 60.83, 58.82 and, 44.21 for summarising source, decompiled, and obfuscated decompiled code, respectively. This indicates that these models can be extended to decompiled binaries successfully. Finally, we found that the performance of BinT5 is not heavily dependent on the dataset size and compiler optimisation level. We recommend future research to further investigate transferring knowledge when working with less expressive input formats such as stripped binaries.",
    "bibtex": "@inproceedings{al2023extending,\n abstract = {Binary reverse engineering is used to understand and analyse programs for which the source code is unavailable. Decompilers can help, transforming opaque binaries into a more readable source code-like representation. Still, reverse engineering is difficult and costly, involving considering effort in labelling code with helpful summaries. While the automated summarisation of decompiled code can help reverse engineers understand and analyse binaries, current work mainly focuses on summarising source code, and no suitable dataset exists for this task. In this work, we extend large pre-trained language models of source code to summarise de-compiled binary functions. Further-more, we investigate the impact of input and data properties on the performance of such models. Our approach consists of two main components; the data and the model. We first build CAPYBARA, a dataset of 214K decompiled function-documentation pairs across various compiler optimisations. We extend CAPYBARA further by removing identifiers, and deduplicating the data. Next, we fine-tune the CodeT5 base model with CAPYBARA to create BinT5. BinT5 achieves the state-of-the-art BLEU-4 score of 60.83, 58.82 and, 44.21 for summarising source, decompiled, and obfuscated decompiled code, respectively. This indicates that these models can be extended to decompiled binaries successfully. Finally, we found that the performance of BinT5 is not heavily dependent on the dataset size and compiler optimisation level. We recommend future research to further investigate transferring knowledge when working with less expressive input formats such as stripped binaries.},\n author = {Al-Kaswan, Ali and Ahmed, Toufique and Izadi, Maliheh and Sawant, Anand Ashok and Devanbu, Premkumar and van Deursen, Arie},\n booktitle = {2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)},\n keywords = {LLM},\n link = {https://ieeexplore.ieee.org/abstract/document/10123452/},\n organization = {IEEE},\n pages = {260--271},\n title = {Extending source code pre-trained language models to summarise decompiled binaries},\n year = {2023}\n}\n",
    "link": "https://ieeexplore.ieee.org/abstract/document/10123452/"
  },
  {
    "id": "mcdanel2023chatgpt",
    "title": "ChatGPT as a Java Decompiler",
    "authors": "McDanel, Bradley and Liu, Zhanhao",
    "year": "2023",
    "source": "Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
    "category": "binary-optimization",
    "keywords": [
      "LLM",
      "Java"
    ],
    "abstract": "We propose a novel approach using instruction-tuned large language models (LLMs), such as ChatGPT, to automatically decompile entire Java classes. Our method relies only on a textual representation of the Java bytecode and corresponding unit tests generated from the bytecode. While no additional domain knowledge or fine-tuning is performed, we provide a single training example of this decompilation process in the model’s prompt. To overcome both compilation errors and test failures, we use an iterative prompting approach. We find that ChatGPT-4 is able to generate more human-readable output than existing software-based decompilers while achieving slightly lower pass rates on unit tests. Source code and datasets are available at https://github.com/BradMcDanel/gpt-java-decompiler.",
    "bibtex": "@inproceedings{mcdanel2023chatgpt,\n abstract = {We propose a novel approach using instruction-tuned large language models (LLMs), such as ChatGPT, to automatically decompile entire Java classes. Our method relies only on a textual representation of the Java bytecode and corresponding unit tests generated from the bytecode. While no additional domain knowledge or fine-tuning is performed, we provide a single training example of this decompilation process in the model’s prompt. To overcome both compilation errors and test failures, we use an iterative prompting approach. We find that ChatGPT-4 is able to generate more human-readable output than existing software-based decompilers while achieving slightly lower pass rates on unit tests. Source code and datasets are available at https://github.com/BradMcDanel/gpt-java-decompiler.},\n author = {McDanel, Bradley and Liu, Zhanhao},\n booktitle = {Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)},\n keywords = {LLM,Java},\n link = {https://aclanthology.org/2023.gem-1.19/},\n pages = {224--232},\n title = {ChatGPT as a Java Decompiler},\n year = {2023}\n}\n",
    "link": "https://aclanthology.org/2023.gem-1.19/"
  },
  {
    "id": "taneja2025llm",
    "title": "Llm-vectorizer: Llm-based verified loop vectorizer",
    "authors": "Taneja, Jubi and Laird, Avery and Yan, Cong and Musuvathi, Madan and Lahiri, Shuvendu K",
    "year": "2025",
    "source": "Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization",
    "category": "code-repair",
    "keywords": [
      "vec",
      "LLM",
      "TSVC",
      "intrinsic"
    ],
    "abstract": "Vectorization is a powerful optimization technique that significantly boosts the performance of high performance computing applications operating on large data arrays. Despite decades of research on auto-vectorization, compilers frequently miss opportunities to vectorize code. On the other hand, writing vectorized code manually using compiler intrinsics is still a complex, error-prone task that demands deep knowledge of specific architecture and compilers. In this paper, we evaluate the potential of large-language models (LLMs) to generate vectorized (Single Instruction Multiple Data) code from scalar programs that process individual array elements. We propose a novel finite-state-machine multi-agents based approach that harnesses LLMs and test-based feedback to generate vectorized code. Our findings indicate that LLMs are capable of producing high-performance vectorized code with run-time speedup ranging from 1.1x to 9.4x as compared to the state-of-the-art compilers such as Intel Compiler, GCC, and Clang. To verify the correctness of vectorized code, we use Alive2, a leading bounded translation validation tool for LLVM IR. We describe a few domain-specific techniques to improve the scalability of Alive2 on our benchmark dataset. Overall, our approach is able to verify 38.2% of vectorizations as correct on the TSVC benchmark dataset.",
    "bibtex": "@inproceedings{taneja2025llm,\n abstract = {Vectorization is a powerful optimization technique that significantly boosts the performance of high performance computing applications operating on large data arrays. Despite decades of research on auto-vectorization, compilers frequently miss opportunities to vectorize code. On the other hand, writing vectorized code manually using compiler intrinsics is still a complex, error-prone task that demands deep knowledge of specific architecture and compilers. In this paper, we evaluate the potential of large-language models (LLMs) to generate vectorized (Single Instruction Multiple Data) code from scalar programs that process individual array elements. We propose a novel finite-state-machine multi-agents based approach that harnesses LLMs and test-based feedback to generate vectorized code. Our findings indicate that LLMs are capable of producing high-performance vectorized code with run-time speedup ranging from 1.1x to 9.4x as compared to the state-of-the-art compilers such as Intel Compiler, GCC, and Clang. To verify the correctness of vectorized code, we use Alive2, a leading bounded translation validation tool for LLVM IR. We describe a few domain-specific techniques to improve the scalability of Alive2 on our benchmark dataset. Overall, our approach is able to verify 38.2% of vectorizations as correct on the TSVC benchmark dataset.},\n author = {Taneja, Jubi and Laird, Avery and Yan, Cong and Musuvathi, Madan and Lahiri, Shuvendu K},\n booktitle = {Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},\n keywords = {vec,LLM,TSVC,intrinsic},\n link = {https://dl.acm.org/doi/abs/10.1145/3696443.3708929},\n pages = {137--149},\n title = {Llm-vectorizer: Llm-based verified loop vectorizer},\n year = {2025}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3696443.3708929"
  },
  {
    "id": "ashouri2016cobayn",
    "title": "Cobayn: Compiler autotuning framework using bayesian networks",
    "authors": "Ashouri, Amir Hossein and Mariani, Giovanni and Palermo, Gianluca and Park, Eunjung and Cavazos, John and Silvano, Cristina",
    "year": "2016",
    "source": "ACM Transactions on Architecture and Code Optimization (TACO)",
    "category": "embedded&mobile",
    "keywords": [
      "Bayesian",
      "gcc",
      "embedded"
    ],
    "abstract": "The variety of today’s architectures forces programmers to spend a great deal of time porting and tuning application codes across different platforms. Compilers themselves need additional tuning, which has considerable complexity as the standard optimization levels, usually designed for the average case and the specific target architecture, often fail to bring the best results.This article proposes COBAYN: Compiler autotuning framework using BAYesian Networks, an approach for a compiler autotuning methodology using machine learning to speed up application performance and to reduce the cost of the compiler optimization phases. The proposed framework is based on the application characterization done dynamically by using independent microarchitecture features and Bayesian networks. The article also presents an evaluation based on using static analysis and hybrid feature collection approaches. In addition, the article compares Bayesian networks with respect to several state-of-the-art machine-learning models.Experiments were carried out on an ARM embedded platform and GCC compiler by considering two benchmark suites with 39 applications. The set of compiler configurations, selected by the model (less than 7% of the search space), demonstrated an application performance speedup of up to 4.6 × on Polybench (1.85 × on average) and 3.1 × on cBench (1.54 × on average) with respect to standard optimization levels. Moreover, the comparison of the proposed technique with (i) random iterative compilation, (ii) machine learning--based iterative compilation, and (iii) noniterative predictive modeling techniques shows, on average, 1.2 ×, 1.37 ×, and 1.48 × speedup, respectively. Finally, the proposed method demonstrates 4 × and 3 × speedup, respectively, on cBench and Polybench in terms of exploration efficiency given the same quality of the solutions generated by the random iterative compilation model.",
    "bibtex": "@article{ashouri2016cobayn,\n abstract = {The variety of today’s architectures forces programmers to spend a great deal of time porting and tuning application codes across different platforms. Compilers themselves need additional tuning, which has considerable complexity as the standard optimization levels, usually designed for the average case and the specific target architecture, often fail to bring the best results.This article proposes COBAYN: Compiler autotuning framework using BAYesian Networks, an approach for a compiler autotuning methodology using machine learning to speed up application performance and to reduce the cost of the compiler optimization phases. The proposed framework is based on the application characterization done dynamically by using independent microarchitecture features and Bayesian networks. The article also presents an evaluation based on using static analysis and hybrid feature collection approaches. In addition, the article compares Bayesian networks with respect to several state-of-the-art machine-learning models.Experiments were carried out on an ARM embedded platform and GCC compiler by considering two benchmark suites with 39 applications. The set of compiler configurations, selected by the model (less than 7% of the search space), demonstrated an application performance speedup of up to 4.6 × on Polybench (1.85 × on average) and 3.1 × on cBench (1.54 × on average) with respect to standard optimization levels. Moreover, the comparison of the proposed technique with (i) random iterative compilation, (ii) machine learning--based iterative compilation, and (iii) noniterative predictive modeling techniques shows, on average, 1.2 ×, 1.37 ×, and 1.48 × speedup, respectively. Finally, the proposed method demonstrates 4 × and 3 × speedup, respectively, on cBench and Polybench in terms of exploration efficiency given the same quality of the solutions generated by the random iterative compilation model.},\n author = {Ashouri, Amir Hossein and Mariani, Giovanni and Palermo, Gianluca and Park, Eunjung and Cavazos, John and Silvano, Cristina},\n journal = {ACM Transactions on Architecture and Code Optimization (TACO)},\n keywords = {Bayesian,gcc,embedded},\n link = {https://dl.acm.org/doi/abs/10.1145/2928270},\n number = {2},\n pages = {1--25},\n publisher = {ACM New York, NY, USA},\n title = {Cobayn: Compiler autotuning framework using bayesian networks},\n volume = {13},\n year = {2016}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/2928270"
  },
  {
    "id": "grubisic2024compiler",
    "title": "Compiler generated feedback for large language models",
    "authors": "Grubisic, Dejan and Cummins, Chris and Seeker, Volker and Leather, Hugh",
    "year": "2024",
    "source": "arXiv preprint arXiv:2403.14714",
    "category": "embedded&mobile",
    "keywords": [
      "codesize",
      "llm"
    ],
    "abstract": "We introduce a novel paradigm in compiler optimization powered by Large Language Models with compiler feedback to optimize the code size of LLVM assembly. The model takes unoptimized LLVM IR as input and produces optimized IR, the best optimization passes, and instruction counts of both unoptimized and optimized IRs. Then we compile the input with generated optimization passes and evaluate if the predicted instruction count is correct, generated IR is compilable, and corresponds to compiled code. We provide this feedback back to LLM and give it another chance to optimize code. This approach adds an extra 0.53% improvement over -Oz to the original model. Even though, adding more information with feedback seems intuitive, simple sampling techniques achieve much higher performance given 10 or more samples.",
    "bibtex": "@article{grubisic2024compiler,\n abstract = {We introduce a novel paradigm in compiler optimization powered by Large Language Models with compiler feedback to optimize the code size of LLVM assembly. The model takes unoptimized LLVM IR as input and produces optimized IR, the best optimization passes, and instruction counts of both unoptimized and optimized IRs. Then we compile the input with generated optimization passes and evaluate if the predicted instruction count is correct, generated IR is compilable, and corresponds to compiled code. We provide this feedback back to LLM and give it another chance to optimize code. This approach adds an extra 0.53% improvement over -Oz to the original model. Even though, adding more information with feedback seems intuitive, simple sampling techniques achieve much higher performance given 10 or more samples.},\n author = {Grubisic, Dejan and Cummins, Chris and Seeker, Volker and Leather, Hugh},\n journal = {arXiv preprint arXiv:2403.14714},\n keywords = {codesize,llm},\n link = {https://arxiv.org/abs/2403.14714},\n title = {Compiler generated feedback for large language models},\n year = {2024}\n}\n",
    "link": "https://arxiv.org/abs/2403.14714"
  },
  {
    "id": "rotem2021profile",
    "title": "Profile guided optimization without profiles: A machine learning approach",
    "authors": "Rotem, Nadav and Cummins, Chris",
    "year": "2021",
    "source": "arXiv preprint arXiv:2112.14679",
    "category": "profile&analysis",
    "keywords": [
      "PGO"
    ],
    "abstract": "Profile guided optimization is an effective technique for improving the optimization ability of compilers based on dynamic behavior, but collecting profile data is expensive, cumbersome, and requires regular updating to remain fresh. We present a novel statistical approach to inferring branch probabilities that improves the performance of programs that are compiled without profile guided optimizations. We perform offline training using information that is collected from a large corpus of binaries that have branch probabilities information. The learned model is used by the compiler to predict the branch probabilities of regular uninstrumented programs, which the compiler can then use to inform optimization decisions. We integrate our technique directly in LLVM, supplementing the existing human-engineered compiler heuristics. We evaluate our technique on a suite of benchmarks, demonstrating some gains over compiling without profile information. In deployment, our technique requires no profiling runs and has negligible effect on compilation time.",
    "bibtex": "@article{rotem2021profile,\n abstract = {Profile guided optimization is an effective technique for improving the optimization ability of compilers based on dynamic behavior, but collecting profile data is expensive, cumbersome, and requires regular updating to remain fresh. We present a novel statistical approach to inferring branch probabilities that improves the performance of programs that are compiled without profile guided optimizations. We perform offline training using information that is collected from a large corpus of binaries that have branch probabilities information. The learned model is used by the compiler to predict the branch probabilities of regular uninstrumented programs, which the compiler can then use to inform optimization decisions. We integrate our technique directly in LLVM, supplementing the existing human-engineered compiler heuristics. We evaluate our technique on a suite of benchmarks, demonstrating some gains over compiling without profile information. In deployment, our technique requires no profiling runs and has negligible effect on compilation time.},\n author = {Rotem, Nadav and Cummins, Chris},\n journal = {arXiv preprint arXiv:2112.14679},\n keywords = {PGO},\n link = {https://arxiv.org/abs/2112.14679},\n title = {Profile guided optimization without profiles: A machine learning approach},\n year = {2021}\n}\n",
    "link": "https://arxiv.org/abs/2112.14679"
  },
  {
    "id": "kasampalis2021language",
    "title": "Language-parametric compiler validation with application to LLVM",
    "authors": "Kasampalis, Theodoros and Park, Daejun and Lin, Zhengyao and Adve, Vikram S and Ro{\\c{s}}u, Grigore",
    "year": "2021",
    "source": "Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
    "category": "validation",
    "keywords": [
      "translation",
      "MIR",
      "IR"
    ],
    "abstract": "We propose a new design for a Translation Validation (TV) system geared towards practical use with modern optimizing compilers, such as LLVM. Unlike existing TV systems, which are custom-tailored for a particular sequence of transformations and a specific, common language for input and output programs, our design clearly separates the transformation-specific components from the rest of the system, and generalizes the transformation-independent components. Specifically, we present Keq, the first program equivalence checker that is parametric to the input and output language semantics and has no dependence on the transformation between the input and output programs. The Keq algorithm is based on a rigorous formalization, namely cut-bisimulation, and is proven correct. We have prototyped a TV system for the Instruction Selection pass of LLVM, being able to automatically prove equivalence for translations from LLVM IR to the MachineIR used in compiling to x86-64. This transformation uses different input and output languages, and as such has not been previously addressed by the state of the art. An experimental evaluation shows that Keq successfully proves correct the translation of over 90% of 4732 supported functions in GCC from SPEC 2006.",
    "bibtex": "@inproceedings{kasampalis2021language,\n abstract = {We propose a new design for a Translation Validation (TV) system geared towards practical use with modern optimizing compilers, such as LLVM. Unlike existing TV systems, which are custom-tailored for a particular sequence of transformations and a specific, common language for input and output programs, our design clearly separates the transformation-specific components from the rest of the system, and generalizes the transformation-independent components. Specifically, we present Keq, the first program equivalence checker that is parametric to the input and output language semantics and has no dependence on the transformation between the input and output programs. The Keq algorithm is based on a rigorous formalization, namely cut-bisimulation, and is proven correct. We have prototyped a TV system for the Instruction Selection pass of LLVM, being able to automatically prove equivalence for translations from LLVM IR to the MachineIR used in compiling to x86-64. This transformation uses different input and output languages, and as such has not been previously addressed by the state of the art. An experimental evaluation shows that Keq successfully proves correct the translation of over 90% of 4732 supported functions in GCC from SPEC 2006.},\n author = {Kasampalis, Theodoros and Park, Daejun and Lin, Zhengyao and Adve, Vikram S and Ro{\\c{s}}u, Grigore},\n booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},\n keywords = {translation,MIR,IR},\n link = {https://dl.acm.org/doi/abs/10.1145/3445814.3446751},\n pages = {1004--1019},\n title = {Language-parametric compiler validation with application to LLVM},\n year = {2021}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3445814.3446751"
  },
  {
    "id": "stepp2011equality",
    "title": "Equality-based translation validator for LLVM",
    "authors": "Stepp, Michael and Tate, Ross and Lerner, Sorin",
    "year": "2011",
    "source": "Computer Aided Verification: 23rd International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings 23",
    "category": "validation",
    "keywords": [
      "SPEC"
    ],
    "abstract": "We updated our Peggy tool, previously presented in [6], to perform translation validation for the LLVM compiler using a technique called Equality Saturation. We present the tool, and illustrate its effectiveness at doing translation validation on SPEC 2006 benchmarks.",
    "bibtex": "@inproceedings{stepp2011equality,\n abstract = {We updated our Peggy tool, previously presented in [6], to perform translation validation for the LLVM compiler using a technique called Equality Saturation. We present the tool, and illustrate its effectiveness at doing translation validation on SPEC 2006 benchmarks.},\n author = {Stepp, Michael and Tate, Ross and Lerner, Sorin},\n booktitle = {Computer Aided Verification: 23rd International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings 23},\n keywords = {SPEC},\n link = {https://link.springer.com/chapter/10.1007/978-3-642-22110-1_59},\n organization = {Springer},\n pages = {737--742},\n title = {Equality-based translation validator for LLVM},\n year = {2011}\n}\n",
    "link": "https://link.springer.com/chapter/10.1007/978-3-642-22110-1_59"
  },
  {
    "id": "lopes2021alive2",
    "title": "Alive2: bounded translation validation for LLVM",
    "authors": "Lopes, Nuno P and Lee, Juneyoung and Hur, Chung-Kil and Liu, Zhengyang and Regehr, John",
    "year": "2021",
    "source": "Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation",
    "category": "validation",
    "keywords": [
      "SMT",
      "alive2"
    ],
    "abstract": "We designed, implemented, and deployed Alive2: a bounded translation validation tool for the LLVM compiler’s intermediate representation (IR). It limits resource consumption by, for example, unrolling loops up to some bound, which means there are circumstances in which it misses bugs. Alive2 is designed to avoid false alarms, is fully automatic through the use of an SMT solver, and requires no changes to LLVM. By running Alive2 over LLVM’s unit test suite, we discovered and reported 47 new bugs, 28 of which have been fixed already. Moreover, our work has led to eight patches to the LLVM Language Reference—the definitive description of the semantics of its IR—and we have participated in numerous discussions with the goal of clarifying ambiguities and fixing errors in these semantics. Alive2 is open source and we also made it available on the web, where it has active users from the LLVM community.",
    "bibtex": "@inproceedings{lopes2021alive2,\n abstract = {We designed, implemented, and deployed Alive2: a bounded translation validation tool for the LLVM compiler’s intermediate representation (IR). It limits resource consumption by, for example, unrolling loops up to some bound, which means there are circumstances in which it misses bugs. Alive2 is designed to avoid false alarms, is fully automatic through the use of an SMT solver, and requires no changes to LLVM. By running Alive2 over LLVM’s unit test suite, we discovered and reported 47 new bugs, 28 of which have been fixed already. Moreover, our work has led to eight patches to the LLVM Language Reference—the definitive description of the semantics of its IR—and we have participated in numerous discussions with the goal of clarifying ambiguities and fixing errors in these semantics. Alive2 is open source and we also made it available on the web, where it has active users from the LLVM community.},\n author = {Lopes, Nuno P and Lee, Juneyoung and Hur, Chung-Kil and Liu, Zhengyang and Regehr, John},\n booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},\n keywords = {SMT,alive2},\n link = {https://dl.acm.org/doi/abs/10.1145/3453483.3454030},\n pages = {65--79},\n title = {Alive2: bounded translation validation for LLVM},\n year = {2021}\n}\n",
    "link": "https://dl.acm.org/doi/abs/10.1145/3453483.3454030"
  },
  {
    "id": "zhang2022heterogen",
    "title": "HeteroGen: transpiling C to heterogeneous HLS code with automated test generation and program repair",
    "authors": "Zhang, Qian and Wang, Jiyuan and Xu, Guoqing Harry and Kim, Miryung",
    "year": "2022",
    "source": "Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems",
    "category": "transpiling",
    "keywords": [
      "HLS",
      "transpile",
      "C",
      "APR"
    ],
    "abstract": "Despite the trend of incorporating heterogeneity and specialization in hardware, the development of heterogeneous applications is limited to a handful of engineers with deep hardware expertise. We propose HeteroGen that takes C/C++ code as input and automatically generates an HLS version with test behavior preservation and better performance. Key to the success of HeteroGen is adapting the idea of search-based program repair to the heterogeneous computing domain, while addressing two technical challenges. First, the turn-around time of HLS compilation and simulation is much longer than the usual C/C++ compilation and execution time; therefore, HeteroGen applies pattern-oriented program edits guided by common fix patterns and their dependences. Second, behavior and performance checking requires testing, but test cases are often unavailable. Thus, HeteroGen auto-generates test inputs suitable for checking C to HLS-C conversion errors, while providing high branch coverage for the original C code. An evaluation of HeteroGen shows that it produces an HLS-compatible version for nine out of ten real-world heterogeneous applications fully automatically, applying up to 438 lines of edits to produce an HLS version 1.63x faster than the original version.",
    "bibtex": "@inproceedings{zhang2022heterogen,\n abstract = {Despite the trend of incorporating heterogeneity and specialization in hardware, the development of heterogeneous applications is limited to a handful of engineers with deep hardware expertise. We propose HeteroGen that takes C/C++ code as input and automatically generates an HLS version with test behavior preservation and better performance. Key to the success of HeteroGen is adapting the idea of search-based program repair to the heterogeneous computing domain, while addressing two technical challenges. First, the turn-around time of HLS compilation and simulation is much longer than the usual C/C++ compilation and execution time; therefore, HeteroGen applies pattern-oriented program edits guided by common fix patterns and their dependences. Second, behavior and performance checking requires testing, but test cases are often unavailable. Thus, HeteroGen auto-generates test inputs suitable for checking C to HLS-C conversion errors, while providing high branch coverage for the original C code. An evaluation of HeteroGen shows that it produces an HLS-compatible version for nine out of ten real-world heterogeneous applications fully automatically, applying up to 438 lines of edits to produce an HLS version 1.63x faster than the original version.},\n author = {Zhang, Qian and Wang, Jiyuan and Xu, Guoqing Harry and Kim, Miryung},\n booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},\n keywords = {HLS,transpile,C,APR},\n link = {https://dl.acm.org/doi/10.1145/3503222.3507748},\n pages = {1017--1029},\n title = {HeteroGen: transpiling C to heterogeneous HLS code with automated test generation and program repair},\n year = {2022}\n}\n",
    "link": "https://dl.acm.org/doi/10.1145/3503222.3507748"
  }
]