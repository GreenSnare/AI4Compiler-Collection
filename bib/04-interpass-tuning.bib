@inproceedings{pan2025towards,
  title={Towards efficient compiler auto-tuning: Leveraging synergistic search spaces},
  author={Pan, Haolin and Wei, Yuanyu and Xing, Mingjie and Wu, Yanjun and Zhao, Chen},
  booktitle={Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization},
  pages={614--627},
  year={2025},
    abstract={Determining the optimal sequence of compiler optimization passes is challenging due to the extensive and intricate search space. Traditional auto-tuning techniques, such as iterative compilation and machine learning methods, are often limited by high computational costs and difficulties in generalizing to new programs. These approaches can be inefficient and may not fully address the varying optimization needs across different programs. This paper introduces a novel approach that leverages the synergistic relationships between optimization passes to effectively reduce the search space. By focusing on chained synergy pass pairs that jointly optimize a specific target, our method uses K-means clustering to capture common optimization patterns across programs and forms these pairs into coresets. Leveraging a supervised learning model trained on these coresets, we effectively predict the most beneficial coreset for new programs, streamlining the search for optimal sequences. By integrating various search strategies, our method quickly converges to near-optimal solutions. Our approach achieves state-of-the-art performance on ten benchmark datasets, including MiBench, CBench, NPB, and CHStone, demonstrating an average reduction of 7.5% in Intermediate Representation (IR) instruction count compared to Oz. Furthermore, this set of chained synergy pass pairs is also well-suited for iterative search studies by other researchers, as it enables achieving an average codesize reduction of 13.9% compared to Oz with a simple search strategy that takes only about 5 seconds, outperforming existing search-based techniques in the initial pass search space across five datasets.},
    link = {https://doi.org/10.1145/3696443.3708961},
    keywords = {kmeans,codesize,GA,},
}


@inproceedings{ogilvie2017minimizing,
  title={Minimizing the cost of iterative compilation with active learning},
  author={Ogilvie, William F and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},
  booktitle={2017 IEEE/ACM international symposium on code generation and optimization (CGO)},
  pages={245--256},
  year={2017},
  organization={IEEE},
  link = {https://doi.org/10.1109/CGO.2017.7863744},
  abstract = {Since performance is not portable between platforms, engineers must fine-tune heuristics for each processor in turn. This is such a laborious task that high-profile compilers, supporting many architectures, cannot keep up with hardware innovation and are actually out-of-date. Iterative compilation driven by machine learning has been shown to be efficient at generating portable optimization models automatically. However, good quality models require costly, repetitive, and extensive training which greatly hinders the wide adoption of this powerful technique. In this work, we show that much of this cost is spent collecting training data, runtime measurements for different optimization decisions, which contribute little to the final heuristic. Current implementations evaluate randomly chosen, often redundant, training examples a pre-configured, almost always excessive, number of times - a large source of wasted effort. Our approach optimizes not only the selection of training examples but also the number of samples per example, independently. To evaluate, we construct 11 high-quality models which use a combination of optimization settings to predict the runtime of benchmarks from the SPAPT suite. Our novel, broadly applicable, methodology is able to reduce the training overhead by up to 26x compared to an approach with a fixed number of sample runs, transforming what is potentially months of work into days.},
}

@inproceedings{liang2023learning,
  title={Learning compiler pass orders using coreset and normalized value prediction},
  author={Liang, Youwei and Stone, Kevin and Shameli, Ali and Cummins, Chris and Elhoushi, Mostafa and Guo, Jiadong and Steiner, Benoit and Yang, Xiaomeng and Xie, Pengtao and Leather, Hugh James and others},
  booktitle={International Conference on Machine Learning},
  pages={20746--20762},
  year={2023},
  organization={PMLR},
  abstract = {Finding the optimal pass sequence of compilation can lead to a significant reduction in program size. Prior works on compilation pass ordering have two major drawbacks. They either require an excessive budget (in terms of the number of compilation passes) at compile time or fail to generalize to unseen programs. In this work, instead of predicting passes sequentially, we directly learn a policy on the pass sequence space, which outperforms the default -Oz flag by an average of 4.5% over a large collection (4683) of unseen code repositories from diverse domains across 14 datasets. To achieve this, we first identify a small set (termed coreset) of pass sequences that generally optimize the size of most programs. Then, a policy is learned to pick the optimal sequences by predicting the normalized values of the pass sequences in the coreset. Our results demonstrate that existing human-designed compiler passes can be improved with a simple yet effective technique that leverages pass sequence space which contains dense rewards, while approaches operating on the individual pass space may suffer from issues of sparse reward, and do not generalize well to held-out programs from different domains. Website: https://rlcompopt.github.io.},
  link = {https://proceedings.mlr.press/v202/liang23f.html},
  keywords = {codesize,opensource},
}

@article{liu2022compiler,
  title={Compiler optimization parameter selection method based on ensemble learning},
  author={Liu, Hui and Xu, Jinlong and Chen, Sen and Guo, Te},
  journal={Electronics},
  volume={11},
  number={15},
  pages={2452},
  year={2022},
  publisher={MDPI},
  abstract = {Iterative compilation based on machine learning can effectively predict a program’s compiler optimization parameters. Although having some limits, such as the low efficiency of optimization parameter search and prediction accuracy, machine learning-based solutions have been a frontier research field in the field of iterative compilation and have gained increasing attention. The research challenges are focused on learning algorithm selection, optimal parameter search, and program feature representation. For the existing problems, we propose an ensemble learning-based optimization parameter selection (ELOPS) method for the compiler. First, in order to further improve the optimization parameter search efficiency and accuracy, we proposed a multi-objective particle swarm optimization (PSO) algorithm to determine the optimal compiler parameters of the program. Second, we extracted the mixed features of the program through the feature-class relevance method, rather than using static or dynamic features alone. Finally, as the existing research usually uses a separate machine learning algorithm to build prediction models, an ensemble learning model using program features and optimization parameters was constructed to effectively predict compiler optimization parameters of the new program. Using standard performance evaluation corporation 2006 (SPEC2006) and NAS parallel benchmark (NPB) benchmarks as well as some typical scientific computing programs, we compared ELOPS with the existing methods. The experimental results showed that we can respectively achieve 1.29× and 1.26× speedup when using our method on two platforms, which are better results than those of existing methods.},
  keywords = {PSO},
  link = {https://www.mdpi.com/2079-9292/11/15/2452},
}

@article{zhu2024compiler,
  title={Compiler autotuning through multiple-phase learning},
  author={Zhu, Mingxuan and Hao, Dan and Chen, Junjie},
  journal={ACM Transactions on Software Engineering and Methodology},
  volume={33},
  number={4},
  pages={1--38},
  year={2024},
  publisher={ACM New York, NY},
  abstract = {Widely used compilers like GCC and LLVM usually have hundreds of optimizations controlled by optimization flags, which are enabled or disabled during compilation to improve the runtime performance (e.g., small execution time) of the compiler program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to manually tune compiler optimization flags. In the literature, a number of autotuning techniques have been proposed, which tune optimization flags for a compiled program by comparing its actual runtime performance with different optimization flag combinations. Due to the huge search space and heavy actual runtime cost, these techniques suffer from the widely recognized efficiency problem. To reduce the heavy runtime cost, in this article we propose a lightweight learning approach that uses a small number of actual runtime performance data to predict the runtime performance of a compiled program with various optimization flag combinations. Furthermore, to reduce the search space, we design a novel particle swarm algorithm that tunes compiler optimization flags with the prediction model. To evaluate the performance of the proposed approach, CompTuner, we conduct an extensive experimental study on two popular C compilers, GCC and LLVM, with two widely used benchmarks, cBench and PolyBench. The experimental results show that CompTuner significantly outperforms the six compared techniques, including the state-of-the-art technique BOCA.},
  keywords = {PSO},
  link = {https://dl.acm.org/doi/abs/10.1145/3640330},
}

@article{ni2024tsoa,
  title={Tsoa: a two-stage optimization approach for GCC compilation options to minimize execution time},
  author={Ni, Youcong and Du, Xin and Yuan, Yuan and Xiao, Ruliang and Chen, Gaolin},
  journal={Automated Software Engineering},
  volume={31},
  number={2},
  pages={39},
  year={2024},
  publisher={Springer},
  abstract = {The open-source compiler GCC offers numerous options to improve execution time. Two categories of approaches, machine learning-based and design space exploration, have emerged for selecting the optimal set of options. However, they continue to face challenge in quickly obtaining high-quality solutions due to the large and discrete optimization space, time-consuming utility evaluation for selected options, and complex interactions among options. To address these challenges, we propose TSOA, a Two-Stage Optimization Approach for GCC compilation options to minimize execution time. In the first stage, we present OPPM, an Option Preselection algorithm based on Pattern Mining. OPPM generates diverse samples to cover a wide range of option interactions. It subsequently mines frequent options from both objective-improved and non-improved samples. The mining results are further validated using CRC codes to precisely preselect options and reduce the optimization space. Transitioning to the second stage, we present OSEA, an Option Selection Evolutionary optimization Algorithm. OSEA is grounded in solution preselection and an option interaction graph. The solution preselection employs a random forest to build a classifier, efficiently identifying promising solutions for the next-generation population and thereby reducing the time spent on utility evaluation. Simultaneously, the option interaction graph is built to capture option interplays and their influence on objectives from evaluated solutions. Then, high-quality solutions are generated based on the option interaction graph. We evaluate the performance of TSOA by comparing it with representative machine learning-based and design space exploration approaches across a diverse set of 20 problem instances from two benchmark platforms. Additionally, we validate the effectiveness of OPPM and conduct related ablation experiments. The experimental results show that TSOA outperforms state-of-the-art approaches significantly in both optimization time and solution quality. Moreover, OPPM outperforms other option preselection algorithms, while the effectiveness of random forest-assisted solution preselection, along with new solution generation based on the option interaction graph, has been verified.},
  link = {https://link.springer.com/article/10.1007/s10515-024-00437-w},
  keywords = {randomforest},
}

@inproceedings{park2022srtuner,
  title={Srtuner: Effective compiler optimization customization by exposing synergistic relations},
  author={Park, Sunghyun and Latifi, Salar and Park, Yongjun and Behroozi, Armand and Jeon, Byungsoo and Mahlke, Scott},
  booktitle={2022 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  pages={118--130},
  year={2022},
  organization={IEEE},
  abstract = {Despite ceaseless efforts, extremely large and complex optimization space makes even the state-of-the-art compilers fail in delivering the most performant setting that can fully utilize the underlying hardware. Although this inefficiency suggests opportunity for tuning, it has been challenging for prior tuning methods to consider the complex interactions between optimizations and maximize the tuning quality while handling local optima efficiently. To tackle this problem, we suggest an intelligent auto-tuning strategy, called SRTuner, which searches for the best optimization setting by exposing important optimization interactions and directly using them to focus on promising subspaces. To reveal high-impact inter-optimization relations, SRTuner proposes a multistage structure and a distribution-based estimation method that approximates the impact of an optimization effectively. Besides, to efficiently handle local optima, our technique defines optimization decisions as a series of multi-armed bandit problems to formulate the exploration-exploitation dilemma. SRTuner is evaluated with three representative compilers from various domains on different target hardware: GCC (traditional C/ C++ compiler) on CPU, TVM (domain-specific machine learning compiler) on GPU, and OpenCL compilers (kernel compiler for heterogeneous computing) on both CPU/GPU. Results show that SRTuner accelerates target executions by 1.24×, 2.03× and 34.4× compared to the highest level of optimization provided by each compiler and outperforms state-of-the-art works by 1.04×−1.14×. As a byproduct of our unique tuning strategy, SRTuner can offer synergistic optimizations for each workload, which allows it to in part identify why it outperformed current compilers. With this information, we are able to find important optimizations that each compiler misused and demonstrate how this information can benefit future tuning strategies.},
  link = {https://ieeexplore.ieee.org/abstract/document/9741263},
}

@inproceedings{taugtekin2021foga,
  title={Foga: Flag optimization with genetic algorithm},
  author={Ta{\u{g}}tekin, Burak and H{\"o}ke, Berkan and Sezer, Mert Kutay and {\"O}zt{\"u}rk, Mahiye Uluya{\u{g}}mur},
  booktitle={2021 International Conference on INnovations in Intelligent SysTems and Applications (INISTA)},
  pages={1--6},
  year={2021},
  organization={IEEE},
  abstract = {Recently, program autotuning has become very popular especially in embedded systems, when we have limited resources such as computing power and memory where these systems run generally time-critical applications. Compiler optimization space gradually expands with the renewed compiler options and inclusion of new architectures. These advancements bring autotuning even more important position. In this paper, we introduced Flag Optimization with Genetic Algorithm (FOGA) as an autotuning solution for GCC flag optimization. FOGA has two main advantages over the other autotuning approaches: the first one is the hyperparameter tuning of the genetic algorithm (GA), the second one is the maximum iteration parameter to stop when no further improvement occurs. We demonstrated remarkable speedup in the execution time of C++ source codes with the help of optimization flags provided by FOGA when compared to the state of the art framework OpenTuner.},
  link = {https://doi.org/10.1109/INISTA52262.2021.9548573},
  keywords = {GA},
}

@article{liu2021iterative,
  title={Iterative compilation optimization based on metric learning and collaborative filtering},
  author={Liu, Hongzhi and Luo, Jie and Li, Ying and Wu, Zhonghai},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={19},
  number={1},
  pages={1--25},
  year={2021},
  publisher={ACM New York, NY},
  keywords = {PCA,EFA},
  link = {https://dl.acm.org/doi/full/10.1145/3480250},
  abstract = {Pass selection and phase ordering are two critical compiler auto-tuning problems. Traditional heuristic methods cannot effectively address these NP-hard problems especially given the increasing number of compiler passes and diverse hardware architectures. Recent research efforts have attempted to address these problems through machine learning. However, the large search space of candidate pass sequences, the large numbers of redundant and irrelevant features, and the lack of training program instances make it difficult to learn models well. Several methods have tried to use expert knowledge to simplify the problems, such as using only the compiler passes or subsequences in the standard levels (e.g., -O1, -O2, and -O3) provided by compiler designers. However, these methods ignore other useful compiler passes that are not contained in the standard levels. Principal component analysis (PCA) and exploratory factor analysis (EFA) have been utilized to reduce the redundancy of feature data. However, these unsupervised methods retain all the information irrelevant to the performance of compilation optimization, which may mislead the subsequent model learning.
To solve these problems, we propose a compiler pass selection and phase ordering approach, called Iterative Compilation based on Metric learning and Collaborative filtering (ICMC). First, we propose a data-driven method to construct pass subsequences according to the observed collaborative interactions and dependency among passes on a given program set. Therefore, we can make use of all available compiler passes and prune the search space. Then, a supervised metric learning method is utilized to retain useful feature information for compilation optimization while removing both the irrelevant and the redundant information. Based on the learned similarity metric, a neighborhood-based collaborative filtering method is employed to iteratively recommend a few superior compiler passes for each target program. Last, an iterative data enhancement method is designed to alleviate the problem of lacking training program instances and to enhance the performance of iterative pass recommendations. The experimental results using the LLVM compiler on all 32 cBench programs show the following: (1) ICMC significantly outperforms several state-of-the-art compiler phase ordering methods, (2) it performs the same or better than the standard level -O3 on all the test programs, and (3) it can reach an average performance speedup of 1.20 (up to 1.46) compared with the standard level -O3.},
}

@inproceedings{jayatilaka2021towards,
  title={Towards compile-time-reducing compiler optimization selection via machine learning},
  author={Jayatilaka, Tarindu and Ueno, Hideto and Georgakoudis, Giorgis and Park, EunJung and Doerfert, Johannes},
  booktitle={50th International Conference on Parallel Processing Workshop},
  pages={1--6},
  year={2021},
  abstract = {Compilers come with a multitude of optimizations to choose from, and the chosen optimizations significantly affect the performance of the code being optimized. Selecting the optimal set of optimizations to apply and determining the order to run them is non-trivial. All of these optimizations closely interact with each other, and an optimization’s ability to improve the code heavily depends on how the previous optimizations modified it. The current approach to solve this is to use a one-size-fits-all optimization sequence, that is designed to perform reasonably well for any given source code. In other words, they are not designed to optimize depending on the nature of the code, which usually results in sub-optimal performance. In this paper, we present preliminary work tackling the problem from the perspective of compile-time by adapting the optimization sequence to cater to different program types. We start by analyzing how the source code interacts with the passes, as well as how the passes interact with each other. We use this information and propose two potential methods driven by machine learning to run customized optimization sequences on the source code. First, we look at how we can use a neural network to predict and skip passes that do not optimize the code to improve compilation time. Second, we look at how we can use clustering and predictive models to select custom pass pipelines. We believe that our approach has the potential to replace the current one-size-fits-all approach, with better optimization sequences that are tailored to perform better depending on the code. At the same time, it will allow testing the potential pipelines thoroughly, a practical requirement to gain confidence in the correctness of the compiler.},
  link = {https://dl.acm.org/doi/abs/10.1145/3458744.3473355},
}

@inproceedings{colucci2021mlcomp,
  title={MLComp: A methodology for machine learning-based performance estimation and adaptive selection of Pareto-optimal compiler optimization sequences},
  author={Colucci, Alessio and Juh{\'a}sz, D{\'a}vid and Mosbeck, Martin and Marchisio, Alberto and Rehman, Semeen and Kreutzer, Manfred and Nadbath, G{\"u}nther and Jantsch, Axel and Shafique, Muhammad},
  booktitle={2021 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  pages={108--113},
  year={2021},
  organization={IEEE},
  abstract = {Embedded systems have proliferated in various consumer and industrial applications with the evolution of Cyber-Physical Systems and the Internet of Things. These systems are subjected to stringent constraints so that embedded software must be optimized for multiple objectives simultaneously, namely reduced energy consumption, execution time, and code size. Compilers offer optimization phases to improve these metrics. However, proper selection and ordering of them depends on multiple factors and typically requires expert knowledge. State-of-the-art optimizers facilitate different platforms and applications case by case, and they are limited by optimizing one metric at a time, as well as requiring a time-consuming adaptation for different targets through dynamic profiling. To address these problems, we propose the novel MLComp methodology, in which optimization phases are sequenced by a Reinforcement Learning-based policy. Training of the policy is supported by Machine Learning-based analytical models for quick performance estimation, thereby drastically reducing the time spent for dynamic profiling. In our framework, different Machine Learning models are automatically tested to choose the best-fitting one. The trained Performance Estimator model is leveraged to efficiently devise Reinforcement Learning-based multi-objective policies for creating quasi-optimal phase sequences. Compared to state-of-the-art estimation models, our Performance Estimator model achieves lower relative error (< 2%) with up to 50 × faster training time over multiple platforms and application domains. Our Phase Selection Policy improves execution time and energy consumption of a given code by up to 12% and 6%, respectively. The Performance Estimator and the Phase Selection Policy can be trained efficiently for any target platform and application domain.},
  link = {https://ieeexplore.ieee.org/abstract/document/9474158},
}

@inproceedings{chen2021efficient,
  title={Efficient compiler autotuning via bayesian optimization},
  author={Chen, Junjie and Xu, Ningxin and Chen, Peiqi and Zhang, Hongyu},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  pages={1198--1209},
  year={2021},
  organization={IEEE},
  abstract = {A typical compiler such as GCC supports hundreds of optimizations controlled by compilation flags for improving the runtime performance of the compiled program. Due to the large number of compilation flags and the exponential number of flag combinations, it is impossible for compiler users to manually tune these optimization flags in order to achieve the required runtime performance of the compiled programs. Over the years, many compiler autotuning approaches have been proposed to automatically tune optimization flags, but they still suffer from the efficiency problem due to the huge search space. In this paper, we propose the first Bayesian optimization based approach, called BOCA, for efficient compiler autotuning. In BOCA, we leverage a tree-based model for approximating the objective function in order to make Bayesian optimization scalable to a large number of optimization flags. Moreover, we design a novel searching strategy to improve the efficiency of Bayesian optimization by incorporating the impact of each optimization flag measured by the tree-based model and a decay function to strike a balance between exploitation and exploration. We conduct extensive experiments to investigate the effectiveness of BOCA on two most popular C compilers (i.e., GCC and LLVM) and two widely-used C benchmarks (i.e., cBench and PolyBench). The results show that BOCA significantly outperforms the state-of-the-art compiler autotuning approaches and Bayesion optimization methods in terms of the time spent on achieving specified speedups, demonstrating the effectiveness of BOCA.},
  link = {https://ieeexplore.ieee.org/abstract/document/9401979},
  keywords = {Bayesian},
}

@inproceedings{huang2019autophase,
  title={Autophase: Compiler phase-ordering for hls with deep reinforcement learning},
  author={Huang, Qijing and Haj-Ali, Ameer and Moses, William and Xiang, John and Stoica, Ion and Asanovic, Krste and Wawrzynek, John},
  booktitle={2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
  pages={308--308},
  year={2019},
  organization={IEEE},
  abstract = {The performance of the code generated by a compiler depends on the order in which the optimization passes are applied. In high-level synthesis, the quality of the generated circuit relates directly to the code generated by the front-end compiler. Choosing a good order-often referred to as the phase-ordering problem-is an NP-hard problem. In this paper, we evaluate a new technique to address the phase-ordering problem: deep reinforcement learning. We implement a framework in the context of the LLVM compiler to optimize the ordering for HLS programs and compare the performance of deep reinforcement learning to state-of-the-art algorithms that address the phase-ordering problem. Overall, our framework runs one to two orders of magnitude faster than these algorithms, and achieves a 16% improvement in circuit performance over the -O3 compiler flag.},
  link = {https://doi.org/10.1109/FCCM.2019.00049},
}

@article{ashouri2017micomp,
  title={Micomp: Mitigating the compiler phase-ordering problem using optimization sub-sequences and machine learning},
  author={Ashouri, Amir H and Bignoli, Andrea and Palermo, Gianluca and Silvano, Cristina and Kulkarni, Sameer and Cavazos, John},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={14},
  number={3},
  pages={1--28},
  year={2017},
  publisher={ACM New York, NY, USA},
  abstract = {Recent compilers offer a vast number of multilayered optimizations targeting different code segments of an application. Choosing among these optimizations can significantly impact the performance of the code being optimized. The selection of the right set of compiler optimizations for a particular code segment is a very hard problem, but finding the best ordering of these optimizations adds further complexity. Finding the best ordering represents a long standing problem in compilation research, named the phase-ordering problem. The traditional approach of constructing compiler heuristics to solve this problem simply cannot cope with the enormous complexity of choosing the right ordering of optimizations for every code segment in an application.This article proposes an automatic optimization framework we call MiCOMP, which <u>Mi</u>tigates the <u>Com</u>piler <u>P</u>hase-ordering problem. We perform phase ordering of the optimizations in LLVM’s highest optimization level using optimization sub-sequences and machine learning. The idea is to cluster the optimization passes of LLVM’s O3 setting into different clusters to predict the speedup of a complete sequence of all the optimization clusters instead of having to deal with the ordering of more than 60 different individual optimizations. The predictive model uses (1) dynamic features, (2) an encoded version of the compiler sequence, and (3) an exploration heuristic to tackle the problem.Experimental results using the LLVM compiler framework and the Cbench suite show the effectiveness of the proposed clustering and encoding techniques to application-based reordering of passes, while using a number of predictive models. We perform statistical analysis on the results and compare against (1) random iterative compilation, (2) standard optimization levels, and (3) two recent prediction approaches. We show that MiCOMP’s iterative compilation using its sub-sequences can reach an average performance speedup of 1.31 (up to 1.51). Additionally, we demonstrate that MiCOMP’s prediction model outperforms the -O1, -O2, and -O3 optimization levels within using just a few predictions and reduces the prediction error rate down to only 5%. Overall, it achieves 90% of the available speedup by exploring less than 0.001% of the optimization space.},
  link = {https://dl.acm.org/doi/abs/10.1145/3124452},
}

@inproceedings{seeker2024revealing,
  title={Revealing compiler heuristics through automated discovery and optimization},
  author={Seeker, Volker and Cummins, Chris and Cole, Murray and Franke, Bj{\"o}rn and Hazelwood, Kim and Leather, Hugh},
  booktitle={2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  pages={55--66},
  year={2024},
  organization={IEEE},
  abstract = {Tuning compiler heuristics and parameters is well known to improve optimization outcomes dramatically. Prior works have tuned command line flags and a few expert identified heuristics. However, there are an unknown number of heuristics buried, unmarked and unexposed inside the compiler as a consequence of decades of development without auto-tuning being foremost in the minds of developers. Many may not even have been considered heuristics by the developers who wrote them. The result is that auto-tuning search and machine learning can optimize only a tiny fraction of what could be possible if all heuristics were available to tune. Manually discovering all of these heuristics hidden among millions of lines of code and exposing them to auto-tuning tools is a Herculean task that is simply not practical. What is needed is a method of automatically finding these heuristics to extract every last drop of potential optimization. In this work, we propose Heureka, a framework that automatically identifies potential heuristics in the compiler that are highly profitable optimization targets and then automatically finds available tuning parameters for those heuristics with minimal human involvement. Our work is based on the following key insight: When modifying the output of a heuristic within an acceptable value range, the calling code using that output will still function correctly and produce semantically correct results. Building on that, we automatically manipulate the output of potential heuristic code in the compiler and decide using a Differential Testing approach if we found a heuristic or not. During output manipulation, we also explore acceptable value ranges of the targeted code. Heuristics identified in this way can then be tuned to optimize an objective function. We used Heureka to search for heuristics among eight thousand functions from the LLVM optimization passes, which is about 2% of all available functions. We then use identified heuristics to tune the compilation of 38 applications from the NAS and Polybench benchmark suites. Compared to an -ozbaseline we reduce binary sizes by up to 11.6% considering single heuristics only and up to 19.5% when stacking the effects of multiple identified tuning targets and applying a random search with minimal search effort. Generalizing from existing analysis results, Heureka needs, on average, a little under an hour on a single machine to identify relevant heuristic targets for a previously unseen application.},
  link = {https://doi.org/10.1109/CGO57630.2024.10444847},
  keywords = {search space},
}

@inproceedings{burgstaller2024optimization,
  title={Optimization Space Learning: A Lightweight, Noniterative Technique for Compiler Autotuning},
  author={Burgstaller, Tamim and Garber, Damian and Le, Viet-Man and Felfernig, Alexander},
  booktitle={Proceedings of the 28th ACM International Systems and Software Product Line Conference},
  pages={36--46},
  year={2024},
  abstract = {Compilers are highly configurable systems. One can influence the performance of a compiled program by activating and deactivating selected compiler optimizations. However, automatically finding well-performing configurations is a challenging task. We consider expensive iteration, paired with recompilation of the program to optimize, as one of the main shortcomings of state-of-the-art approaches. Therefore, we propose Optimization Space Learning, a lightweight and noniterative technique. It exploits concepts known from configuration space learning and recommender systems to discover well-performing compiler configurations. This reduces the overhead induced by the approach significantly, compared to existing approaches. The process of finding a well-performing configuration is 800k times faster than with the state-of-the-art techniques.},
  link = {https://dl.acm.org/doi/abs/10.1145/3646548.3672588},
}

@inproceedings{garciarena2016evolutionary,
  title={Evolutionary optimization of compiler flag selection by learning and exploiting flags interactions},
  author={Garciarena, Unai and Santana, Roberto},
  booktitle={Proceedings of the 2016 on Genetic and Evolutionary Computation Conference Companion},
  pages={1159--1166},
  year={2016},
  abstract = {Compiler flag selection can be an effective way to increase the quality of executable code according to different code quality criteria. Evolutionary algorithms have been successfully applied to this optimization problem. However, previous approaches have only partially addressed the question of capturing and exploiting the interactions between compilation options to improve the search. In this paper we deal with this question comparing estimation of distribution algorithms (EDAs) and a traditional genetic algorithm approach. We show that EDAs that learn bivariate interactions can improve the results of GAs for some of the programs considered. We also show that the probabilistic models generated as a result of the search for optimal flag combinations can be used to unveil the (problem-dependent) interactions between the flags, allowing the user a more informed choice of compilation options.},
  keywords = {EA},
  link = {https://dl.acm.org/doi/abs/10.1145/2908961.2931696},
}

@article{Gong2025OptimizingWB,
  title={Optimizing WebAssembly Bytecode for IoT Devices Using Deep Reinforcement Learning},
  author={Kaijie Gong and Ruiqi Yang and Haoyu Li and Yi Gao and Wei Dong},
  journal={ACM Transactions on Internet Technology},
  year={2025},
  volume={25},
  pages={1 - 26},
  abstract = {WebAssembly has shown promising potential on various IoT devices to achieve the desired features such as multi-language support and seamless device-cloud integration. The execution performance of WebAssembly bytecode is directly influenced by compilation sequences. While existing research has explored the optimization of compilation sequences for native code, these approaches are not suitable to WebAssembly bytecode due to its unique instruction format and control flow graph structure. In this work, we propose WasmRL, a novel efficient deep reinforcement learning (DRL)-based compiler optimization framework tailored for WebAssembly bytecode. We conduct a fine-grained analysis of the characteristics of WebAssembly instructions and associated compilation flags. We observe that the same compilation sequence may yield contrasting performance outcomes in WebAssembly and native code. Motivated by our observation, we introduce a WebAssembly-specific DRL state representation that simultaneously captures the impact of various compilation sequences on the WebAssembly bytecode and its runtime performance. To enhance the training efficiency of the DRL model, we propose a tree-based action space refinement method. Furthermore, we develop a pluggable cross-platform training strategy to optimize WebAssembly bytecode across different IoT devices. We evaluate the performance of WasmRL extensively on PolybenchC, MiBench, Shootout public datasets and real-world IoT applications. Experimental results show: (1) The DRL model trained on a specific device achieves 1.4x/1.1x speedups over -O3 for seen/unseen programs; (2) The DRL model trained on different devices simultaneously achieves 1.21x/1.06x improvements respectively. The code has been available at https://github.com/CarrollAdmin/WasmRL.},
  keywords = {drl},
  link={https://dl.acm.org/doi/abs/10.1145/3731451},
}

@inproceedings{zhu2023compiler,
  title={Compiler auto-tuning via critical flag selection},
  author={Zhu, Mingxuan and Hao, Dan},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={1000--1011},
  year={2023},
  organization={IEEE},
  keywords = {gcc},
  abstract = {Widely used compilers like GCC usually have hundreds of optimizations controlled by optimization flags, which can be enabled or disabled during compilation to improve the runtime performance of a compiled program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to tune compiler optimization flags manually. In the literature, many auto-tuning techniques have been proposed, which find a desired setting on all optimization flags (i.e., an optimization sequence) by designing different search strategies in the entire optimization space. Due to the huge search space, these techniques suffer from the widely-recognized efficiency problem. To reduce the search space, in this paper, we propose a critical-flag selection based approach CFSCA which first finds flags potentially relevant to the target program by analyzing program structure and compiler documentation, and then identifies critical flags through statistical analysis on the program's predicted runtime performance with various optimization sequences. With the reduced search space, CFSCA selects a desired optimization sequence. To evaluate the performance of the proposed approach CFSCA, we conduct an extensive experimental study on the latest version of the compiler GCC with a widely used benchmark cBench. The experimental results show that CFSCA significantly outperforms the four compared techniques, including the state-of-art technique BOCA.},
  link = {https://ieeexplore.ieee.org/abstract/document/10298446},
}

@inproceedings{cereda2020collaborative,
  title={A collaborative filtering approach for the automatic tuning of compiler optimisations},
  author={Cereda, Stefano and Palermo, Gianluca and Cremonesi, Paolo and Doni, Stefano},
  booktitle={The 21st ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems},
  pages={15--25},
  year={2020},
  keywords = {},
  abstract = {Selecting the right compiler optimisations has a severe impact on programs' performance. Still, the available optimisations keep increasing, and their effect depends on the specific program, making the task human intractable. Researchers proposed several techniques to search in the space of compiler optimisations. Some approaches focus on finding better search algorithms, while others try to speed up the search by leveraging previously collected knowledge. The possibility to effectively reuse previous compilation results inspired us toward the investigation of techniques derived from the Recommender Systems field. The proposed approach exploits previously collected knowledge and improves its characterisation over time. Differently from current state-of-the-art solutions, our approach is not based on performance counters but relies on Reaction Matching, an algorithm able to characterise programs looking at how they react to different optimisation sets. The proposed approach has been validated using two widely used benchmark suites, cBench and PolyBench, including 54 different programs. Our solution, on average, extracted 90% of the available performance improvement 10 iterations before current state-of-the-art solutions,which corresponds to 40% fewer compilations and performance tests to perform.},
  link = {https://dl.acm.org/doi/abs/10.1145/3372799.3394361},
}

@inproceedings{shahzad2022reinforcement,
  title={Reinforcement learning strategies for compiler optimization in high level synthesis},
  author={Shahzad, Hafsah and Sanaullah, Ahmed and Arora, Sanjay and Munafo, Robert and Yao, Xiteng and Drepper, Ulrich and Herbordt, Martin},
  booktitle={2022 IEEE/ACM Eighth Workshop on the LLVM Compiler Infrastructure in HPC (LLVM-HPC)},
  pages={13--22},
  year={2022},
  organization={IEEE},
  keywords = {ML,llvm,HLS},
  abstract = {High Level Synthesis (HLS) offers a possible programmability solution for FPGAs by automatically compiling CPU codes to custom hardware configurations, but currently delivers far lower hardware quality than circuits written using Hardware Description Languages (HDLs). One reason is because the standard set of code optimizations used by CPU compilers, such as LLVM, are not well suited for a FPGA back end. Code performance is impacted largely by the order in which passes are applied. Similarly, it is also imperative to find a reasonable number of passes to apply and the optimum pass parameter values. In order to bridge the gap between hand tuned and automatically generated hardware, it is thus important to determine the optimal sequence of passes for HLS compilations, which could vary substantially across different workloads. Machine learning (ML) offers one popular approach to automate finding optimal compiler passes but requires selecting the right method. Supervised ML is not ideal since it requires labeled data mapping workload to optimal (or close to optimal) sequence of passes, which is computationally prohibitive. Unsupervised ML techniques don’t take into account the requirement that a quantity representing performance needs to be maximized. Reinforcement learning, which represents the problem of maximizing longterm rewards without requiring labeled data has been used for such planning problems before. While much work has been done along these lines for compilers in general, that directed towards HLS has been limited and conservative. In this paper, we address these limitations by expanding both the number of learning strategies for HLS compiler tuning and the metrics used to evaluate their impact. Our results show improvements over state-of-art for each standard benchmark evaluated and learning quality metric investigated. Choosing just the right strategy can give an improvement of 23× in learning speed, 4× in performance potential, 3×...},
  link = {https://ieeexplore.ieee.org/abstract/document/10027131},

}

@inproceedings{xiao2024eatuner,
  title={EAtuner: Comparative Study of Evolutionary Algorithms for Compiler Auto-tuning},
  author={Xiao, Guojian and Qin, Siyuan and Li, Kuan and Chen, Juan and Yin, Jianping},
  booktitle={2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)},
  pages={419--426},
  year={2024},
  organization={IEEE},
  keywords = {DE,llvm},
  abstract = {The manual adjustment of compilation flags by compiler users is impractical due to the exponential size of the search space. To address this, machine learning-based compiler auto-tuning methods, particularly evolutionary algorithms, have been proposed. However, existing works use different benchmarks and experimental setups, making it difficult to compare the strengths and weaknesses of various algorithms. To address this, we present EAtuner, an evolutionary algorithm-based framework for compiler auto-tuning, with the goal of benchmarking and identifying suitable algorithms for compiler auto-tuning. We implement ten discrete binary evolutionary algorithms and evaluate their effectiveness on the LLVM compiler through experiments. Notably, eight of these algorithms have not been previously applied to compiler flag optimization problems before our work. The results show that all ten algorithms can effectively achieve compiler auto-tuning, resulting in an average speedup of 1.204. However, there are notable differences in the effectiveness and efficiency of each algorithm, particularly in optimization efficiency, which is positively correlated with the number of program compilations. Based on this, we classify the algorithms into three levels, with Differential Evolution (DE) showing significant advantages in optimization effectiveness and efficiency. Additionally, we provide a comprehensive summary of the applicability of compiler flags, the correlation between them, and their relationship with programs.},
  link = {https://ieeexplore.ieee.org/abstract/document/10580120},
}

@inproceedings{gao2025grouptuner,
  title={Grouptuner: Efficient Group-Aware Compiler Auto-tuning},
  author={Gao, Bingyu and Yao, Mengyu and Wang, Ziming and Liu, Dong and Li, Ding and Chen, Xiangqun and Guo, Yao},
  booktitle={Proceedings of the 26th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
  pages={122--133},
  year={2025},
  abstract={Modern compilers typically provide hundreds of options to optimize program performance, but users often cannot fully leverage them due to the huge number of options. While standard optimization combinations (e.g., -O3) provide reasonable defaults, they often fail to deliver near-peak performance across diverse programs and architectures. To address this challenge, compiler auto-tuning techniques have emerged to automate the discovery of improved option combinations. Existing techniques typically focus on identifying critical options and prioritizing them during the search to improve efficiency. However, due to limited tuning iterations, the resulting data is often sparse and noisy, making it highly challenging to accurately identify critical options. As a result, these algorithms are prone to being trapped in local optima. To address this limitation, we propose GroupTuner, a group-aware auto-tuning technique that directly applies localized mutation to coherent option groups based on historically best-performing combinations, thus avoiding explicitly identifying critical options. By forgoing the need to know precisely which options are most important, GroupTuner maximizes the use of existing performance data, ensuring more targeted exploration. Extensive experiments demonstrate that GroupTuner can efficiently discover competitive option combinations, achieving an average performance improvement of 12.39% over -O3 while requiring only 77.21% of the time compared to the random search algorithm, significantly outperforming state-of-the-art methods.},
  link={https://doi.org/10.1145/3735452.3735530},
  keywords = {},
}

@inproceedings{ansel2014opentuner,
  title={Opentuner: An extensible framework for program autotuning},
  author={Ansel, Jason and Kamil, Shoaib and Veeramachaneni, Kalyan and Ragan-Kelley, Jonathan and Bosboom, Jeffrey and O'Reilly, Una-May and Amarasinghe, Saman},
  booktitle={Proceedings of the 23rd international conference on Parallel architectures and compilation},
  pages={303--316},
  year={2014},
  keywords = {},
  link = {https://dl.acm.org/doi/abs/10.1145/2628071.2628092},
  abstract = {Program autotuning has been shown to achieve better or more portable performance in a number of domains. However, autotuners themselves are rarely portable between projects, for a number of reasons: using a domain-informed search space representation is critical to achieving good results; search spaces can be intractably large and require advanced machine learning techniques; and the landscape of search spaces can vary greatly between different problems, sometimes requiring domain specific search techniques to explore efficiently. This paper introduces OpenTuner, a new open source framework for building domain-specific multi-objective program autotuners. OpenTuner supports fully-customizable configuration representations, an extensible technique representation to allow for domain-specific techniques, and an easy to use interface for communicating with the program to be autotuned. A key capability inside OpenTuner is the use of ensembles of disparate search techniques simultaneously; techniques that perform well will dynamically be allocated a larger proportion of tests. We demonstrate the efficacy and generality of OpenTuner by building autotuners for 7 distinct projects and 16 total benchmarks, showing speedups over prior techniques of these projects of up to 2.8x with little programmer effort.},
}

@article{chen2012deconstructing,
  title={Deconstructing iterative optimization},
  author={Chen, Yang and Fang, Shuangde and Huang, Yuanjie and Eeckhout, Lieven and Fursin, Grigori and Temam, Olivier and Wu, Chengyong},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={9},
  number={3},
  pages={1--30},
  year={2012},
  publisher={ACM New York, NY, USA},
  link = {https://dl.acm.org/doi/abs/10.1145/2355585.2355594},
  abstract = {Iterative optimization is a popular compiler optimization approach that has been studied extensively over the past decade. In this article, we deconstruct iterative optimization by evaluating whether it works across datasets and by analyzing why it works. Up to now, most iterative optimization studies are based on a premise which was never truly evaluated: that it is possible to learn the best compiler optimizations across datasets. In this article, we evaluate this question for the first time with a very large number of datasets. We therefore compose KDataSets, a dataset suite with 1000 datasets for 32 programs, which we release to the public. We characterize the diversity of KDataSets, and subsequently use it to evaluate iterative optimization. For all 32 programs, we find that there exists at least one combination of compiler optimizations that achieves at least 83% or more of the best possible speedup across all datasets on two widely used compilers (Intel's ICC and GNU's GCC). This optimal combination is program-specific and yields speedups up to 3.75× (averaged across datasets of a program) over the highest optimization level of the compilers (-O3 for GCC and -fast for ICC). This finding suggests that optimizing programs across datasets might be much easier than previously anticipated. In addition, we evaluate the idea of introducing compiler choice as part of iterative optimization. We find that it can further improve the performance of iterative optimization because different programs favor different compilers. We also investigate why iterative optimization works by analyzing the optimal combinations. We find that only a handful optimizations yield most of the speedup. Finally, we show that optimizations interact in a complex and sometimes counterintuitive way through two case studies, which confirms that iterative optimization is an irreplaceable and important compiler strategy.},
  keywords = {GCC},
}

%@inproceedings{he2022multi,
%  title={Multi-intention-aware configuration selection for performance tuning},
%  author={He, Haochen and Jia, Zhouyang and Li, Shanshan and Yu, Yue and Zhou, Chenglong and Liao, Qing and Wang, Ji and Liao, Xiangke},
%  booktitle={Proceedings of the 44th International Conference on Software Engineering},
%  pages={1431--1442},
%  year={2022},
%  link = {https://dl.acm.org/doi/abs/10.1145/3510003.3510094},
%  abstract = {Automatic configuration tuning helps users who intend to improve software performance. However, the auto-tuners are limited by the huge configuration search space. More importantly, they focus only on performance improvement while being unaware of other important user intentions (e.g., reliability, security). To reduce the search space, researchers mainly focus on pre-selecting performance-related parameters which requires a heavy stage of dynamically running under different configurations to build performance models. Given that other important user intentions are not paid attention to, we focus on guiding users in pre-selecting performance-related parameters in general while warning about side-effects on non-performance intentions. We find that the configuration document often, if it does not always, contains rich information about the parameters' relationship with diverse user intentions, but documents might also be long and domain-specific. In this paper, we first conduct a comprehensive study on 13 representative software containing 7,349 configuration parameters, and derive six types of ways in which configuration parameters may affect non-performance intentions. Guided by this study, we design SafeTune, a multi-intention-aware method that preselects important performance-related parameters and warns about their side-effects on non-performance intentions. Evaluation on target software shows that SafeTune correctly identifies 22--26 performance-related parameters that are missed by state-of-the-art tools but have significant performance impact (up to 14.7x). Furthermore, we illustrate eight representative cases to show that SafeTune can effectively prevent real-world and critical side-effects on other user intentions.},
%  keywords = {},
%}

@article{almagor2004finding,
  title={Finding effective compilation sequences},
  author={Almagor, Lelac and Cooper, Keith D and Grosul, Alexander and Harvey, Timothy J and Reeves, Steven W and Subramanian, Devika and Torczon, Linda and Waterman, Todd},
  journal={ACM SIGPLAN Notices},
  volume={39},
  number={7},
  pages={231--239},
  year={2004},
  publisher={ACM New York, NY, USA},
  link = {https://dl.acm.org/doi/abs/10.1145/998300.997196},
  abstract = {Most modern compilers operate by applying a fixed, program-independent sequence of optimizations to all programs. Compiler writers choose a single "compilation sequence", or perhaps a couple of compilation sequences. In choosing a sequence, they may consider performance of benchmarks or other important codes. These sequences are intended as general-purpose tools, accessible through command-line flags such as -O2 and -O3.Specific compilation sequences make a significant difference in the quality of the generated code, whether compiling for speed, for space, or for other metrics. A single universal compilation sequence does not produce the best results over all programs [8, 10, 29, 32]. Finding an optimal program-specific compilation sequence is difficult because the space of potential sequences is huge and the interactions between optimizations are poorly understood. Moreover, there is no systematic exploration of the costs and benefits of searching for good (i.e., within a certain percentage of optimal) program-specific compilation sequences.In this paper, we perform a large experimental study of the space of compilation sequences over a set of known benchmarks, using our prototype adaptive compiler. Our goal is to characterize these spaces and to determine if it is cost-effective to construct custom compilation sequences. We report on five exhaustive enumerations which demonstrate that 80% of the local minima in the space are within 5 to 10% of the optimal solution. We describe three algorithms tailored to search such spaces and report on experiments that use these algorithms to find good compilation sequences. These experiments suggest that properties observed in the enumerations hold for larger search spaces and larger programs. Our findings indicate that for the cost of 200 to 4,550 compilations, we can find custom sequences that are 15 to 25% better than the human-designed fixed-sequence originally used in our compiler.},
  keywords = {},
}

@inproceedings{hoste2008cole,
  title={Cole: compiler optimization level exploration},
  author={Hoste, Kenneth and Eeckhout, Lieven},
  booktitle={Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization},
  pages={165--174},
  year={2008},
  link = {https://dl.acm.org/doi/abs/10.1145/1356058.1356080},
  abstract = {Modern compilers implement a large number of optimizations which all interact in complex ways, and which all have a different impact on code quality, compilation time, code size, energy consumption, etc. For this reason, compilers typically provide a limited number of standard optimization levels, such as -O1, -O2, -O3 and -Os, that combine various optimizations providing a number of trade-offs between multiple objective functions (such as code quality, compilation time and code size). The construction of these optimization levels, i.e., choosing which optimizations to activate at each level, is a manual process typically done using high-level heuristics based on the compiler developer's experience. This paper proposes COLE, Compiler Optimization Level Exploration, a framework for automatically finding Pareto optimal optimization levels through multi-objective evolutionary searching. Our experimental results using GCC and the SPEC CPU benchmarks show that the automatic construction of optimization levels is feasible in practice, and in addition, yields better optimization levels than GCC's manually derived (-Os, -O1, -O2 and -O3) optimization levels, as well as the optimization levels obtained through random sampling. We also demonstrate that COLE can be used to gain insight into the effectiveness of compiler optimizations as well as to better understand a benchmark's sensitivity to compiler optimizations.},
  keywords = {SPEC, GA},
}

@article{倪友聪2019evolutionary,
  title={Evolutionary algorithm for optimization of energy consumption at GCC compile time based on frequent pattern mining},
  author={倪友聪 and 吴瑞 and 杜欣 and 叶鹏 and 李汪彪 and 肖如良},
  journal={Journal of Software},
  volume={30},
  number={5},
  pages={1269--1287},
  year={2019},
  link = {https://www.jos.org.cn/josen/article/abstract/5734},
  abstract = {The evolutionary algorithms have been used to improve the energy consumption of executable code of embedded software by searching the optimal compilation options of GCC compiler. However, such algorithms do not consider the possible interaction between multiple compilation options so that the quality of their solutions is not high, and their convergence speed is slow. To solve this problem, this study designs an evolutionary algorithm based on frequent pattern mining, called GA-FP. In the process of evolution, GA-FP uses frequent pattern mining to obtain a set of compilation options which are of high-frequency and contribute to significant improvement on energy consumption. The derived options are used as the heuristic information and two mutation operators of ADD and DELETE are designed to increase the quality of solution and accelerate the convergence speed. The comparative experiments are done on 8 typical cases in 5 different fields between Tree-EDA and GA-FP. The experimental results indicate that the GA-FP can not only reduce the energy consumption of software more effectively (the average and maximal reduction ratios are 2.5% and 21.1% respectively), but also converge faster (the average of 34.5% faster and up to 83.3% faster) when the energy optimization effect obtained by GA-FP is no less than that of Tree-EDA. The correlation analysis of compilation options in the optimal solution further validates the effectiveness of the designed mutation operators.},
  keywords = {GA},
}

@inproceedings{sandran2012optimized,
  title={An optimized tuning of genetic algorithm parameters in compiler flag selection based on compilation and execution duration},
  author={Sandran, Thayalan and Zakaria, Nordin and Pal, Anindya Jyoti},
  booktitle={Proceedings of the International Conference on Soft Computing for Problem Solving (SocProS 2011) December 20-22, 2011: Volume 2},
  pages={599--610},
  year={2012},
  organization={Springer},
  link = {https://link.springer.com/chapter/10.1007/978-81-322-0491-6_55},
  abstract = {Compiler flags exist to provide option for the software developer to dictate certain parameter to the compiler. Such parameters provide hints to the compiler on how to handle certain portion of the source code. In the realm of optimization, compiler flags provide the fastest way to speed up a program. The right combination of flags will provide significant enhancement in speed without compromising the integrity of the output. However, the main challenge is choosing that particular right set of flags. Many a times, developers work around this issue by dictating the optimization level. In that way, the compiler imposes a package of flags. This process may lead to degradation of performance in terms of execution speed and also significant increase in program size. In this work, we are studying the usage of Genetic Algorithm as a way to select the optimization flags that could produce codes which compile and execute fast.},
  keywords = {},
}

@inproceedings{perez2018automatic,
  title={Automatic configuration of GCC using irace},
  author={P{\'e}rez C{\'a}ceres, Leslie and Pagnozzi, Federico and Franzin, Alberto and St{\"u}tzle, Thomas},
  booktitle={Artificial Evolution: 13th International Conference, {\'E}volution Artificielle, EA 2017, Paris, France, October 25--27, 2017, Revised Selected Papers 13},
  pages={202--216},
  year={2018},
  organization={Springer},
  link = {https://link.springer.com/chapter/10.1007/978-3-319-78133-4_15},
  keywords = {},
  abstract = {Automatic algorithm configuration techniques have proved to be successful in finding performance-optimizing parameter settings of many search-based decision and optimization algorithms. A recurrent, important step in software development is the compilation of source code written in some programming language into machine-executable code. The generation of performance-optimized machine code itself is a difficult task that can be parametrized in many different possible ways. While modern compilers usually offer different levels of optimization as possible defaults, they have a larger number of other flags and numerical parameters that impact properties of the generated machine-code. While the generation of performance-optimized machine code has received large attention and is dealt with in the research area of auto-tuning, the usage of standard automatic algorithm configuration software has not been explored, even though, as we show in this article, the performance of the compiled code has significant stochasticity, just as standard optimization algorithms. As a practical case study, we consider the configuration of the well-known GNU compiler collection (GCC) for minimizing the run-time of machine code for various heuristic search methods. Our experimental results show that, depending on the specific code to be optimized, improvements of up to 40% of execution time when compared to the -O2 and -O3 optimization flags is possible.},
}

@inproceedings{agakov2006using,
  title={Using machine learning to focus iterative optimization},
  author={Agakov, Felix and Bonilla, Edwin and Cavazos, John and Franke, Bj{\"o}rn and Fursin, Grigori and O'Boyle, Michael FP and Thomson, John and Toussaint, Marc and Williams, Christopher KI},
  booktitle={International Symposium on Code Generation and Optimization (CGO'06)},
  pages={11--pp},
  year={2006},
  organization={IEEE},
  link = {https://ieeexplore.ieee.org/abstract/document/1611549/},
    abstract = {Iterative compiler optimization has been shown to outperform static approaches. This, however, is at the cost of large numbers of evaluations of the program. This paper develops a new methodology to reduce this number and hence speed up iterative optimization. It uses predictive modelling from the domain of machine learning to automatically focus search on those areas likely to give greatest performance. This approach is independent of search algorithm, search space or compiler infrastructure and scales gracefully with the compiler optimization space size. Off-line, a training set of programs is iteratively evaluated and the shape of the spaces and program features are modelled. These models are learnt and used to focus the iterative optimization of a new program. We evaluate two learnt models, an independent and Markov model, and evaluate their worth on two embedded platforms, the Texas Instrument C67I3 and the AMD Au1500. We show that such learnt models can speed up iterative search on large spaces by an order of magnitude. This translates into an average speedup of 1.22 on the TI C6713 and 1.27 on the AMD Au1500 in just 2 evaluations.},
  keywords = {Markov,RL},
}

@article{stephenson2003meta,
  title={Meta optimization: Improving compiler heuristics with machine learning},
  author={Stephenson, Mark and Amarasinghe, Saman and Martin, Martin and O'Reilly, Una-May},
  journal={ACM sigplan notices},
  volume={38},
  number={5},
  pages={77--90},
  year={2003},
  publisher={ACM New York, NY, USA},
  link = {https://dl.acm.org/doi/abs/10.1145/780822.781141},
  keywords = {},
  abstract = {Compiler writers have crafted many heuristics over the years to approximately solve NP-hard problems efficiently. Finding a heuristic that performs well on a broad range of applications is a tedious and difficult process. This paper introduces Meta Optimization, a methodology for automatically fine-tuning compiler heuristics. Meta Optimization uses machine-learning techniques to automatically search the space of compiler heuristics. Our techniques reduce compiler design complexity by relieving compiler writers of the tedium of heuristic tuning. Our machine-learning system uses an evolutionary algorithm to automatically find effective compiler heuristics. We present promising experimental results. In one mode of operation Meta Optimization creates application-specific heuristics which often result in impressive speedups. For hyperblock formation, one optimization we present in this paper, we obtain an average speedup of 23% (up to 73%) for the applications in our suite. Furthermore, by evolving a compiler's heuristic over several benchmarks, we can create effective, general-purpose heuristics. The best general-purpose heuristic our system found for hyperblock formation improved performance by an average of 25% on our training set, and 9% on a completely unrelated test set. We demonstrate the efficacy of our techniques on three different optimizations in this paper: hyperblock formation, register allocation, and data prefetching.},
}