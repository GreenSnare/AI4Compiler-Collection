@inproceedings{cummins2017end,
  title={End-to-end deep learning of optimization heuristics},
  author={Cummins, Chris and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},
  booktitle={2017 26th International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  pages={219--232},
  year={2017},
  organization={IEEE},
  abstract = {Accurate automatic optimization heuristics are necessary for dealing with thecomplexity and diversity of modern hardware and software. Machine learning is aproven technique for learning such heuristics, but its success is bound by thequality of the features used. These features must be hand crafted by developersthrough a combination of expert domain knowledge and trial and error. This makesthe quality of the final model directly dependent on the skill and availabletime of the system architect. Our work introduces a better way for building heuristics. We develop a deepneural network that learns heuristics over raw code, entirely without using codefeatures. The neural network simultaneously constructs appropriaterepresentations of the code and learns how best to optimize, removing the needfor manual feature creation. Further, we show that our neural nets can transferlearning from one optimization problem to another, improving the accuracy of newmodels, without the help of human experts. We compare the effectiveness of our automatically generated heuristics againstones with features hand-picked by experts. We examine two challenging tasks:predicting optimal mapping for heterogeneous parallelism and GPU threadcoarsening factors. In 89% of the cases, the quality of our fully automaticheuristics matches or surpasses that of state-of-the-art predictive models usinghand-crafted features, providing on average 14% and 12% more performance withno human effort expended on designing features.},
  keywords = {herustic learning},
  link = {https://doi.org/10.1109/PACT.2017.24},
}

@inproceedings{cummins2021programl,
  title={Programl: A graph-based program representation for data flow analysis and compiler optimizations},
  author={Cummins, Chris and Fisches, Zacharias V and Ben-Nun, Tal and Hoefler, Torsten and O’Boyle, Michael FP and Leather, Hugh},
  booktitle={International Conference on Machine Learning},
  pages={2244--2253},
  year={2021},
  organization={PMLR},
  abstract = {Machine learning (ML) is increasingly seen as a viable approach for building compiler optimization heuristics, but many ML methods cannot replicate even the simplest of the data flow analyses that are critical to making good optimization decisions. We posit that if ML cannot do that, then it is insufficiently able to reason about programs. We formulate data flow analyses as supervised learning tasks and introduce a large open dataset of programs and their corresponding labels from several analyses. We use this dataset to benchmark ML methods and show that they struggle on these fundamental program reasoning tasks. We propose ProGraML - Program Graphs for Machine Learning - a language-independent, portable representation of program semantics. ProGraML overcomes the limitations of prior works and yields improved performance on downstream optimization tasks.},
  link = {https://proceedings.mlr.press/v139/cummins21a.html},
  keywords = {loop,dfg,SVM},
}

@inproceedings{haj2020neurovectorizer,
  title={Neurovectorizer: End-to-end vectorization with deep reinforcement learning},
  author={Haj-Ali, Ameer and Ahmed, Nesreen K and Willke, Ted and Shao, Yakun Sophia and Asanovic, Krste and Stoica, Ion},
  booktitle={Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
  pages={242--255},
  year={2020},
  abstract = {One of the key challenges arising when compilers vectorize loops for today’s SIMD-compatible architectures is to decide if vectorization or interleaving is beneficial. Then, the compiler has to determine the number of instructions to pack together and the interleaving level (stride). Compilers are designed today to use fixed-cost models that are based on heuristics to make vectorization decisions on loops. However, these models are unable to capture the data dependency, the computation graph, or the organization of instructions. Alternatively, software engineers often hand-write the vectorization factors of every loop. This, however, places a huge burden on them, since it requires prior experience and significantly increases the development time.
In this work, we explore a novel approach for handling loop vectorization and propose an end-to-end solution using deep reinforcement learning (RL). We conjecture that deep RL can capture different instructions, dependencies, and data structures to enable learning a sophisticated model that can better predict the actual performance cost and determine the optimal vectorization factors. We develop an end-to-end framework, from code to vectorization, that integrates deep RL in the LLVM compiler. Our proposed framework takes benchmark codes as input and extracts the loop codes. These loop codes are then fed to a loop embedding generator that learns an embedding for these loops. Finally, the learned embeddings are used as input to a Deep RL agent, which dynamically determines the vectorization factors for all the loops. We further extend our framework to support random search, decision trees, supervised neural networks, and nearest-neighbor search. We evaluate our approaches against the currently used LLVM vectorizer and loop polyhedral optimization techniques. Our experiments show 1.29×−4.73× performance speedup compared to baseline and only 3% worse than the brute-force search on a wide range of benchmarks.},
  link = {https://dl.acm.org/doi/abs/10.1145/3368826.3377928},
  keywords = {loop,rl},
}

@inproceedings{brauckmann2021polygym,
  title={Polygym: Polyhedral optimizations as an environment for reinforcement learning},
  author={Brauckmann, Alexander and Goens, Andr{\'e}s and Castrillon, Jeronimo},
  booktitle={2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  pages={17--29},
  year={2021},
  organization={IEEE},
  keywords = {rl,poly},
  abstract = {The polyhedral model allows a structured way of defining semantics-preserving transformations to improve the performance of a large class of loops. Finding profitable points in this space is a hard problem which is usually approached by heuristics that generalize from domain-expert knowledge. Existing search space formulations in state-of-the-art heuristics depend on the shape of particular loops, making it hard to leverage generic and more powerful optimization techniques from the machine learning domain. In this paper, we propose a shape-agnostic formulation for the space of legal transformations in the polyhedral model as a Markov Decision Process (MDP). Instead of using transformations, the formulation is based on an abstract space of possible schedules. In this formulation, states model partial schedules, which are constructed by actions that are reusable across different loops. With a simple heuristic to traverse the space, we demonstrate that our formulation is powerful enough to match and outperform state-of-the-art heuristics. On the Polybench benchmark suite, we found the search space to contain transformations that lead to a speedup of 3.39x over LLVM O3, which is 1.34x better than the best transformations found in the search space of isl, and 1.83x better than the speedup achieved by the default heuristics of isl. Our generic MDP formulation enables future work to use reinforcement learning to learn optimization heuristics over a wide range of loops. This also contributes to the emerging field of machine learning in compilers, as it exposes a novel problem formulation that can push the limits of existing methods.},
  link = {https://doi.org/10.1109/PACT52795.2021.00009},
}

@article{trofin2021mlgo,
  title={Mlgo: a machine learning guided compiler optimizations framework},
  author={Trofin, Mircea and Qian, Yundi and Brevdo, Eugene and Lin, Zinan and Choromanski, Krzysztof and Li, David},
  journal={arXiv preprint arXiv:2101.04808},
  year={2021},
  abstract = {Leveraging machine-learning (ML) techniques for compiler optimizations has been widely studied and explored in academia. However, the adoption of ML in general-purpose, industry strength compilers has yet to happen. We propose MLGO, a framework for integrating ML techniques systematically in an industrial compiler -- LLVM. As a case study, we present the details and results of replacing the heuristics-based inlining-for-size optimization in LLVM with machine learned models. To the best of our knowledge, this work is the first full integration of ML in a complex compiler pass in a real-world setting. It is available in the main LLVM repository. We use two different ML algorithms: Policy Gradient and Evolution Strategies, to train the inlining-for-size model, and achieve up to 7\% size reduction, when compared to state of the art LLVM -Oz. The same model, trained on one corpus, generalizes well to a diversity of real-world targets, as well as to the same set of targets after months of active development. This property of the trained models is beneficial to deploy ML techniques in real-world settings.},
  keywords = {codesize,inline},
  link = {https://arxiv.org/abs/2101.04808},
}

@inproceedings{venkatakeerthy2023rl4real,
  title={Rl4real: Reinforcement learning for register allocation},
  author={VenkataKeerthy, S and Jain, Siddharth and Kundu, Anilava and Aggarwal, Rohit and Cohen, Albert and Upadrasta, Ramakrishna},
  booktitle={Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction},
  pages={133--144},
  year={2023},
  abstract = {We aim to automate decades of research and experience in register allocation, leveraging machine learning. We tackle this problem by embedding a multi-agent reinforcement learning algorithm within LLVM, training it with the state of the art techniques. We formalize the constraints that precisely define the problem for a given instruction-set architecture, while ensuring that the generated code preserves semantic correctness. We also develop a gRPC based framework providing a modular and efficient compiler interface for training and inference. Our approach is architecture independent: we show experimental results targeting Intel x86 and ARM AArch64. Our results match or out-perform the heavily tuned, production-grade register allocators of LLVM.},
  keywords = {rl,register alloc,llvm-mca},
  link = {https://dl.acm.org/doi/abs/10.1145/3578360.3580273},
}

@inproceedings{zheng2024mloop,
  title={mLOOP: Optimize Loop Unrolling in Compilation with a ML-based Approach},
  author={Zheng, Zhongchun and Wu, Yuan and Zhang, Xianwei},
  booktitle={2024 International Conference on Networking, Architecture and Storage (NAS)},
  pages={1--8},
  year={2024},
  organization={IEEE},
  abstract = {Loops are a fundamental component of programs, providing an structured and efficient way to execute repetitive tasks. Given their prevalence and significance, the performance of loops has a direct impact on the overall execution of a program. Predicting loop unroll factor holds remarkable importance in the domain of loop optimization and vectorization parallelism. With the rapid advancements in this field, leveraging machine learning (ML) methods for compilation optimization has emerged as a new research focus. Whereas traditional heuristic algorithms lack precision and Profile-Guided Optimization (PGO) techniques incur considerable compilation overhead, ML method serve as a more balanced approach with respect to accuracy and compilation time. Nonetheless, existing ML approaches are commonly confined to individual optimizations and fail to consider the interplay between multiple optimizations. Additionally, there is inadequate utilization of compilation optimization parameters, resulting in redundant calculations across different optimization processes. This paper proposes mLOOP, a method that employs the XGBoost model to predict loop unroll factors which are integrated into the metadata for use throughout the compilation pipeline. To facilitate deployment and testing in practices, mLOOP is encapsulated into a LLVM optimization pass. By testing on multiple loop-intensive benchmarks, mLOOP achieves 7% speedup on X86 platform and 12% on ARM.},
  link = {https://doi.org/10.1109/NAS63802.2024.10781373},
  keywords = {loop},
}

@article{ashouri2022mlgoperf,
  title={Mlgoperf: An ml guided inliner to optimize performance},
  author={Ashouri, Amir H and Elhoushi, Mostafa and Hua, Yuzhe and Wang, Xiang and Manzoor, Muhammad Asif and Chan, Bryan and Gao, Yaoqing},
  journal={arXiv preprint arXiv:2207.08389},
  year={2022},
  abstract = {For the past 25 years, we have witnessed an extensive application of Machine Learning to the Compiler space; the selection and the phase-ordering problem. However, limited works have been upstreamed into the state-of-the-art compilers, i.e., LLVM, to seamlessly integrate the former into the optimization pipeline of a compiler to be readily deployed by the user. MLGO was among the first of such projects and it only strives to reduce the code size of a binary with an ML-based Inliner using Reinforcement Learning.This paper presents MLGOPerf; the first end-to-end framework capable of optimizing performance using LLVM's ML-Inliner. It employs a secondary ML model to generate rewards used for training a retargeted Reinforcement learning agent, previously used as the primary model by MLGO. It does so by predicting the post-inlining speedup of a function under analysis and it enables a fast training framework for the primary model which otherwise wouldn't be practical. The experimental results show MLGOPerf is able to gain up to 1.8% and 2.2% with respect to LLVM's optimization at O3 when trained for performance on SPEC CPU2006 and Cbench benchmarks, respectively. Furthermore, the proposed approach provides up to 26% increased opportunities to autotune code regions for our benchmarks which can be translated into an additional 3.7% speedup value.},
  link = {https://arxiv.org/abs/2207.08389},
  keywords = {inline}
}

@article{mendis2019compiler,
  title={Compiler auto-vectorization with imitation learning},
  author={Mendis, Charith and Yang, Cambridge and Pu, Yewen and Amarasinghe, Dr Saman and Carbin, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  abstract = {Modern microprocessors are equipped with single instruction multiple data (SIMD) or vector instruction sets which allow compilers to exploit fine-grained data level parallelism. To exploit this parallelism, compilers employ auto-vectorization techniques to automatically convert scalar code into vector code. Larsen & Amarasinghe (2000) first introduced superword level parallelism (SLP) based vectorization, which is one form of vectorization popularly used by compilers. Current compilers employ hand-crafted heuristics and typically only follow one SLP vectorization strategy which can be suboptimal. Recently, Mendis & Amarasinghe (2018) formulated the instruction packing problem of SLP vectorization by leveraging an integer linear programming (ILP) solver, achieving superior runtime performance. In this work, we explore whether it is feasible to imitate optimal decisions made by their ILP solution by fitting a graph neural network policy. We show that the learnt policy produces a vectorization scheme which is better than industry standard compiler heuristics both in terms of static measures and runtime performance. More specifically, the learnt agent produces a vectorization scheme which has a 22.6% higher average reduction in cost compared to LLVM compiler when measured using its own cost model and achieves a geometric mean runtime speedup of 1.015× on the NAS benchmark suite when compared to LLVM’s SLP vectorizer.},
  keywords = {loop,slp},
  link = {https://proceedings.neurips.cc/paper/2019/hash/d1d5923fc822531bbfd9d87d4760914b-Abstract.html},
}

@article{wang2014integrating,
  title={Integrating profile-driven parallelism detection and machine-learning-based mapping},
  author={Wang, Zheng and Tournavitis, Georgios and Franke, Bj{\"o}rn and O'boyle, Michael FP},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={11},
  number={1},
  pages={1--26},
  year={2014},
  publisher={ACM New York, NY, USA},
  link = {https://dl.acm.org/doi/abs/10.1145/2579561},
  abstract = {Compiler-based auto-parallelization is a much-studied area but has yet to find widespread application. This is largely due to the poor identification and exploitation of application parallelism, resulting in disappointing performance far below that which a skilled expert programmer could achieve. We have identified two weaknesses in traditional parallelizing compilers and propose a novel, integrated approach resulting in significant performance improvements of the generated parallel code. Using profile-driven parallelism detection, we overcome the limitations of static analysis, enabling the identification of more application parallelism, and only rely on the user for final approval. We then replace the traditional target-specific and inflexible mapping heuristics with a machine-learning-based prediction mechanism, resulting in better mapping decisions while automating adaptation to different target architectures. We have evaluated our parallelization strategy on the NAS and SPEC CPU2000 benchmarks and two different multicore platforms (dual quad-core Intel Xeon SMP and dual-socket QS20 Cell blade). We demonstrate that our approach not only yields significant improvements when compared with state-of-the-art parallelizing compilers but also comes close to and sometimes exceeds the performance of manually parallelized codes. On average, our methodology achieves 96% of the performance of the hand-tuned OpenMP NAS and SPEC parallel benchmarks on the Intel Xeon platform and gains a significant speedup for the IBM Cell platform, demonstrating the potential of profile-guided and machine-learning- based parallelization for complex multicore platforms.},
  keywords = {loop,Openmp},
}

@inproceedings{wood2021artemis,
  title={Artemis: Automatic runtime tuning of parallel execution parameters using machine learning},
  author={Wood, Chad and Georgakoudis, Giorgis and Beckingsale, David and Poliakoff, David and Gimenez, Alfredo and Huck, Kevin and Malony, Allen and Gamblin, Todd},
  booktitle={High Performance Computing: 36th International Conference, ISC High Performance 2021, Virtual Event, June 24--July 2, 2021, Proceedings 36},
  pages={453--472},
  year={2021},
  organization={Springer},
  abstract = {Portable parallel programming models provide the potential for high performance and productivity, however they come with a multitude of runtime parameters that can have significant impact on execution performance. Selecting the optimal set of those parameters is non-trivial, so that HPC applications perform well in different system environments and on different input data sets, without the need of time consuming parameter exploration or major algorithmic adjustments. We present Artemis, a method for online, feedback-driven, automatic parameter tuning using machine learning that is generalizable and suitable for integration into high-performance codes. Artemis monitors execution at runtime and creates adaptive models for tuning execution parameters, while being minimally invasive in application development and runtime overhead. We demonstrate the effectiveness of Artemis by optimizing the execution times of three HPC proxy applications: Cleverleaf, LULESH, and Kokkos Kernels SpMV. Evaluation shows that Artemis selects the optimal execution policy with over 85% accuracy, has modest monitoring overhead of less than 9%, and increases execution speed by up to 47% despite its runtime overhead.},
  link = {https://link.springer.com/chapter/10.1007/978-3-030-78713-4_24},
  keywords = {loop},
}

@article{wu2022autotuning,
  title={Autotuning polybench benchmarks with llvm clang/polly loop optimization pragmas using bayesian optimization},
  author={Wu, Xingfu and Kruse, Michael and Balaprakash, Prasanna and Finkel, Hal and Hovland, Paul and Taylor, Valerie and Hall, Mary},
  journal={Concurrency and Computation: Practice and Experience},
  volume={34},
  number={20},
  pages={e6683},
  year={2022},
  publisher={Wiley Online Library},
  keywords = {loop,bayes},
  abstract = {We develop a ytopt autotuning framework that leverages Bayesian optimization to explore the parameter space search and compare four different supervised learning methods within Bayesian optimization and evaluate their effectiveness. We select six of the most complex PolyBench benchmarks and apply the newly developed LLVM Clang/Polly loop optimization pragmas to the benchmarks to optimize them. We then use the autotuning framework to optimize the pragma parameters to improve their performance. The experimental results show that our autotuning approach outperforms the other compiling methods to provide the smallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu, and covariance with two large datasets in 200 code evaluations for effectively searching the parameter spaces with up to 170,368 different configurations. We find that the Floyd–Warshall benchmark did not benefit from autotuning. To cope with this issue, we provide some compiler option solutions to improve the performance. Then we present loop autotuning without a user's knowledge using a simple mctree autotuning framework to further improve the performance of the Floyd–Warshall benchmark. We also extend the ytopt autotuning framework to tune a deep learning application.},
  link = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6683},
}

@inproceedings{roy2021bliss,
  title={Bliss: auto-tuning complex applications using a pool of diverse lightweight learning models},
  author={Roy, Rohan Basu and Patel, Tirthak and Gadepally, Vijay and Tiwari, Devesh},
  booktitle={Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  pages={1280--1295},
  year={2021},
  keywords = {loop,bayes},
  abstract = {As parallel applications become more complex, auto-tuning becomes more desirable, challenging, and time-consuming. We propose, Bliss, a novel solution for auto-tuning parallel applications without requiring apriori information about applications, domain-specific knowledge, or instrumentation. Bliss demonstrates how to leverage a pool of Bayesian Optimization models to find the near-optimal parameter setting 1.64× faster than the state-of-the-art approaches.},
  link = {https://dl.acm.org/doi/abs/10.1145/3453483.3454109},
}

@inproceedings{zhang2021dynatune,
  title={DynaTune: Dynamic tensor program optimization in deep neural network compilation},
  author={Zhang, Minjia and Li, Menghao and Wang, Chi and Li, Mingqin},
  booktitle={International Conference on Learning Representations},
  year={2021},
  keywords = {RL,DNN},
  abstract = {Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency. In this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2-2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures.},
  link = {https://experts.illinois.edu/en/publications/dynatune-dynamic-tensor-program-optimization-in-deep-neural-netwo},
}

@article{singh2024llm,
  title={An llm-tool compiler for fused parallel function calling},
  author={Singh, Simranjit and Karatzas, Andreas and Fore, Michael and Anagnostopoulos, Iraklis and Stamoulis, Dimitrios},
  journal={arXiv preprint arXiv:2405.17438},
  year={2024},
  link = {https://arxiv.org/abs/2405.17438},
  abstract = {State-of-the-art sequential reasoning in Large Language Models (LLMs) has expanded the capabilities of Copilots beyond conversational tasks to complex function calling, managing thousands of API calls. However, the tendency of compositional prompting to segment tasks into multiple steps, each requiring a round-trip to the GPT APIs, leads to increased system latency and costs. Although recent advancements in parallel function calling have improved tool execution per API call, they may necessitate more detailed in-context instructions and task breakdown at the prompt level, resulting in higher engineering and production costs. Inspired by the hardware design principles of multiply-add (MAD) operations, which fuse multiple arithmetic operations into a single task from the compiler's perspective, we propose LLM-Tool Compiler, which selectively fuses similar types of tool operations under a single function at runtime, presenting them as a unified task to the LLM. This selective fusion inherently enhances parallelization and efficiency. Benchmarked on a large-scale Copilot platform, LLM-Tool Compiler achieves up to four times more parallel calls than existing methods, reducing token costs and latency by up to 40% and 12%, respectively.},
  keywords = {LLM},
}

@inproceedings{stephenson2005predicting,
  title={Predicting unroll factors using supervised classification},
  author={Stephenson, Mark and Amarasinghe, Saman},
  booktitle={International symposium on code generation and optimization},
  pages={123--134},
  year={2005},
  organization={IEEE},
  keywords = {loop},
  link = {https://ieeexplore.ieee.org/abstract/document/1402082},
  abstract = {Compilers base many critical decisions on abstracted architectural models. While recent research has shown that modeling is effective for some compiler problems, building accurate models requires a great deal of human time and effort. This paper describes how machine learning techniques can be leveraged to help compiler writers model complex systems. Because learning techniques can effectively make sense of high dimensional spaces, they can be a valuable tool for clarifying and discerning complex decision boundaries. In this work we focus on loop unrolling, a well-known optimization for exposing instruction level parallelism. Using the Open Research Compiler as a testbed, we demonstrate how one can use supervised learning techniques to determine the appropriateness of loop unrolling. We use more than 2,500 loops - drawn from 72 benchmarks - to train two different learning algorithms to predict unroll factors (i.e., the amount by which to unroll a loop) for any novel loop. The technique correctly predicts the unroll factor for 65% of the loops in our dataset, which leads to a 5% overall improvement for the SPEC 2000 benchmark suite (9% for the SPEC 2000 floating point benchmarks).},
}

@article{moss1997learning,
  title={Learning to schedule straight-line code},
  author={Moss, J and Utgoff, Paul and Cavazos, John and Precup, Doina and Stefanovic, Darko and Brodley, Carla and Scheeff, David},
  journal={Advances in Neural Information Processing Systems},
  volume={10},
  year={1997},
  keywords = {ISched},
  abstract = {Program execution speed on modem computers is sensitive, by a factor of two or more, to the order in which instructions are presented to the proces(cid:173) sor. To realize potential execution efficiency, an optimizing compiler must employ a heuristic algorithm for instruction scheduling. Such algorithms are painstakingly hand-crafted, which is expensive and time-consuming. We show how to cast the instruction scheduling problem as a learning task, ob(cid:173) taining the heuristic scheduling algorithm automatically. Our focus is the narrower problem of scheduling straight-line code (also called basic blocks of instructions). Our empirical results show that just a few features are ad(cid:173) equate for quite good performance at this task for a real modem processor, and that any of several supervised learning methods perform nearly opti(cid:173) mally with respect to the features used.},
  link = {https://proceedings.neurips.cc/paper/1997/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html},
}

@article{cavazos2006method,
  title={Method-specific dynamic compilation using logistic regression},
  author={Cavazos, John and O'boyle, Michael FP},
  journal={ACM SIGPLAN Notices},
  volume={41},
  number={10},
  pages={229--240},
  year={2006},
  publisher={ACM New York, NY, USA},
  abstract = {Determining the best set of optimizations to apply to a program has been a long standing problem for compiler writers. To reduce the complexity of this task, existing approaches typically apply the same set of optimizations to all procedures within a program, without regard to their particular structure. This paper develops a new method-specific approach that automatically selects the best optimizations on a per method basis within a dynamic compiler. Our approach uses the machine learning technique of logistic regression to automatically derive a predictive model that determines which optimizations to apply based on the features of a method. This technique is implemented in the Jikes RVM Java JIT compiler. Using this approach we reduce the average total execution time of the SPECjvm98 benchmarks by 29%. When the same heuristic is applied to the DaCapo+ benchmark suite, we obtain an average 33% reduction over the default level O2 setting.},
  link = {https://dl.acm.org/doi/abs/10.1145/1167515.1167492},
  keywords = {inline},
}

@article{leather2014automatic,
  title={Automatic feature generation for machine learning--based optimising compilation},
  author={Leather, Hugh and Bonilla, Edwin and O'boyle, Michael},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={11},
  number={1},
  pages={1--32},
  year={2014},
  publisher={ACM New York, NY, USA},
  link = {https://dl.acm.org/doi/abs/10.1145/2536688},
  abstract = {Recent work has shown that machine learning can automate and in some cases outperform handcrafted compiler optimisations. Central to such an approach is that machine learning techniques typically rely upon summaries or features of the program. The quality of these features is critical to the accuracy of the resulting machine learned algorithm; no machine learning method will work well with poorly chosen features. However, due to the size and complexity of programs, theoretically there are an infinite number of potential features to choose from. The compiler writer now has to expend effort in choosing the best features from this space. This article develops a novel mechanism to automatically find those features that most improve the quality of the machine learned heuristic. The feature space is described by a grammar and is then searched with genetic programming and predictive modelling. We apply this technique to loop unrolling in GCC 4.3.1 and evaluate our approach on a Pentium 6. On a benchmark suite of 57 programs, GCCs hard-coded heuristic achieves only 3% of the maximum performance available, whereas a state-of-the-art machine learning approach with hand-coded features obtains 59%. Our feature generation technique is able to achieve 76% of the maximum available speedup, outperforming existing approaches.},
  keywords = {GCC},
}

@article{calder1997evidence,
  title={Evidence-based static branch prediction using machine learning},
  author={Calder, Brad and Grunwald, Dirk and Jones, Michael and Lindsay, Donald and Martin, James and Mozer, Michael and Zorn, Benjamin},
  journal={ACM Transactions on Programming Languages and Systems (TOPLAS)},
  volume={19},
  number={1},
  pages={188--222},
  year={1997},
  publisher={ACM New York, NY, USA},
  link = {https://dl.acm.org/doi/abs/10.1145/239912.239923},
  abstract = {Correctly predicting the direction that branches will take is increasingly important in today's wide-issue computer architectures. The name program-based branch prediction is given to static branch prediction techniques that base their prediction on a program's structure. In this article, we investigate a new approach to program-based branch prediction that uses a body of existing programs to predict the branch behavior in a new program. We call this approach to program-based branch prediction evidence-based static prediction, or ESP. The main idea of ESP is that the behavior of a corpus of programs can be used to infer the behavior of new programs. In this article, we use neural networks and decision trees to map static features associated with each branch to a prediction that the branch will be taken. ESP shows significant advantages over other prediction mechanisms. Specifically, it is a program-based technique; it is effective across a range of programming languages and programming styles; and it does not rely on the use of expert-defined heuristics. In this article, we describe the application of ESP to the problem of static branch prediction and compare our results to existing program-based branch predictors. We also investigate the applicability of ESP across computer architectures, programming languages, compilers, and run-time systems. We provide results showing how sensitive ESP is to the number and type of static features and programs included in the ESP training sets, and we compare the efficacy of static branch prediction for subroutine libraries. Averaging over a body of 43 C and Fortran programs, ESP branch prediction results in a miss rate of 20%, as compared with the 25% miss rate obtained using the best existing program-based heuristics.},
  keywords = {branch},
}