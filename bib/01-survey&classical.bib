@inproceedings{leather2020machine,
  title={Machine learning in compilers: Past, present and future},
  author={Leather, Hugh and Cummins, Chris},
  booktitle={2020 Forum for Specification and Design Languages (FDL)},
  pages={1--8},
  year={2020},
  organization={IEEE},
  link = {https://doi.org/10.1109/FDL50818.2020.9232934},
  abstract = {Writing optimising compilers is difficult. The range of programs that may be presented to the compiler is huge and the systems on which they run are complex, heterogeneous, non-deterministic, and constantly changing. The space of possible optimisations is also vast, making it very hard for compiler writers to design heuristics that take all of these considerations into account. As a result, many compiler optimisations are out of date or poorly tuned. Near the turn of the century it was first shown how compilers could be made to automatically search the optimisation space, producing programs far better optimised than previously possible, and without the need for compiler writers to worry about architecture or program specifics. The searches, though, were slow, so in the years that followed, machine learning was developed to learn heuristics from the results of previous searches so that thereafter the search could be avoided and much of the benefit could be gained in a single shot. In this paper we will give a retrospective of machine learning in compiler optimisation from its earliest inception, through some of the works that set themselves apart, to today's deep learning, finishing with our vision of the field's future.},
}

@article{wang2018machine,
  title={Machine learning in compiler optimisation},
  author={Wang, Zheng and O'Boyle, Michael},
  journal={arXiv preprint arXiv:1805.03441},
  year={2018},
  link = {https://doi.org/10.1109/JPROC.2018.2817118},
  abstract = {In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements.
  },
}

@article{ashouri2018survey,
  title={A survey on compiler autotuning using machine learning},
  author={Ashouri, Amir H and Killian, William and Cavazos, John and Palermo, Gianluca and Silvano, Cristina},
  journal={ACM Computing Surveys (CSUR)},
  volume={51},
  number={5},
  pages={1--42},
  year={2018},
  publisher={ACM New York, NY, USA},
  link = {https://doi.org/10.1145/3197978},
  abstract = {Since the mid-1990s, researchers have been trying to use machine-learning-based approaches to solve a number of different compiler optimization problems. These techniques primarily enhance the quality of the obtained results and, more importantly, make it feasible to tackle two main compiler optimization problems: optimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of applying optimizations). The compiler optimization space continues to grow due to the advancement of applications, increasing number of compiler optimizations, and new target architectures. Generic optimization passes in compilers cannot fully leverage newly introduced optimizations and, therefore, cannot keep up with the pace of increasing options. This survey summarizes and classifies the recent advances in using machine learning for the compiler optimization field, particularly on the two major problems of (1) selecting the best optimizations, and (2) the phase-ordering of optimizations. The survey highlights the approaches taken so far, the obtained results, the fine-grain classification among different approaches, and finally, the influential papers of the field.},
}

@article{ashouri2016compiler,
  title={Compiler autotuning using machine learning techniques},
  author={Ashouri, Amir Hossein},
  year={2016},
  publisher={Politecnico di Milano},
  link={https://www.politesi.polimi.it/handle/10589/129561},
  abstract = {Recent developments in silicon production and fabrication led to the creation of much faster computational units such as CPUs, GPUs, FPGAs, and similar chips with varying instruction set architectures (ISAs). Software (SW) programming paradigms including OpenMP, MPI, OpenCL, and OpenACC allow software developers to exploit Hardware (HW) parallelism to port legacy serial codes on these emerging platforms to attain application speedups. Compilers struggle to keep up with the increasing development pace of ever-expanding hardware and software programming paradigms. Additionally, growing complexity of the modern compilers and the concern over security are among the more serious problems that compilers should answer. Moore’s law states that transistor density should double every two years; however, the rate of compilers, which are faced with many open-research problems, have not been able to improve more than a few percentage points each year. Diversity of today’s architectures have forced programmers to spend additional ef- fort to port and tune their application code across different platforms. Compilers within this process need additional tuning which is a hard task itself. Recent compilers of- fer a vast number of multilayered optimizations, capable of targeting different code segments of an application. Choosing among these optimizations can significantly im- pact the performance of the code being optimized. The selection of the right set of compiler optimizations for a particular code segment is a very hard problem, but find- ing the best ordering of these optimizations adds further complexity. In fact, finding the best ordering is a long standing problem in compilation research called the phase- ordering problem. The traditional approach of constructing compiler heuristics to solve this problem simply can not cope with the enormous complexity of choosing the right ordering of optimizations for every code segment in an application. In this PhD thesis, we provide break-through approaches to tackle and mitigate the well-known problems of compiler optimization using design space exploration and ma- chine learning techniques. We show that not all the optimization passes are beneficial to be used within an optimization sequence and in fact many of the available passes are obliterating the effect of one another when ordering of the phases are taken into account. Experimental results show major improvement in performance metrics when our customized prediction models are in place versus standard fixed optimization passes predefined within state-of-the-art compiler frameworks e.g. GCC, LLVM, etc. We per- form application specific optimization based on the characteristics of applications under analysis and we show that this methodology is beneficial to mitigate the hard problem of selecting the best compiler optimizations and the phase-ordering problem. Late but not least, we hope that the proposed approaches in this PhD thesis will be useful for a wide range of readers, including computer architects, compiler developers, researchers and technical professionals.},
}

@inproceedings{mithul2024exploring,
  title={Exploring Compiler Optimization: A Survey of ML, DL and RL Techniques},
  author={Mithul, C and Abdulla, D Mohammad and Virinchi, M Hari and Sathvik, M and Belwal, Meena},
  booktitle={2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS)},
  pages={1--6},
  year={2024},
  organization={IEEE},
  abstract = {The past few years, traditional compiler optimization methods have been found to be further enhanced by machine learning (ML), deep learning (DL) and reinforcement learning (RL). These differ from classical techniques that often use rule of thumb based decision making. Rather, ML/DL/RL based approaches provide a means for learning from data thus improving performance in different dimensions such as code generation, resource allocation and runtime. In this paper we give an overview of current research and methodologies utilizing ML, DL and RL for compiler optimization purposes. We analyze the major models in terms of their employed learning strategies and desired optimizations within a compiler framework. Moreover, we highlight some of the difficulties faced when these compilers are embedded with these learning models such as adaptability, generalization and overhead trade-offs. Additionally, our survey presents case studies demonstrating Quantitative improvements on well-known benchmarks mainly focusing on models' adaptability to different architectures and their role in supporting the decision-making process of compilers. We conclude outlining open research questions as well as possible future directions for further investigations into this emerging interdisciplinary field.},
  link = {https://doi.org/10.1109/CSITSS64042.2024.10816929},
}

@inproceedings{pandey2024survey,
  title={A Survey of Optimized Compiler Using Advanced Machine learning and Deep Learning Techniques},
  author={Pandey, Lal Bahadur and Sharma, Manisha and Tiwari, Rajesh and Panda, Radhe Shyam and Roy, Partha},
  booktitle={2024 IEEE 6th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)},
  pages={256--259},
  year={2024},
  organization={IEEE},
  abstract = {Optimizing compilers is a difficult and time-consuming task, especially when done by hand. As far as we know, the compiler handles both translation and optimization. An efficient compiler system can become more automated and simple, as evidenced by recent studies using deep learning and machine learning approaches. Model training, prediction, optimization, and feature selection are handled by most machine learning and deep learning methods. In this case, choosing the optimal characteristics is necessary in order to use deep learning and machine learning techniques to enhance the optimization quality. This study examines various approaches that might be utilized to enhance and refine the quality of the chosen heuristics as well as the general quality of machine learning and deep learning models in order to boost the compiler's efficiency. The phase-ordering problem, the amount of iterative program evaluations, and the time needed to obtain the best forecast are only a few of the many subjects covered by these approaches.},
  link = {https://doi.org/10.1109/ICCCMLA63077.2024.10871813},
}

@article{wu2022survey,
  title={A survey of machine learning for computer architecture and systems},
  author={Wu, Nan and Xie, Yuan},
  journal={ACM Computing Surveys (CSUR)},
  volume={55},
  number={3},
  pages={1--39},
  year={2022},
  publisher={ACM New York, NY},
  abstract = {It has been a long time that computer architecture and systems are optimized for efficient execution of machine learning (ML) models. Now, it is time to reconsider the relationship between ML and systems and let ML transform the way that computer architecture and systems are designed. This embraces a twofold meaning: improvement of designers’ productivity and completion of the virtuous cycle. In this article, we present a comprehensive review of the work that applies ML for computer architecture and system design. First, we perform a high-level taxonomy by considering the typical role that ML techniques take in architecture/system design, i.e., either for fast predictive modeling or as the design methodology. Then, we summarize the common problems in computer architecture/system design that can be solved by ML techniques and the typical ML techniques employed to resolve each of them. In addition to emphasis on computer architecture in a narrow sense, we adopt the concept that data centers can be recognized as warehouse-scale computers; sketchy discussions are provided in adjacent computer systems, such as code generation and compiler; we also give attention to how ML techniques can aid and transform design automation. We further provide a future vision of opportunities and potential directions and envision that applying ML for computer architecture and systems would thrive in the community.},
  link = {https://doi.org/10.1145/3494523},
}

@inproceedings{lattner2004llvm,
  title={LLVM: A compilation framework for lifelong program analysis \& transformation},
  author={Lattner, Chris and Adve, Vikram},
  booktitle={International symposium on code generation and optimization, 2004. CGO 2004.},
  pages={75--86},
  year={2004},
  organization={IEEE},
  link = {https://ieeexplore.ieee.org/abstract/document/1281665/},
  abstract = {We describe LLVM (low level virtual machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM defines a common, low-level code representation in static single assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The LLVM compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the benefits LLVM provides for several challenging compiler problems.},
  keywords = {},
}

@book{cooper2022engineering,
  title={Engineering a compiler},
  author={Cooper, Keith D and Torczon, Linda},
  year={2022},
  publisher={Morgan Kaufmann}
}

@inproceedings {199317,
  author = {Mart{\'\i}n Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
  title = {{TensorFlow}: A System for {Large-Scale} Machine Learning},
  booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
  year = {2016},
  isbn = {978-1-931971-33-1},
  address = {Savannah, GA},
  pages = {265--283},
  url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
  publisher = {USENIX Association},
  abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor- Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous “parameter server” designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor- Flow achieves for several real-world applications.}
  month = nov
}
