@article{ashouri2016cobayn,
  title={Cobayn: Compiler autotuning framework using bayesian networks},
  author={Ashouri, Amir Hossein and Mariani, Giovanni and Palermo, Gianluca and Park, Eunjung and Cavazos, John and Silvano, Cristina},
  journal={ACM Transactions on Architecture and Code Optimization (TACO)},
  volume={13},
  number={2},
  pages={1--25},
  year={2016},
  publisher={ACM New York, NY, USA},
  abstract = {The variety of today’s architectures forces programmers to spend a great deal of time porting and tuning application codes across different platforms. Compilers themselves need additional tuning, which has considerable complexity as the standard optimization levels, usually designed for the average case and the specific target architecture, often fail to bring the best results.This article proposes COBAYN: Compiler autotuning framework using BAYesian Networks, an approach for a compiler autotuning methodology using machine learning to speed up application performance and to reduce the cost of the compiler optimization phases. The proposed framework is based on the application characterization done dynamically by using independent microarchitecture features and Bayesian networks. The article also presents an evaluation based on using static analysis and hybrid feature collection approaches. In addition, the article compares Bayesian networks with respect to several state-of-the-art machine-learning models.Experiments were carried out on an ARM embedded platform and GCC compiler by considering two benchmark suites with 39 applications. The set of compiler configurations, selected by the model (less than 7% of the search space), demonstrated an application performance speedup of up to 4.6 × on Polybench (1.85 × on average) and 3.1 × on cBench (1.54 × on average) with respect to standard optimization levels. Moreover, the comparison of the proposed technique with (i) random iterative compilation, (ii) machine learning--based iterative compilation, and (iii) noniterative predictive modeling techniques shows, on average, 1.2 ×, 1.37 ×, and 1.48 × speedup, respectively. Finally, the proposed method demonstrates 4 × and 3 × speedup, respectively, on cBench and Polybench in terms of exploration efficiency given the same quality of the solutions generated by the random iterative compilation model.},
  link = {https://dl.acm.org/doi/abs/10.1145/2928270},
  keywords = {Bayesian,gcc,embedded},
}

@article{grubisic2024compiler,
  title={Compiler generated feedback for large language models},
  author={Grubisic, Dejan and Cummins, Chris and Seeker, Volker and Leather, Hugh},
  journal={arXiv preprint arXiv:2403.14714},
  year={2024},
  abstract = {We introduce a novel paradigm in compiler optimization powered by Large Language Models with compiler feedback to optimize the code size of LLVM assembly. The model takes unoptimized LLVM IR as input and produces optimized IR, the best optimization passes, and instruction counts of both unoptimized and optimized IRs. Then we compile the input with generated optimization passes and evaluate if the predicted instruction count is correct, generated IR is compilable, and corresponds to compiled code. We provide this feedback back to LLM and give it another chance to optimize code. This approach adds an extra 0.53% improvement over -Oz to the original model. Even though, adding more information with feedback seems intuitive, simple sampling techniques achieve much higher performance given 10 or more samples.},
  keywords = {codesize,llm},
  link = {https://arxiv.org/abs/2403.14714},
}

@article{pallister2015identifying,
  title={Identifying compiler options to minimize energy consumption for embedded platforms},
  author={Pallister, James and Hollis, Simon J and Bennett, Jeremy},
  journal={The Computer Journal},
  volume={58},
  number={1},
  pages={95--109},
  year={2015},
  publisher={Oxford University Press},
  link = {https://ieeexplore.ieee.org/abstract/document/8130281},
  abstract = {This paper presents an analysis of the energy consumption of an extensive number of the optimizations a modern compiler can perform. Using GCC as a test case, we evaluate a set of 10 carefully selected benchmarks for 5 different embedded platforms. A fractional factorial design is used to systematically explore the large optimization space (282 possible combinations), while still accurately determining the effects of optimizations and optimization combinations. Hardware power measurements on each platform are taken to ensure all architectural effects on the energy consumption are captured. We show that fractional factorial design can find more optimal combinations than relying on built-in compiler settings. We explore the relationship between run-time and energy consumption, and identify scenarios where they are and are not correlated. A further conclusion of this study is the structure of the benchmark has a larger effect than the hardware architecture on whether the optimization will be effective, and that no single optimization is universally beneficial for execution time or energy consumption.},
  keywords = {GCC},
}